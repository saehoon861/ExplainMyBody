"""
ScienceON API Gatewayë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë…¼ë¬¸ ìë™ ìˆ˜ì§‘

KISTIì˜ ScienceON APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê³¼í•™ê¸°ìˆ  ë…¼ë¬¸ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
1ì–µ 3780ë§Œ ê±´ ì´ìƒì˜ ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ (2026-01-19 ê¸°ì¤€)

API í‚¤ ë°œê¸‰:
https://scienceon.kisti.re.kr/apigateway/
"""

import requests
import time
import json
from typing import List, Optional, Dict
from datetime import datetime
from pathlib import Path

from models import PaperMetadata


class ScienceOnAPICollector:
    """ScienceON API Gateway ìˆ˜ì§‘ê¸°"""

    def __init__(self, client_id: str, client_secret: str):
        """
        Args:
            client_id: API í´ë¼ì´ì–¸íŠ¸ ID
            client_secret: API í´ë¼ì´ì–¸íŠ¸ ì‹œí¬ë¦¿
        """
        self.client_id = client_id
        self.client_secret = client_secret

        # ScienceON API Gateway ì—”ë“œí¬ì¸íŠ¸
        self.base_url = "https://scienceon.kisti.re.kr/api"
        self.token_url = f"{self.base_url}/auth/token"
        self.search_url = f"{self.base_url}/article/search"

        # ì•¡ì„¸ìŠ¤ í† í° (2ì‹œê°„ ìœ íš¨)
        self.access_token = None
        self.token_expires_at = None

        # Rate limiting
        self.rate_limit = 0.5  # 0.5ì´ˆ ëŒ€ê¸°

    def _get_access_token(self) -> str:
        """OAuth 2.0 ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰"""

        # í† í°ì´ ìœ íš¨í•˜ë©´ ì¬ì‚¬ìš©
        if self.access_token and self.token_expires_at:
            if datetime.now().timestamp() < self.token_expires_at:
                return self.access_token

        print("ğŸ”‘ ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰ ì¤‘...")

        try:
            # í† í° ìš”ì²­
            payload = {
                'grant_type': 'client_credentials',
                'client_id': self.client_id,
                'client_secret': self.client_secret
            }

            response = requests.post(self.token_url, data=payload, timeout=30)

            if response.status_code != 200:
                raise Exception(f"í† í° ë°œê¸‰ ì‹¤íŒ¨ (status: {response.status_code})")

            data = response.json()

            self.access_token = data['access_token']
            # 2ì‹œê°„ í›„ ë§Œë£Œ (ì¡°ê¸ˆ ì—¬ìœ ìˆê²Œ 1ì‹œê°„ 50ë¶„ìœ¼ë¡œ ì„¤ì •)
            self.token_expires_at = datetime.now().timestamp() + (110 * 60)

            print("âœ… ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰ ì™„ë£Œ")
            return self.access_token

        except Exception as e:
            print(f"âŒ í† í° ë°œê¸‰ ì‹¤íŒ¨: {e}")
            raise

    def search_papers(
        self,
        query: str,
        max_results: int = 100,
        start_year: Optional[int] = None,
        end_year: Optional[int] = None,
        language: str = "ko"  # ko: í•œêµ­ì–´, en: ì˜ì–´
    ) -> List[dict]:
        """
        ScienceONì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            start_year: ì‹œì‘ ì—°ë„
            end_year: ì¢…ë£Œ ì—°ë„
            language: ì–¸ì–´ í•„í„° (ko, en)

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        papers = []

        print(f"\nğŸ” ScienceON ê²€ìƒ‰: '{query}' (ìµœëŒ€ {max_results}ê°œ)")

        # ì•¡ì„¸ìŠ¤ í† í° íšë“
        token = self._get_access_token()

        # í˜ì´ì§€ë„¤ì´ì…˜
        page_size = 100
        total_pages = (max_results + page_size - 1) // page_size

        for page in range(1, total_pages + 1):
            try:
                # API ìš”ì²­ í—¤ë”
                headers = {
                    'Authorization': f'Bearer {token}',
                    'Content-Type': 'application/json'
                }

                # API ìš”ì²­ íŒŒë¼ë¯¸í„°
                params = {
                    'q': query,
                    'size': min(page_size, max_results - len(papers)),
                    'from': (page - 1) * page_size,
                    'lang': language,
                }

                # ì—°ë„ í•„í„°
                if start_year or end_year:
                    year_filter = {}
                    if start_year:
                        year_filter['gte'] = start_year
                    if end_year:
                        year_filter['lte'] = end_year
                    params['year'] = year_filter

                # API ìš”ì²­
                response = requests.get(
                    self.search_url,
                    headers=headers,
                    params=params,
                    timeout=30
                )

                if response.status_code == 401:
                    # í† í° ë§Œë£Œ, ì¬ë°œê¸‰
                    print("  ğŸ”„ í† í° ë§Œë£Œ, ì¬ë°œê¸‰...")
                    self.access_token = None
                    token = self._get_access_token()
                    headers['Authorization'] = f'Bearer {token}'
                    response = requests.get(
                        self.search_url,
                        headers=headers,
                        params=params,
                        timeout=30
                    )

                if response.status_code != 200:
                    print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨ (status: {response.status_code})")
                    break

                # JSON íŒŒì‹±
                data = response.json()

                # ì´ ê²°ê³¼ ìˆ˜
                if page == 1 and 'total' in data:
                    total = data['total']
                    print(f"  ğŸ“Š ì´ {total:,}ê°œ ë…¼ë¬¸ ë°œê²¬")

                # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
                items = data.get('items', [])

                if not items:
                    print(f"  âš ï¸ {page}í˜ì´ì§€ì— ê²°ê³¼ ì—†ìŒ")
                    break

                for item in items:
                    paper_info = self._parse_scienceon_item(item)
                    if paper_info:
                        papers.append(paper_info)

                print(f"  âœ… {page}/{total_pages} í˜ì´ì§€: {len(items)}ê°œ ìˆ˜ì§‘ (ì´ {len(papers)}ê°œ)")

                # Rate limiting
                time.sleep(self.rate_limit)

                # ëª©í‘œ ë‹¬ì„±
                if len(papers) >= max_results:
                    break

            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"âœ… ScienceON ê²€ìƒ‰ ì™„ë£Œ: {len(papers)}ê°œ ìˆ˜ì§‘")
        return papers

    def _parse_scienceon_item(self, item: dict) -> Optional[dict]:
        """ScienceON JSON ì•„ì´í…œì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±"""
        try:
            # ì œëª©
            title = item.get('title', '').strip()
            if not title:
                return None

            # ì´ˆë¡
            abstract = item.get('abstract', '').strip()

            # ì´ˆë¡ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if len(abstract) < 100:
                return None

            # í‚¤ì›Œë“œ
            keywords = item.get('keywords', [])
            if isinstance(keywords, str):
                keywords = [k.strip() for k in keywords.split(',')]

            # ì—°ë„
            year = item.get('year')
            if year:
                try:
                    year = int(year)
                except:
                    year = None

            # ì €ì
            authors_raw = item.get('authors', [])
            authors = []
            if isinstance(authors_raw, list):
                for author in authors_raw[:5]:
                    if isinstance(author, dict):
                        name = author.get('name', '')
                    else:
                        name = str(author)
                    if name:
                        authors.append(name.strip())
            elif isinstance(authors_raw, str):
                authors = [a.strip() for a in authors_raw.split(',')][:5]

            # ì €ë„
            journal = item.get('journal', {})
            if isinstance(journal, dict):
                journal_name = journal.get('title', 'ScienceON')
            else:
                journal_name = str(journal) if journal else 'ScienceON'

            # DOI
            doi = item.get('doi')

            return {
                'title': title,
                'abstract': abstract,
                'keywords': keywords,
                'year': year,
                'authors': authors,
                'journal': journal_name,
                'doi': doi
            }

        except Exception as e:
            print(f"  âš ï¸ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def collect_domain(
        self,
        domain: str,
        queries: List[str],
        target_count: int,
        start_year: int = 2010
    ) -> List[PaperMetadata]:
        """
        íŠ¹ì • ë„ë©”ì¸ì˜ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            domain: ë„ë©”ì¸ ë¶„ë¥˜
            queries: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
            target_count: ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
            start_year: ì‹œì‘ ì—°ë„

        Returns:
            PaperMetadata ë¦¬ìŠ¤íŠ¸
        """
        all_papers = []
        seen_titles = set()

        results_per_query = max(10, target_count // len(queries))
        current_year = datetime.now().year

        for query in queries:
            papers_data = self.search_papers(
                query=query,
                max_results=results_per_query,
                start_year=start_year,
                end_year=current_year,
                language='ko'
            )

            # PaperMetadataë¡œ ë³€í™˜
            for data in papers_data:
                # ì¤‘ë³µ ì²´í¬
                title_normalized = data['title'].lower().strip()
                if title_normalized in seen_titles:
                    continue

                seen_titles.add(title_normalized)

                # PaperMetadata ìƒì„±
                paper = PaperMetadata(
                    domain=domain,
                    language='ko',
                    title=data['title'],
                    abstract=data['abstract'],
                    keywords=data['keywords'],
                    source='ScienceON',
                    year=data['year'],
                    pmid=None,
                    doi=data.get('doi'),
                    authors=data['authors'],
                    journal=data['journal']
                )

                all_papers.append(paper)

            # ëª©í‘œ ë‹¬ì„±
            if len(all_papers) >= target_count:
                break

            # ì¿¼ë¦¬ ê°„ ëŒ€ê¸°
            time.sleep(2)

        return all_papers[:target_count]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸ‡°ğŸ‡· ScienceON API í•œêµ­ì–´ ë…¼ë¬¸ ìˆ˜ì§‘")
    print("=" * 60)
    print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤: 1ì–µ 3780ë§Œ ê±´ ì´ìƒ (2026-01-19 ê¸°ì¤€)")
    print("=" * 60)

    # API í‚¤ ì…ë ¥
    print("\nğŸ“‹ ScienceON API í‚¤ ë°œê¸‰:")
    print("  1. https://scienceon.kisti.re.kr/apigateway/ ì ‘ì†")
    print("  2. íšŒì›ê°€ì… â†’ API ì‚¬ìš© ì‹ ì²­")
    print("  3. ìŠ¹ì¸ í›„ Client ID, Client Secret ë°œê¸‰")
    print("")

    client_id = input("Client IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    client_secret = input("Client Secretì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not client_id or not client_secret:
        print("âŒ Client IDì™€ Client Secretì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = ScienceOnAPICollector(
        client_id=client_id,
        client_secret=client_secret
    )

    # í•œêµ­ì–´ ê²€ìƒ‰ì–´
    KOREAN_DIET_QUERIES = [
        "í•œì‹ ì˜ì–‘",
        "ê¹€ì¹˜ ê±´ê°•",
        "ë°œíš¨ì‹í’ˆ",
        "í•œêµ­ ì‹ìŠµê´€",
        "ì „í†µìŒì‹",
    ]

    BODY_COMPOSITION_QUERIES = [
        "ê·¼ê°ì†Œì¦",
        "ì²´ì„±ë¶„ ë¶„ì„",
        "ê³¨ê²©ê·¼ëŸ‰",
        "ì²´ì§€ë°©",
        "ìƒì²´ì „ê¸°ì €í•­",
    ]

    # í•œêµ­ ì‹ë‹¨ ìˆ˜ì§‘
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 1: í•œêµ­í˜• ì‹ë‹¨ (ëª©í‘œ: 300ê°œ)")
    print("=" * 60)

    korean_diet_papers = collector.collect_domain(
        domain='korean_diet',
        queries=KOREAN_DIET_QUERIES,
        target_count=300,
        start_year=2010
    )

    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 2: ì²´í˜• ë¶„ì„/ì¸ë°”ë”” (ëª©í‘œ: 300ê°œ)")
    print("=" * 60)

    body_comp_papers = collector.collect_domain(
        domain='body_composition',
        queries=BODY_COMPOSITION_QUERIES,
        target_count=300,
        start_year=2010
    )

    # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼
    all_papers = korean_diet_papers + body_comp_papers

    # ê²°ê³¼ ì €ì¥
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ì „ì²´ ì €ì¥
    corpus_path = output_dir / f"scienceon_korean_{timestamp}.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump([p.model_dump() for p in all_papers], f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {corpus_path}")

    # ë„ë©”ì¸ë³„ ì €ì¥
    if korean_diet_papers:
        diet_path = output_dir / f"korean_diet_scienceon_{timestamp}.json"
        with open(diet_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in korean_diet_papers], f, ensure_ascii=False, indent=2)
        print(f"   - í•œêµ­ ì‹ë‹¨: {diet_path} ({len(korean_diet_papers)}ê°œ)")

    if body_comp_papers:
        body_path = output_dir / f"body_composition_scienceon_{timestamp}.json"
        with open(body_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in body_comp_papers], f, ensure_ascii=False, indent=2)
        print(f"   - ì²´í˜• ë¶„ì„: {body_path} ({len(body_comp_papers)}ê°œ)")

    # í†µê³„
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print(f"ì´ ìˆ˜ì§‘: {len(all_papers)}ê°œ")
    print(f"  - í•œêµ­ ì‹ë‹¨: {len(korean_diet_papers)}ê°œ")
    print(f"  - ì²´í˜• ë¶„ì„: {len(body_comp_papers)}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
