"""
KCI OpenAPIë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë…¼ë¬¸ ìë™ ìˆ˜ì§‘

ê³µê³µë°ì´í„°í¬í„¸(data.go.kr)ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬
í•œêµ­í•™ìˆ ì§€ì¸ìš©ìƒ‰ì¸(KCI)ì˜ ë…¼ë¬¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

API í‚¤ ë°œê¸‰:
https://www.data.go.kr/data/3049042/openapi.do
https://www.data.go.kr/data/15085348/openapi.do
"""

import requests
import time
import json
from typing import List, Optional
from datetime import datetime
from pathlib import Path
import xml.etree.ElementTree as ET

from models import PaperMetadata


class KCIAPICollector:
    """KCI OpenAPI ìˆ˜ì§‘ê¸°"""

    def __init__(self, api_key: str):
        """
        Args:
            api_key: ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤
        """
        self.api_key = api_key

        # KCI Open API ì—”ë“œí¬ì¸íŠ¸ (KCI ì§ì ‘)
        self.base_url = "https://open.kci.go.kr/po/openapi/openApiSearch.kci"

        # Rate limiting
        self.rate_limit = 1.0  # 1ì´ˆì— 1ê°œ ìš”ì²­

    def search_papers(
        self,
        query: str,
        max_results: int = 100,
        start_year: Optional[int] = None,
        end_year: Optional[int] = None
    ) -> List[dict]:
        """
        KCIì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´ (í•œêµ­ì–´)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            start_year: ì‹œì‘ ì—°ë„
            end_year: ì¢…ë£Œ ì—°ë„

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        papers = []

        print(f"\nğŸ” KCI ê²€ìƒ‰: '{query}' (ìµœëŒ€ {max_results}ê°œ)")

        # í˜ì´ì§€ë„¤ì´ì…˜ (í•œ ë²ˆì— 100ê°œì”© ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
        page_size = 100
        total_pages = (max_results + page_size - 1) // page_size

        for page in range(1, total_pages + 1):
            try:
                # API ìš”ì²­ íŒŒë¼ë¯¸í„° (KCI ì§ì ‘ API í˜•ì‹)
                params = {
                    'apiCode': 'articleSearch',  # ë…¼ë¬¸ ê²€ìƒ‰
                    'key': self.api_key,  # API í‚¤
                    'keyword': query,  # í‚¤ì›Œë“œ ê²€ìƒ‰ (ì œëª©+ì´ˆë¡)
                    'displayCount': min(page_size, max_results - len(papers)),  # í•œ í˜ì´ì§€ ê²°ê³¼ ìˆ˜
                    'pageNo': page,  # í˜ì´ì§€ ë²ˆí˜¸
                }

                # ì—°ë„ í•„í„° (ìˆëŠ” ê²½ìš°)
                if start_year:
                    params['startYear'] = str(start_year)
                if end_year:
                    params['endYear'] = str(end_year)

                # API ìš”ì²­
                response = requests.get(self.base_url, params=params, timeout=30)

                if response.status_code != 200:
                    print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨ (status: {response.status_code})")
                    print(f"     ì‘ë‹µ: {response.text[:200]}")
                    break

                # XML íŒŒì‹±
                root = ET.fromstring(response.content)

                # ì—ëŸ¬ ì²´í¬ (ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„)
                error_elem = root.find('.//error')
                if error_elem is not None:
                    error_msg = error_elem.text or "Unknown error"
                    print(f"  âš ï¸ API ì—ëŸ¬: {error_msg}")
                    break

                # ì´ ê²°ê³¼ ìˆ˜ í™•ì¸ (ë‹¤ì–‘í•œ íƒœê·¸ ì‹œë„)
                total = None
                for tag in ['.//totalCount', './/total', './/recordCount']:
                    total_elem = root.find(tag)
                    if total_elem is not None and total_elem.text:
                        try:
                            total = int(total_elem.text)
                            if page == 1:
                                print(f"  ğŸ“Š ì´ {total}ê°œ ë…¼ë¬¸ ë°œê²¬")
                            break
                        except:
                            pass

                # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ë‹¤ì–‘í•œ ê²½ë¡œ ì‹œë„)
                items = []
                for path in ['.//records/record', './/items/item', './/list/item', './/record', './/item']:
                    items = root.findall(path)
                    if items:
                        break

                if not items:
                    print(f"  âš ï¸ {page}í˜ì´ì§€ì— ê²°ê³¼ ì—†ìŒ")
                    break

                for item in items:
                    paper_info = self._parse_kci_item(item)
                    if paper_info:
                        papers.append(paper_info)

                print(f"  âœ… {page}/{total_pages} í˜ì´ì§€: {len(items)}ê°œ ìˆ˜ì§‘ (ì´ {len(papers)}ê°œ)")

                # Rate limiting
                time.sleep(self.rate_limit)

                # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
                if len(papers) >= max_results:
                    break

            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"âœ… KCI ê²€ìƒ‰ ì™„ë£Œ: {len(papers)}ê°œ ìˆ˜ì§‘")
        return papers

    def _parse_kci_item(self, item: ET.Element) -> Optional[dict]:
        """KCI XML ì•„ì´í…œì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±"""
        try:
            # ì œëª© (ì—¬ëŸ¬ ê°€ëŠ¥í•œ íƒœê·¸ ì‹œë„)
            title = None
            for tag in ['.//articleTitle', './/title', './/article-title']:
                title_elem = item.find(tag)
                if title_elem is not None and title_elem.text:
                    title = title_elem.text.strip()
                    break

            if not title:
                return None

            # ì´ˆë¡
            abstract = ""
            for tag in ['.//abstract', './/summary']:
                abstract_elem = item.find(tag)
                if abstract_elem is not None and abstract_elem.text:
                    abstract = abstract_elem.text.strip()
                    break

            # ì´ˆë¡ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if len(abstract) < 100:
                return None

            # í‚¤ì›Œë“œ
            keywords = []
            for tag in ['.//keyword', './/keywords']:
                keyword_elem = item.find(tag)
                if keyword_elem is not None and keyword_elem.text:
                    # ì„¸ë¯¸ì½œë¡  ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„
                    kw_text = keyword_elem.text.replace(';', ',')
                    keywords = [k.strip() for k in kw_text.split(',') if k.strip()]
                    break

            # ì—°ë„
            year = None
            for tag in ['.//pubiYr', './/pub-year', './/year']:
                year_elem = item.find(tag)
                if year_elem is not None and year_elem.text:
                    try:
                        year = int(year_elem.text.strip())
                    except:
                        pass
                    break

            # ì €ì
            authors = []
            for tag in ['.//author', './/authors', './/creator']:
                author_elem = item.find(tag)
                if author_elem is not None and author_elem.text:
                    # ì„¸ë¯¸ì½œë¡  ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„
                    author_text = author_elem.text.replace(';', ',')
                    authors = [a.strip() for a in author_text.split(',') if a.strip()][:5]
                    break

            # ì €ë„
            journal = "KCI í•™ìˆ ì§€"
            for tag in ['.//journalTitle', './/journal-title', './/journal']:
                journal_elem = item.find(tag)
                if journal_elem is not None and journal_elem.text:
                    journal = journal_elem.text.strip()
                    break

            # DOI
            doi = None
            doi_elem = item.find('.//doi')
            if doi_elem is not None and doi_elem.text:
                doi = doi_elem.text.strip()

            return {
                'title': title,
                'abstract': abstract,
                'keywords': keywords,
                'year': year,
                'authors': authors,
                'journal': journal,
                'doi': doi
            }

        except Exception as e:
            print(f"  âš ï¸ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def collect_domain(
        self,
        domain: str,
        queries: List[str],
        target_count: int,
        start_year: int = 2010
    ) -> List[PaperMetadata]:
        """
        íŠ¹ì • ë„ë©”ì¸ì˜ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            domain: ë„ë©”ì¸ ë¶„ë¥˜
            queries: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
            target_count: ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
            start_year: ì‹œì‘ ì—°ë„

        Returns:
            PaperMetadata ë¦¬ìŠ¤íŠ¸
        """
        all_papers = []
        seen_titles = set()  # ì¤‘ë³µ ì œê±°

        results_per_query = max(10, target_count // len(queries))
        current_year = datetime.now().year

        for query in queries:
            papers_data = self.search_papers(
                query=query,
                max_results=results_per_query,
                start_year=start_year,
                end_year=current_year
            )

            # PaperMetadataë¡œ ë³€í™˜
            for data in papers_data:
                # ì¤‘ë³µ ì²´í¬
                title_normalized = data['title'].lower().strip()
                if title_normalized in seen_titles:
                    continue

                seen_titles.add(title_normalized)

                # PaperMetadata ìƒì„±
                paper = PaperMetadata(
                    domain=domain,
                    language='ko',
                    title=data['title'],
                    abstract=data['abstract'],
                    keywords=data['keywords'],
                    source='KCI',
                    year=data['year'],
                    pmid=None,
                    doi=data.get('doi'),
                    authors=data['authors'],
                    journal=data['journal']
                )

                all_papers.append(paper)

            # ëª©í‘œ ë‹¬ì„± í™•ì¸
            if len(all_papers) >= target_count:
                break

            # ì¿¼ë¦¬ ê°„ ëŒ€ê¸°
            time.sleep(2)

        return all_papers[:target_count]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸ‡°ğŸ‡· KCI OpenAPI í•œêµ­ì–´ ë…¼ë¬¸ ìˆ˜ì§‘")
    print("=" * 60)

    # API í‚¤ ì…ë ¥
    print("\nğŸ“‹ KCI API í‚¤ ë°œê¸‰:")
    print("  1. https://www.data.go.kr/ íšŒì›ê°€ì…")
    print("  2. 'KCI ë…¼ë¬¸ì •ë³´ì„œë¹„ìŠ¤' ê²€ìƒ‰")
    print("  3. í™œìš©ì‹ ì²­ â†’ API í‚¤ ë°œê¸‰")
    print("")

    api_key = input("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not api_key:
        print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = KCIAPICollector(api_key=api_key)

    # í•œêµ­ì–´ ê²€ìƒ‰ì–´
    BODY_COMPOSITION_QUERIES = [
    "BIA ì²´ì„±ë¶„ ê·¸ë˜í”„ íŒ¨í„´ë³„ ëŒ€ì‚¬ì  ìœ„í—˜ë„ ë¶„ì„"
    "ì²´ì§€ë°©ë¥  ë° ê³¨ê²©ê·¼ëŸ‰ ì§€ìˆ˜(SMI) ê¸°ë°˜ì˜ ì²´í˜• ë¶„ë¥˜ ëª¨ë¸"
    "ì²´í˜•ë¶€ìœ„ë³„ ê·¼ìœ¡ ë¶ˆê· í˜•(Segmental Lean Analysis)ê³¼ ì‹ ì²´ ê¸°ëŠ¥ì˜ ìƒê´€ê´€ê³„"
    "ìƒÂ·í•˜ì²´ ê·¼ìœ¡ëŸ‰ ë¹„ìœ¨ì— ë”°ë¥¸ ê·¼ê°ì†Œì„± ë¹„ë§Œ(Sarcopenic Obesity) íŒì • ê¸°ì¤€"
    "InBody ë°ì´í„°ë¥¼ í™œìš©í•œ ì²´í˜• ì§€ìˆ˜(Body Shape Index) ì‚°ì¶œ ë¡œì§"
    "ì²´ì„±ë¶„ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ë§ì¶¤í˜• ìš´ë™ ê°•ë„(FITT) ì„¤ì • ê·¼ê±°"
    "ê·¼ìœ¡ëŸ‰ ë° ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ ê¸°ë°˜ì˜ ìœ ì‚°ì†ŒÂ·ë¬´ì‚°ì†Œ ìš´ë™ ë°°ë¶„ ì „ëµ"
    "ìš´ë™ ìˆ™ë ¨ë„ë³„ ì²´ì„±ë¶„ ë³€í™” ì–‘ìƒ ë° ì ì • ìš´ë™ ì²˜ë°© ëª¨ë¸"
    "í™ˆ íŠ¸ë ˆì´ë‹ê³¼ íœ˜íŠ¸ë‹ˆìŠ¤ ì„¼í„° ê¸°ë°˜ ìš´ë™ í”„ë¡œê·¸ë¨ì˜ ì²´ì„±ë¶„ ê°œì„  íš¨ê³¼ ë¹„êµ"
    "ì‹¬ë°•ìˆ˜ ë° ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ì„ ê³ ë ¤í•œ ëª©í‘œ ì¹¼ë¡œë¦¬ ì†Œë¹„ëŸ‰ ì‚°ì • ë¡œì§"
    "ë³µë¶€ì§€ë°©ë¥ (WHR) ë° ë‚´ì¥ì§€ë°©ë ˆë²¨ì— ë”°ë¥¸ ê³ ê°•ë„ ì¸í„°ë²Œ íŠ¸ë ˆì´ë‹(HIIT)ì˜ íš¨ê³¼"
    "ì¢Œìš°ì¸¡ ìƒí•˜ì§€ ê·¼ìœ¡ ë¶ˆê· í˜• êµì •ì„ ìœ„í•œ í¸ì¸¡ì„± ìš´ë™(Unilateral Exercise) ì²˜ë°©"
    "ì‹ ì²´ ë¶€ìœ„ë³„ ì²´ì§€ë°© ë¶„í¬ì™€ ì¸ìŠë¦° ì €í•­ì„± ê°„ì˜ ê´€ê³„"
    "ë¬´ê¸°ì§ˆ ë° ë‹¨ë°±ì§ˆ ì„­ì·¨ ìƒíƒœì™€ ê·¼ë ¥ ìš´ë™ íš¨ìœ¨ì˜ ìƒê´€ì„±"
        
    ]

    INBODY_BIA_KR_QUERIES = [
    "ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ì²´ì„±ë¶„ í‰ê°€",
    "ì¸ë°”ë”” ì²´ì„±ë¶„ ë¶„ì„ ì‹ ë¢°ë„",
    "ìƒì²´ì „ê¸°ì €í•­ë²• ê³¨ê²©ê·¼ëŸ‰ ì •í™•ë„",
    "DXAì™€ ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ë¹„êµ",
    "ì²´ì„±ë¶„ ì¸¡ì •ë°©ë²• íƒ€ë‹¹ë„ ì—°êµ¬",
    ]

    BODY_TYPE_CLASSIFICATION_KR_QUERIES = [
    "ì²´ì„±ë¶„ ê¸°ë°˜ ì²´í˜• ë¶„ë¥˜",
    "ì²´ì§€ë°©ëŸ‰ ê³¨ê²©ê·¼ëŸ‰ ì²´í˜• ìœ í˜•",
    "ê·¼ìœ¡-ì§€ë°© ë¶ˆê· í˜• ì²´í˜• ë¶„ì„",
    "ì²´ì„±ë¶„ ì§€í‘œë¥¼ ì´ìš©í•œ êµ°ì§‘ë¶„ì„",
    "ë¹„ë§Œ ìœ í˜• ì²´ì„±ë¶„ phenotype ì—°êµ¬",
    ]

    SARCOPENIA_KR_QUERIES = [
    "ê·¼ê°ì†Œì¦ ê³¨ê²©ê·¼ëŸ‰ ê¸°ì¤€ í•œêµ­ì¸",
    "ì•„ì‹œì•„ ê·¼ê°ì†Œì¦ ì§„ë‹¨ê¸°ì¤€ ìƒì²´ì „ê¸°ì €í•­",
    "ì‚¬ì§€ê³¨ê²©ê·¼ëŸ‰ì§€ìˆ˜(SMI) ì°¸ê³ ì¹˜",
    "ë…¸ì¸ ê·¼ìœ¡ëŸ‰ ê°ì†Œ ì²´ì„±ë¶„ ì—°êµ¬",
    "ê·¼ê°ì†Œì„± ë¹„ë§Œ í•œêµ­ì¸ ìœ ë³‘ë¥ ",
    ]

    BODYFAT_OBESITY_KR_QUERIES = [
    "ì²´ì§€ë°©ë¥  ê¸°ì¤€ í•œêµ­ì¸",
    "BMIì™€ ì²´ì§€ë°©ë¥  ë¹„êµ ì—°êµ¬",
    "ì •ìƒì²´ì¤‘ë¹„ë§Œ ì²´ì„±ë¶„ ë¶„ì„",
    "ë¹„ë§Œë„ í‰ê°€ ì²´ì„±ë¶„ ì§€í‘œ",
    "ì²´ì§€ë°©ëŸ‰ê³¼ ëŒ€ì‚¬ì§ˆí™˜ ìœ„í—˜",
    ]
    
    VISCERAL_FAT_KR_QUERIES = [
    "ë‚´ì¥ì§€ë°© ìˆ˜ì¤€ ëŒ€ì‚¬ì¦í›„êµ° ìœ„í—˜",
    "ë³µë¶€ë¹„ë§Œ ë‚´ì¥ì§€ë°© ì²´ì„±ë¶„ ë¶„ì„",
    "ë‚´ì¥ì§€ë°©ë©´ì ê³¼ ì¸ìŠë¦°ì €í•­ì„±",
    "ì¤‘ì‹¬ì„±ë¹„ë§Œ ê±´ê°•ìœ„í—˜ ì—°êµ¬",
    "ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ë‚´ì¥ì§€ë°© ì¶”ì •",
    ]

    SEGMENTAL_BALANCE_KR_QUERIES = [
    "ë¶€ìœ„ë³„ ê³¨ê²©ê·¼ëŸ‰ ë¶ˆê· í˜• ë¶„ì„",
    "ì‚¬ì§€ ê·¼ìœ¡ëŸ‰ ì¢Œìš° ì°¨ì´",
    "íŒ” ë‹¤ë¦¬ ê·¼ìœ¡ ë¹„ëŒ€ì¹­ ì²´ì„±ë¶„",
    "ë¶€ìœ„ë³„ ì²´ì§€ë°© ë¶„í¬ ì—°êµ¬",
    "êµ­ì†Œ ì²´ì„±ë¶„ ë¶ˆê· í˜• ìš´ë™ì²˜ë°©",
    ]

    EXERCISE_INTERVENTION_KR_QUERIES = [
    "ì €í•­ìš´ë™ ê³¨ê²©ê·¼ëŸ‰ ì¦ê°€ ì²´ì„±ë¶„ ë³€í™”",
    "ìœ ì‚°ì†Œìš´ë™ ë‚´ì¥ì§€ë°© ê°ì†Œ íš¨ê³¼",
    "ë³µí•©ìš´ë™ ì²´ì§€ë°©ë¥  ê°œì„  ì—°êµ¬",
    "ìš´ë™ì¤‘ì¬ ì²´ì„±ë¶„ ê°œì„  í”„ë¡œê·¸ë¨",
    "ìš´ë™ì²˜ë°© ê¸°ë°˜ ì²´ì„±ë¶„ ë¶„ì„",
    ]

    MUSCLE_ADJUSTMENT_KR_QUERIES = [
    "ê·¼ìœ¡ëŸ‰ ì¦ê°€ í”„ë¡œê·¸ë¨ íš¨ê³¼",
    "ë‹¨ë°±ì§ˆ ì„­ì·¨ì™€ ê·¼ìœ¡ëŸ‰ ë³€í™”",
    "ê·¼ê°ì†Œ ì˜ˆë°© ì €í•­ì„±ìš´ë™ ì²˜ë°©",
    "ì œì§€ë°©ëŸ‰ ì¦ê°€ ì¤‘ì¬ì—°êµ¬",
    ]

    BMR_NUTRITION_KR_QUERIES = [
    "ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ê³¼ ì œì§€ë°©ëŸ‰ ê´€ê³„",
    "ì²´ì„±ë¶„ ê¸°ë°˜ ì—ë„ˆì§€ í•„ìš”ëŸ‰ ì¶”ì •",
    "ê¶Œì¥ì„­ì·¨ì—´ëŸ‰ ì‚°ì • ì²´ì„±ë¶„ ì—°êµ¬",
    "ì²´ì¤‘ì¡°ì ˆ í”„ë¡œê·¸ë¨ ëŒ€ì‚¬ëŸ‰ ë³€í™”",
    ]

    METABOLIC_RISK_KR_QUERIES = [
    "ì²´ì„±ë¶„ê³¼ ëŒ€ì‚¬ì¦í›„êµ° ìœ„í—˜",
    "ê³¨ê²©ê·¼ëŸ‰ê³¼ ë‹¹ë‡¨ë³‘ ìœ„í—˜",
    "ë‚´ì¥ì§€ë°©ê³¼ ì‹¬í˜ˆê´€ì§ˆí™˜ ì—°ê´€",
    "ì²´ì„±ë¶„ ì§€í‘œ ê±´ê°•ì˜ˆì¸¡ëª¨ë¸",
    ]

    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 2: ì²´í˜• ë¶„ì„/ì¸ë°”ë”” (ëª©í‘œ: 500ê°œ)")
    print("=" * 60)

    body_comp_papers1 = collector.collect_domain(
        domain='body_composition',
        queries=BODY_COMPOSITION_QUERIES,
        target_count=500,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 3: ì¸ë°”ë””/ì²´ì„±ë¶„ ë¶„ì„ í•µì‹¬ í‚¤ì›Œë“œ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers2 = collector.collect_domain(
        domain='body_composition',
        queries=INBODY_BIA_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 4: ì²´í˜•ë¶„ì„/ì²´ì„±ë¶„ ê¸°ë°˜ ìœ í˜•í™” (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers3 = collector.collect_domain(
        domain='body_composition',
        queries=BODY_TYPE_CLASSIFICATION_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 5: ê·¼ê°ì†Œì¦ + ê·¼ê°ì†Œì„±ë¹„ë§Œ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers4 = collector.collect_domain(
        domain='body_composition',
        queries=SARCOPENIA_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 6: ì²´ì§€ë°©ë¥ Â·ë¹„ë§Œë„Â·BMI í•œê³„ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers5 = collector.collect_domain(
        domain='body_composition',
        queries=BODYFAT_OBESITY_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 7: ë³µë¶€ì§€ë°©ë¥ Â·ë‚´ì¥ì§€ë°© ë ˆë²¨ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers6 = collector.collect_domain(
        domain='body_composition',
        queries=VISCERAL_FAT_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 8: ë¶€ìœ„ë³„ ê·¼ìœ¡/ì§€ë°© ë¶ˆê· í˜• (Segmental) (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers7 = collector.collect_domain(
        domain='body_composition',
        queries=SEGMENTAL_BALANCE_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 9: ìš´ë™ì²˜ë°© ê·¼ê±° (ê·¼ìœ¡ ì¦ê°€/ì§€ë°© ê°ì†Œ) (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers8 = collector.collect_domain(
        domain='body_composition',
        queries=EXERCISE_INTERVENTION_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 10: ê¸°ì´ˆëŒ€ì‚¬ëŸ‰(BMR) + ì—ë„ˆì§€ ì²˜ë°© (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers9 = collector.collect_domain(
        domain='body_composition',
        queries=MUSCLE_ADJUSTMENT_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 11: ê¸°ì´ˆëŒ€ì‚¬ëŸ‰(BMR) + ì—ë„ˆì§€ ì²˜ë°© (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers10 = collector.collect_domain(
        domain='body_composition',
        queries=BMR_NUTRITION_KR_QUERIES,
        target_count=250,
        start_year=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 12: ì²´ì„±ë¶„ê³¼ ëŒ€ì‚¬ì¦í›„êµ° (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers11 = collector.collect_domain(
        domain='body_composition',
        queries=METABOLIC_RISK_KR_QUERIES,
        target_count=250,
        start_year=2010
    )



    # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼
    all_papers = korean_diet_papers + body_comp_papers1 + body_comp_papers2 + body_comp_papers3 + body_comp_papers4 + body_comp_papers5 + body_comp_papers6 + body_comp_papers7 + body_comp_papers8 + body_comp_papers9 + body_comp_papers10 + body_comp_papers11


    # ê²°ê³¼ ì €ì¥
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ì „ì²´ ì €ì¥
    corpus_path = output_dir / f"kci_korean_{timestamp}.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump([p.model_dump() for p in all_papers], f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {corpus_path}")

    # ë„ë©”ì¸ë³„ ì €ì¥
    if korean_diet_papers:
        diet_path = output_dir / f"korean_diet_kci_{timestamp}.json"
        with open(diet_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in korean_diet_papers], f, ensure_ascii=False, indent=2)
        print(f"   - í•œêµ­ ì‹ë‹¨: {diet_path} ({len(korean_diet_papers)}ê°œ)")

    if body_comp_papers:
        body_path = output_dir / f"body_composition_kci_{timestamp}.json"
        with open(body_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in body_comp_papers], f, ensure_ascii=False, indent=2)
        print(f"   - ì²´í˜• ë¶„ì„: {body_path} ({len(body_comp_papers)}ê°œ)")

    # í†µê³„
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print(f"ì´ ìˆ˜ì§‘: {len(all_papers)}ê°œ")
    print(f"  - í•œêµ­ ì‹ë‹¨: {len(korean_diet_papers)}ê°œ")
    print(f"  - ì²´í˜• ë¶„ì„: {len(body_comp_papers)}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
