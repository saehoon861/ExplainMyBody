"""
Graph RAG êµ¬ì¶• íŒŒì´í”„ë¼ì¸

outputs í´ë”ì˜ ë…¼ë¬¸ ì´ˆë¡ì„ ì½ì–´ì„œ Graph RAGë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import json
from pathlib import Path
from typing import List, Dict, Set, Optional
from collections import defaultdict
import os
import time

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸ python-dotenv ì—†ìŒ. .env íŒŒì¼ì„ ì½ìœ¼ë ¤ë©´ pip install python-dotenv ì‹¤í–‰")

# Neo4j (ì„ íƒ) ë˜ëŠ” NetworkX (ê°„ë‹¨)
try:
    import networkx as nx
    USE_NETWORKX = True
except ImportError:
    USE_NETWORKX = False
    print("âš ï¸ NetworkX ì—†ìŒ. pip install networkx ê¶Œì¥")

# OpenAI (í•œêµ­ì–´ ìš”ì•½ ìƒì„±ìš©)
try:
    from openai import OpenAI
    USE_OPENAI = True
except ImportError:
    USE_OPENAI = False
    print("âš ï¸ OpenAI ì—†ìŒ. í•œêµ­ì–´ ìš”ì•½ ìƒì„± ë¶ˆê°€. pip install openai ê¶Œì¥")

# Ollama (ë¡œì»¬ ì„ë² ë”©)
try:
    import ollama
    USE_OLLAMA = True
except ImportError:
    USE_OLLAMA = False
    print("âš ï¸ Ollama ì—†ìŒ. í•œêµ­ì–´ ì„ë² ë”© ìƒì„± ë¶ˆê°€. pip install ollama ê¶Œì¥")


class GraphRAGBuilder:
    """Graph RAG êµ¬ì¶•ê¸°"""

    def __init__(self, schema_path: str = "graph_rag_schema.json",
                 generate_ko_summaries: bool = False,
                 generate_ko_embeddings: bool = False,
                 embedding_provider: str = "openai",
                 openai_api_key: Optional[str] = None,
                 ollama_model: str = "exaone3.5:7.8b",
                 ollama_embedding_model: str = "bge-m3:latest"):
        """
        Args:
            schema_path: ìŠ¤í‚¤ë§ˆ JSON íŒŒì¼ ê²½ë¡œ
            generate_ko_summaries: ì˜ì–´ ë…¼ë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì—¬ë¶€ (ë¡œì»¬ Ollama ì‚¬ìš©)
            generate_ko_embeddings: í•œêµ­ì–´ ìš”ì•½ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± ì—¬ë¶€
            embedding_provider: ì„ë² ë”© ì œê³µì ("openai" ë˜ëŠ” "ollama")
            openai_api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            ollama_model: Ollama ìš”ì•½ ìƒì„± ëª¨ë¸
            ollama_embedding_model: Ollama ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: bge-m3:latest)
        """
        # ìŠ¤í‚¤ë§ˆ ë¡œë“œ
        with open(schema_path, 'r', encoding='utf-8') as f:
            self.schema = json.load(f)['graph_rag_schema']

        # Concept ë”•ì…”ë„ˆë¦¬ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self.concepts = self._build_concept_dict()

        # Graph ì´ˆê¸°í™”
        if USE_NETWORKX:
            self.graph = nx.MultiDiGraph()
        else:
            self.nodes = {}
            self.edges = []

        # ì„ë² ë”© ì„¤ì •
        self.generate_ko_embeddings = generate_ko_embeddings
        self.embedding_provider = embedding_provider.lower()
        self.ollama_embedding_model = ollama_embedding_model
        
        # Ollama ì´ˆê¸°í™” (í•œêµ­ì–´ ìš”ì•½ ìƒì„±ìš© + ì„ë² ë”©ìš©)
        self.generate_ko_summaries = generate_ko_summaries
        self.ollama_model = ollama_model
        self.ollama_available = False
        
        if (generate_ko_summaries or (generate_ko_embeddings and self.embedding_provider == "ollama")) and USE_OLLAMA:
            try:
                # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
                ollama.list()
                self.ollama_available = True
                if generate_ko_summaries:
                    print(f"âœ… Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ìš”ì•½ ëª¨ë¸: {ollama_model})")
                if generate_ko_embeddings and self.embedding_provider == "ollama":
                    print(f"âœ… Ollama ì„ë² ë”© ëª¨ë¸: {ollama_embedding_model}")
            except Exception as e:
                print(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
                print("   Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
                self.generate_ko_summaries = False
                if self.embedding_provider == "ollama":
                    self.generate_ko_embeddings = False

        # OpenAI ì´ˆê¸°í™” (í•œêµ­ì–´ ì„ë² ë”© ìƒì„±ìš©)
        self.openai_client = None
        if generate_ko_embeddings and self.embedding_provider == "openai" and USE_OPENAI:
            api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ì„ë² ë”©: text-embedding-3-small)")
            else:
                print("âš ï¸ OpenAI API í‚¤ ì—†ìŒ. í•œêµ­ì–´ ì„ë² ë”© ìƒì„± ë¹„í™œì„±í™”")
                self.generate_ko_embeddings = False

        print(f"âœ… ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì™„ë£Œ: {len(self.concepts)}ê°œ ê°œë…")

    def _build_concept_dict(self) -> Dict[str, Dict]:
        """ê°œë… ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• (í•œêµ­ì–´/ì˜ì–´/ë™ì˜ì–´ë¡œ ê²€ìƒ‰)"""
        concepts = {}

        for category_name, category_data in self.schema['concept_categories'].items():
            for concept in category_data['concepts']:
                concept_id = concept['id']
                concepts[concept_id] = concept

                # ë™ì˜ì–´ ë§¤í•‘
                concept['search_terms'] = set()
                concept['search_terms'].add(concept['name_ko'].lower())
                concept['search_terms'].add(concept['name_en'].lower())

                for syn in concept.get('synonyms_ko', []):
                    concept['search_terms'].add(syn.lower())
                for syn in concept.get('synonyms_en', []):
                    concept['search_terms'].add(syn.lower())

        return concepts

    def load_papers(self, corpus_path: str) -> List[Dict]:
        """ë…¼ë¬¸ JSON ë¡œë“œ"""
        with open(corpus_path, 'r', encoding='utf-8') as f:
            papers = json.load(f)

        print(f"ğŸ“„ ë…¼ë¬¸ ë¡œë“œ: {len(papers)}ê°œ")
        return papers

    def extract_mentioned_concepts(self, text: str, ko_summary: Optional[str] = None) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ê°œë… ì¶”ì¶œ

        Args:
            text: ë…¼ë¬¸ ì´ˆë¡ (ì›ë³¸)
            ko_summary: í•œêµ­ì–´ ìš”ì•½ (ìˆëŠ” ê²½ìš°)

        Returns:
            [{"concept_id": "muscle_hypertrophy", "confidence": 0.92}, ...]
        """
        # ì›ë³¸ + í•œêµ­ì–´ ìš”ì•½ ê²°í•©
        combined_text = text
        if ko_summary:
            combined_text = f"{text} {ko_summary}"

        text_lower = combined_text.lower()
        mentioned = []

        for concept_id, concept in self.concepts.items():
            # ê²€ìƒ‰ì–´ ë§¤ì¹­
            for term in concept['search_terms']:
                if term in text_lower:
                    # ì‹ ë¢°ë„ ê³„ì‚° (ë‹¨ìˆœ: ë“±ì¥ íšŸìˆ˜ ê¸°ë°˜)
                    count = text_lower.count(term)
                    confidence = min(0.5 + (count * 0.1), 1.0)

                    mentioned.append({
                        'concept_id': concept_id,
                        'concept_name_ko': concept['name_ko'],
                        'concept_name_en': concept['name_en'],
                        'matched_term': term,
                        'count': count,
                        'confidence': confidence
                    })
                    break  # í•˜ë‚˜ë§Œ ë§¤ì¹­ë˜ë©´ ì¶©ë¶„

        return mentioned

    def generate_korean_summary(self, english_abstract: str) -> Optional[str]:
        """
        ì˜ì–´ ì´ˆë¡ì„ í•œêµ­ì–´ë¡œ ìš”ì•½ ìƒì„± (ë¡œì»¬ Ollama ì‚¬ìš©)

        Args:
            english_abstract: ì˜ì–´ ì´ˆë¡

        Returns:
            í•œêµ­ì–´ ìš”ì•½ (ì‹¤íŒ¨ ì‹œ None)
        """
        if not self.generate_ko_summaries:
            return None

        try:
            prompt = f"""ë‹¤ìŒ ì˜ì–´ ë…¼ë¬¸ ì´ˆë¡ì„ ì½ê³  í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ì˜ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
1. ì£¼ìš” ì—°êµ¬ ëª©ì 
2. í•µì‹¬ ê²°ê³¼ (ìˆ«ì/ìˆ˜ì¹˜ í¬í•¨)
3. ì„ìƒì  ì˜ì˜

ì²´ì„±ë¶„, ê·¼ìœ¡, ì˜ì–‘, ìš´ë™ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì •í™•íˆ ë²ˆì—­í•˜ì„¸ìš”.

ë…¼ë¬¸ ì´ˆë¡:
{english_abstract}

í•œêµ­ì–´ ìš”ì•½:"""

            response = ollama.chat(
                model=self.ollama_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                options={
                    "temperature": 0.6,
                    "num_predict": 300
                }
            )

            summary = response['message']['content'].strip()
            time.sleep(0.2)  # Rate limiting (ë¡œì»¬ì´ë¼ ì§§ê²Œ)
            return summary

        except Exception as e:
            print(f"âš ï¸ í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def generate_korean_embedding(self, korean_text: str) -> Optional[List[float]]:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„± (OpenAI ë˜ëŠ” Ollama)

        Args:
            korean_text: í•œêµ­ì–´ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (ì‹¤íŒ¨ ì‹œ None)
        """
        if not self.generate_ko_embeddings:
            return None
        
        # Ollama ì„ë² ë”©
        if self.embedding_provider == "ollama":
            return self.generate_ollama_embedding(korean_text)
        
        # OpenAI ì„ë² ë”©
        elif self.embedding_provider == "openai":
            return self.generate_openai_embedding(korean_text)
        
        return None
    
    def generate_ollama_embedding(self, text: str) -> Optional[List[float]]:
        """
        Ollamaë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± (bge-m3:latest ë“±)

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (ì‹¤íŒ¨ ì‹œ None)
        """
        if not self.ollama_available:
            return None
        
        # í…ìŠ¤íŠ¸ ê²€ì¦
        if not text or not text.strip():
            return None
        
        # í…ìŠ¤íŠ¸ ì •ì œ (ì œì–´ ë¬¸ì ì œê±°)
        clean_text = text.strip()
        if len(clean_text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
            return None

        try:
            response = ollama.embeddings(
                model=self.ollama_embedding_model,
                prompt=clean_text
            )
            embedding = response['embedding']
            
            # NaN ì²´í¬
            if any(isinstance(x, float) and (x != x) for x in embedding):  # x != xëŠ” NaN ì²´í¬
                return None
            
            time.sleep(0.05)  # Rate limiting (ë¡œì»¬ì´ë¼ ì§§ê²Œ)
            return embedding

        except Exception as e:
            # NaN ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ìŠ¤í‚µ (ë„ˆë¬´ ë§ì€ ì¶œë ¥ ë°©ì§€)
            if "NaN" not in str(e):
                print(f"âš ï¸ Ollama ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_openai_embedding(self, text: str) -> Optional[List[float]]:
        """
        OpenAIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (ì‹¤íŒ¨ ì‹œ None)
        """
        if not self.openai_client:
            return None

        # OpenAI text-embedding-3-small ìµœëŒ€ í† í°: 8191
        # ì•ˆì „í•˜ê²Œ ìµœëŒ€ 6000 í† í°ìœ¼ë¡œ ì œí•œ (ëŒ€ëµ 24000ì)
        MAX_CHARS = 24000
        if len(text) > MAX_CHARS:
            text = text[:MAX_CHARS]

        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            embedding = response.data[0].embedding
            time.sleep(0.1)  # Rate limiting
            return embedding

        except Exception as e:
            print(f"âš ï¸ OpenAI ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def add_paper_node(self, paper: Dict, paper_id: str):
        """ë…¼ë¬¸ ë…¸ë“œ ì¶”ê°€ (ì˜ì–´ ë…¼ë¬¸ì¸ ê²½ìš° í•œêµ­ì–´ ìš”ì•½ ìƒì„± ë° ì„ë² ë”©)"""
        abstract = paper.get('abstract', '')
        lang = paper.get('language', 'unknown')
        
        # abstract ê²€ì¦
        if not abstract or not isinstance(abstract, str):
            abstract = paper.get('title', '')  # fallback to title

        # í•œêµ­ì–´ ìš”ì•½ ìƒì„± (ì˜ì–´ ë…¼ë¬¸ë§Œ, ë¡œì»¬ Ollama)
        chunk_ko_summary = None
        if self.generate_ko_summaries and lang == 'en' and abstract:
            chunk_ko_summary = self.generate_korean_summary(abstract)

        # í•œêµ­ì–´ ì„ë² ë”© ìƒì„± (í•œêµ­ì–´ ìš”ì•½ì´ ìˆëŠ” ê²½ìš°, ë˜ëŠ” í•œêµ­ì–´ ë…¼ë¬¸)
        embedding_ko = None
        if self.generate_ko_embeddings:
            # í•œêµ­ì–´ ìš”ì•½ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ abstract ì‚¬ìš©
            text_to_embed = chunk_ko_summary if chunk_ko_summary else abstract
            if text_to_embed and text_to_embed.strip():
                embedding_ko = self.generate_korean_embedding(text_to_embed)

        node_data = {
            'node_type': 'paper',
            'id': paper_id,
            'title': paper['title'],
            'chunk_text': abstract,  # ì›ë³¸ ì´ˆë¡
            'lang': lang,
            'chunk_ko_summary': chunk_ko_summary,  # í•œêµ­ì–´ ìš”ì•½
            'embedding_ko': embedding_ko,  # í•œêµ­ì–´ ì„ë² ë”© (OpenAI ë˜ëŠ” Ollama)
            'embedding_provider': self.embedding_provider if embedding_ko else None,
            'domain': paper['domain'],
            'source': paper['source'],
            'year': paper.get('year'),
            'pmid': paper.get('pmid'),
            'doi': paper.get('doi')
        }

        if USE_NETWORKX:
            self.graph.add_node(paper_id, **node_data)
        else:
            self.nodes[paper_id] = node_data

    def add_concept_node(self, concept_id: str):
        """ê°œë… ë…¸ë“œ ì¶”ê°€ (concept_type í¬í•¨)"""
        if concept_id not in self.concepts:
            return

        concept = self.concepts[concept_id]
        # concept_typeì€ ì´ë¯¸ concept dictì— í¬í•¨ë˜ì–´ ìˆìŒ

        if USE_NETWORKX:
            if not self.graph.has_node(concept_id):
                self.graph.add_node(concept_id, **concept, node_type='concept')
        else:
            if concept_id not in self.nodes:
                self.nodes[concept_id] = {**concept, 'node_type': 'concept'}

    def add_edge(self, source_id: str, target_id: str, edge_type: str, **properties):
        """ì¼ë°˜ì ì¸ ì—£ì§€ ì¶”ê°€ (ëª¨ë“  ê´€ê³„ íƒ€ì… ì§€ì›)"""
        edge_data = {
            'type': edge_type,
            **properties
        }

        if USE_NETWORKX:
            self.graph.add_edge(source_id, target_id, **edge_data)
        else:
            self.edges.append({
                'source': source_id,
                'target': target_id,
                **edge_data
            })
    
    def add_mentions_edge(self, paper_id: str, mention: Dict):
        """MENTIONS ê´€ê³„ ì¶”ê°€"""
        concept_id = mention['concept_id']
        self.add_edge(
            paper_id, 
            concept_id, 
            'MENTIONS',
            confidence=mention['confidence'],
            matched_term=mention['matched_term'],
            count=mention['count']
        )
    
    def add_correlates_with_edge(self, concept_id1: str, concept_id2: str, correlation: float, direction: str = "bidirectional"):
        """CORRELATES_WITH ê´€ê³„ ì¶”ê°€ (ê°œë… ê°„ ìƒê´€ê´€ê³„)"""
        self.add_edge(
            concept_id1,
            concept_id2,
            'CORRELATES_WITH',
            correlation=correlation,
            direction=direction
        )
    
    def add_similar_to_edge(self, paper_id1: str, paper_id2: str, similarity_score: float, similarity_type: str = "content"):
        """SIMILAR_TO ê´€ê³„ ì¶”ê°€ (ë…¼ë¬¸ ê°„ ìœ ì‚¬ë„)"""
        self.add_edge(
            paper_id1,
            paper_id2,
            'SIMILAR_TO',
            similarity_score=similarity_score,
            similarity_type=similarity_type
        )
    
    def add_increases_edge(self, intervention_id: str, target_id: str, magnitude: float = 0.5, evidence_level: str = "moderate"):
        """INCREASES ê´€ê³„ ì¶”ê°€ (Intervention â†’ Biomarker/Outcome)"""
        self.add_edge(
            intervention_id,
            target_id,
            'INCREASES',
            magnitude=magnitude,
            evidence_level=evidence_level
        )
    
    def add_supports_edge(self, intervention_id: str, target_id: str, evidence_level: str = "moderate"):
        """SUPPORTS ê´€ê³„ ì¶”ê°€ (Intervention â†’ Biomarker/Outcome)"""
        self.add_edge(
            intervention_id,
            target_id,
            'SUPPORTS',
            evidence_level=evidence_level
        )
    
    def add_reduces_edge(self, intervention_id: str, target_id: str, magnitude: float = 0.5, evidence_level: str = "moderate"):
        """REDUCES ê´€ê³„ ì¶”ê°€ (Intervention â†’ Biomarker/Outcome)"""
        self.add_edge(
            intervention_id,
            target_id,
            'REDUCES',
            magnitude=magnitude,
            evidence_level=evidence_level
        )
    
    def add_plan_node(self, plan_id: str, plan_type: str, target_concept: str, **plan_details):
        """ìš´ë™ ì²˜ë°©(Plan) ë…¸ë“œ ì¶”ê°€"""
        node_data = {
            'node_type': 'plan',
            'id': plan_id,
            'plan_type': plan_type,
            'target_concept': target_concept,
            **plan_details
        }
        
        if USE_NETWORKX:
            self.graph.add_node(plan_id, **node_data)
        else:
            self.nodes[plan_id] = node_data

    def build_from_papers(self, papers: List[Dict]) -> Dict:
        """
        ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ê·¸ë˜í”„ êµ¬ì¶•

        Returns:
            í†µê³„ ì •ë³´
        """
        stats = {
            'total_papers': len(papers),
            'papers_with_concepts': 0,
            'total_concepts_found': 0,
            'total_mentions': 0,
            'concepts_by_domain': defaultdict(set),
            'english_papers': 0,
            'korean_papers': 0,
            'ko_summaries_generated': 0,
            'ko_embeddings_generated': 0,
            # ê´€ê³„ íƒ€ì…ë³„ í†µê³„
            'relationships': {
                'MENTIONS': 0,
                'CORRELATES_WITH': 0,
                'SIMILAR_TO': 0,
                'INCREASES': 0,
                'SUPPORTS': 0,
                'REDUCES': 0,
                'AFFECTS': 0,
                'REQUIRES': 0,
                'CONTRADICTS': 0
            },
            # Concept íƒ€ì…ë³„ í†µê³„
            'concept_types': {
                'Outcome': 0,
                'Intervention': 0,
                'Biomarker': 0,
                'Disease': 0,
                'Measurement': 0,
                'Unknown': 0
            }
        }

        print("\nğŸ”¨ ê·¸ë˜í”„ êµ¬ì¶• ì‹œì‘...")
        if self.generate_ko_summaries:
            print(f"   ğŸ“ ì˜ì–´ ë…¼ë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ ìš”ì•½ ìƒì„± í™œì„±í™” (ë¡œì»¬ Ollama: {self.ollama_model})")
        if self.generate_ko_embeddings:
            if self.embedding_provider == "ollama":
                print(f"   ğŸ§® í•œêµ­ì–´ ì„ë² ë”© ìƒì„± í™œì„±í™” (Ollama: {self.ollama_embedding_model})")
            else:
                print(f"   ğŸ§® í•œêµ­ì–´ ì„ë² ë”© ìƒì„± í™œì„±í™” (OpenAI: text-embedding-3-small)")

        for i, paper in enumerate(papers):
            # PMIDê°€ Noneì´ê±°ë‚˜ ì—†ìœ¼ë©´ DOI ë˜ëŠ” index ì‚¬ìš© (í•œêµ­ì–´ ë…¼ë¬¸ ëŒ€ì‘)
            pmid = paper.get('pmid') or paper.get('doi') or f"idx_{i}"
            paper_id = f"paper_{pmid}"
            lang = paper['language']

            # í†µê³„
            if lang == 'en':
                stats['english_papers'] += 1
            elif lang == 'ko':
                stats['korean_papers'] += 1

            # ë…¼ë¬¸ ë…¸ë“œ ì¶”ê°€ (í•œêµ­ì–´ ìš”ì•½ ìƒì„± í¬í•¨)
            self.add_paper_node(paper, paper_id)

            # í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì—¬ë¶€ í™•ì¸
            if USE_NETWORKX:
                node_data = self.graph.nodes[paper_id]
            else:
                node_data = self.nodes[paper_id]

            ko_summary = node_data.get('chunk_ko_summary')
            if ko_summary:
                stats['ko_summaries_generated'] += 1

            ko_embedding = node_data.get('embedding_ko')
            if ko_embedding:
                stats['ko_embeddings_generated'] += 1

            # ê°œë… ì¶”ì¶œ (í•œêµ­ì–´ ìš”ì•½ í¬í•¨)
            mentioned = self.extract_mentioned_concepts(paper['abstract'], ko_summary)

            if mentioned:
                stats['papers_with_concepts'] += 1
                stats['total_mentions'] += len(mentioned)

            # ê°œë… ë…¸ë“œ ë° ê´€ê³„ ì¶”ê°€
            mentioned_concept_ids = []
            for mention in mentioned:
                concept_id = mention['concept_id']
                mentioned_concept_ids.append(concept_id)

                # ê°œë… ë…¸ë“œ ì¶”ê°€
                self.add_concept_node(concept_id)
                
                # Concept íƒ€ì… í†µê³„
                concept_type = self.concepts[concept_id].get('concept_type', 'Unknown')
                stats['concept_types'][concept_type] = stats['concept_types'].get(concept_type, 0) + 1

                # MENTIONS ê´€ê³„ ì¶”ê°€
                self.add_mentions_edge(paper_id, mention)
                stats['relationships']['MENTIONS'] += 1

                # í†µê³„
                stats['concepts_by_domain'][paper['domain']].add(concept_id)
            
            # CORRELATES_WITH ê´€ê³„ ì¶”ê°€ (ê°™ì€ ë…¼ë¬¸ì— ì–¸ê¸‰ëœ ê°œë…ë“¤ ê°„)
            if len(mentioned_concept_ids) >= 2:
                for j in range(len(mentioned_concept_ids)):
                    for k in range(j + 1, len(mentioned_concept_ids)):
                        concept_id1 = mentioned_concept_ids[j]
                        concept_id2 = mentioned_concept_ids[k]
                        
                        # ìƒê´€ê´€ê³„ ì ìˆ˜: ê°™ì€ ë…¼ë¬¸ì— í•¨ê»˜ ë“±ì¥ = 0.7 (ê¸°ë³¸ê°’)
                        correlation = 0.7
                        
                        # CORRELATES_WITH ê´€ê³„ ì¶”ê°€ (ì–‘ë°©í–¥)
                        self.add_correlates_with_edge(concept_id1, concept_id2, correlation)
                        stats['relationships']['CORRELATES_WITH'] += 1

            if (i + 1) % 100 == 0:
                current_total_rels = sum(stats['relationships'].values())
                print(f"  ì²˜ë¦¬: {i + 1}/{len(papers)}ê°œ (í•œêµ­ì–´ ìš”ì•½: {stats['ko_summaries_generated']}ê°œ, ê´€ê³„: {current_total_rels}ê°œ)")

        # Concept ê°„ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê´€ê³„ ì¶”ê°€
        print("\nğŸ”— Concept ê°„ ê´€ê³„ ì¶”ê°€ ì¤‘...")
        concept_relationships = self.schema.get('concept_relationships', {}).get('rules', [])
        
        for rule in concept_relationships:
            source_id = rule['source']
            target_id = rule['target']
            relation = rule['relation']
            
            # ë‘ conceptê°€ ëª¨ë‘ ê·¸ë˜í”„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if USE_NETWORKX:
                has_source = self.graph.has_node(source_id)
                has_target = self.graph.has_node(target_id)
            else:
                has_source = source_id in self.nodes
                has_target = target_id in self.nodes
            
            if has_source and has_target:
                if relation == 'INCREASES':
                    self.add_increases_edge(source_id, target_id)
                    stats['relationships']['INCREASES'] += 1
                elif relation == 'SUPPORTS':
                    self.add_supports_edge(source_id, target_id)
                    stats['relationships']['SUPPORTS'] += 1
                elif relation == 'REDUCES':
                    self.add_reduces_edge(source_id, target_id)
                    stats['relationships']['REDUCES'] += 1
        
        print(f"   âœ… Concept ê´€ê³„ ì¶”ê°€ ì™„ë£Œ: INCREASES({stats['relationships']['INCREASES']}), SUPPORTS({stats['relationships']['SUPPORTS']}), REDUCES({stats['relationships']['REDUCES']})")

        stats['unique_concepts'] = len(set().union(*stats['concepts_by_domain'].values()))
        
        # ì „ì²´ ê´€ê³„ ìˆ˜ ê³„ì‚°
        stats['total_relationships'] = sum(stats['relationships'].values())

        print(f"\nâœ… ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   ğŸ“„ ë…¼ë¬¸ ë…¸ë“œ: {stats['total_papers']}ê°œ")
        print(f"     â€¢ ì˜ì–´: {stats['english_papers']}ê°œ")
        print(f"     â€¢ í•œêµ­ì–´: {stats['korean_papers']}ê°œ")
        if self.generate_ko_summaries:
            print(f"     â€¢ í•œêµ­ì–´ ìš”ì•½ ìƒì„±: {stats['ko_summaries_generated']}ê°œ")
        if self.generate_ko_embeddings:
            print(f"     â€¢ í•œêµ­ì–´ ì„ë² ë”© ìƒì„±: {stats['ko_embeddings_generated']}ê°œ")
        print(f"   ğŸ”— ê°œë… ë°œê²¬: {stats['unique_concepts']}ê°œ")
        
        # Concept íƒ€ì…ë³„ í†µê³„
        print(f"   ğŸ“Š Concept íƒ€ì…ë³„:")
        for c_type, count in stats['concept_types'].items():
            if count > 0:
                print(f"     â€¢ {c_type}: {count}ê°œ")
        
        print(f"   ğŸ•¸ï¸  ê´€ê³„ (Relationships): {stats['total_relationships']}ê°œ")
        
        # ê´€ê³„ íƒ€ì…ë³„ ìƒì„¸ í†µê³„
        for rel_type, count in stats['relationships'].items():
            if count > 0:
                icon = "âœ…" if count > 0 else "âšª"
                print(f"     {icon} {rel_type}: {count}ê°œ")
        
        # 0ê°œì¸ ê´€ê³„ íƒ€ì… í‘œì‹œ
        zero_rels = [rel_type for rel_type, count in stats['relationships'].items() if count == 0]
        if zero_rels:
            print(f"     âš ï¸  ë¯¸êµ¬í˜„: {', '.join(zero_rels)}")

        return stats

    def export_to_json(self, output_path: str):
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if USE_NETWORKX:
            from networkx.readwrite import json_graph
            data = json_graph.node_link_data(self.graph)
        else:
            data = {
                'nodes': list(self.nodes.values()),
                'edges': self.edges
            }
        
        # setì„ listë¡œ ë³€í™˜ (JSON serializableí•˜ê²Œ)
        def convert_sets(obj):
            """ì¬ê·€ì ìœ¼ë¡œ setì„ listë¡œ ë³€í™˜"""
            if isinstance(obj, set):
                return list(obj)
            elif isinstance(obj, dict):
                return {k: convert_sets(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_sets(item) for item in obj]
            else:
                return obj
        
        data = convert_sets(data)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

    def export_to_neo4j_cypher(self, output_path: str):
        """Neo4j Cypher ìŠ¤í¬ë¦½íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°"""
        cypher_lines = []

        # ê°œë… ë…¸ë“œ ìƒì„± (concept_type ë ˆì´ë¸” í¬í•¨)
        cypher_lines.append("// Concept Nodes (by type)")
        
        # Conceptì„ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        concepts_by_type = {}
        for concept_id, concept in self.concepts.items():
            c_type = concept.get('concept_type', 'Unknown')
            if c_type not in concepts_by_type:
                concepts_by_type[c_type] = []
            concepts_by_type[c_type].append((concept_id, concept))
        
        # íƒ€ì…ë³„ë¡œ ë…¸ë“œ ìƒì„±
        for c_type, concepts in concepts_by_type.items():
            cypher_lines.append(f"\n// {c_type} Concepts ({len(concepts)}ê°œ)")
            for concept_id, concept in concepts:
                props = {
                    'id': concept_id,
                    'name_ko': concept['name_ko'],
                    'name_en': concept['name_en'],
                    'importance': concept.get('importance', 0.5)
                }
                props_str = ', '.join([f"{k}: '{v}'" if isinstance(v, str) else f"{k}: {v}"
                                       for k, v in props.items()])
                # Conceptì™€ êµ¬ì²´ì  íƒ€ì… ëª¨ë‘ ë ˆì´ë¸”ë¡œ ì¶”ê°€
                cypher_lines.append(f"CREATE (c{concept_id}:Concept:{c_type} {{{props_str}}});")

        cypher_lines.append("\n// Paper Nodes")
        if USE_NETWORKX:
            for node_id, node_data in self.graph.nodes(data=True):
                if node_data.get('node_type') == 'paper':
                    props_str = f"id: '{node_id}', title: '{node_data['title'][:100]}'"
                    cypher_lines.append(f"CREATE (p{node_id.replace('-', '_')}:Paper {{{props_str}}});")

            # ê´€ê³„ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
            cypher_lines.append("\n// Relationships")
            
            relationships_by_type = {}
            for source, target, edge_data in self.graph.edges(data=True):
                rel_type = edge_data.get('type', 'UNKNOWN')
                if rel_type not in relationships_by_type:
                    relationships_by_type[rel_type] = []
                relationships_by_type[rel_type].append((source, target, edge_data))
            
            # ê° ê´€ê³„ íƒ€ì…ë³„ë¡œ ì¶œë ¥
            for rel_type, edges in relationships_by_type.items():
                cypher_lines.append(f"\n// {rel_type} Relationships ({len(edges)}ê°œ)")
                
                for source, target, edge_data in edges:
                    source_safe = source.replace('-', '_')
                    target_safe = target.replace('-', '_')
                    
                    # ê´€ê³„ë³„ ì†ì„± ì²˜ë¦¬
                    props = {k: v for k, v in edge_data.items() if k != 'type'}
                    props_str = ', '.join([f"{k}: {v}" if isinstance(v, (int, float)) else f"{k}: '{v}'"
                                          for k, v in props.items()])
                    props_clause = f" {{{props_str}}}" if props_str else ""
                    
                    # ë…¸ë“œ íƒ€ì… ì¶”ë¡ 
                    if rel_type == 'MENTIONS':
                        cypher_lines.append(
                            f"MATCH (p:Paper {{id: '{source}'}}), (c:Concept {{id: '{target}'}}) "
                            f"CREATE (p)-[:{rel_type}{props_clause}]->(c);"
                        )
                    elif rel_type == 'CORRELATES_WITH':
                        cypher_lines.append(
                            f"MATCH (c1:Concept {{id: '{source}'}}), (c2:Concept {{id: '{target}'}}) "
                            f"CREATE (c1)-[:{rel_type}{props_clause}]->(c2);"
                        )
                    elif rel_type in ['INCREASES', 'SUPPORTS', 'REDUCES']:
                        cypher_lines.append(
                            f"MATCH (i:Intervention {{id: '{source}'}}), (t:Concept {{id: '{target}'}}) "
                            f"CREATE (i)-[:{rel_type}{props_clause}]->(t);"
                        )
                    elif rel_type == 'SIMILAR_TO':
                        cypher_lines.append(
                            f"MATCH (p1:Paper {{id: '{source}'}}), (p2:Paper {{id: '{target}'}}) "
                            f"CREATE (p1)-[:{rel_type}{props_clause}]->(p2);"
                        )
                    else:
                        # ê¸°íƒ€ ê´€ê³„ íƒ€ì…
                        cypher_lines.append(
                            f"MATCH (n1 {{id: '{source}'}}), (n2 {{id: '{target}'}}) "
                            f"CREATE (n1)-[:{rel_type}{props_clause}]->(n2);"
                        )

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(cypher_lines))

        print(f"ğŸ’¾ Neo4j Cypher ì €ì¥: {output_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys

    print("=" * 60)
    print("ğŸ•¸ï¸ Graph RAG êµ¬ì¶• íŒŒì´í”„ë¼ì¸")
    print("=" * 60)

    # ì˜µì…˜ íŒŒì‹±
    generate_ko_summary = '--ko-summary' in sys.argv
    generate_ko_embedding = '--ko-embedding' in sys.argv

    # ì„ë² ë”© ì œê³µì ì„ íƒ (ê¸°ë³¸: openai)
    embedding_provider = "openai"
    for arg in sys.argv:
        if arg.startswith('--embedding-provider='):
            embedding_provider = arg.split('=')[1]
    
    # Ollama ëª¨ë¸ ì§€ì •
    ollama_model = "qwen3:14b"
    for arg in sys.argv:
        if arg.startswith('--ollama-model='):
            ollama_model = arg.split('=')[1]
    
    # Ollama ì„ë² ë”© ëª¨ë¸ ì§€ì • (ê¸°ë³¸: bge-m3:latest)
    ollama_embedding_model = "bge-m3:latest"
    for arg in sys.argv:
        if arg.startswith('--ollama-embedding-model='):
            ollama_embedding_model = arg.split('=')[1]
    
    # ë…¼ë¬¸ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    limit = None
    for arg in sys.argv:
        if arg.startswith('--limit='):
            try:
                limit = int(arg.split('=')[1])
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²˜ìŒ {limit}ê°œ ë…¼ë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            except ValueError:
                print(f"âš ï¸ --limit ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì „ì²´ ë…¼ë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # 1. ë¹Œë” ì´ˆê¸°í™”
    builder = GraphRAGBuilder(
        schema_path="graph_rag_schema.json",
        generate_ko_summaries=generate_ko_summary,
        generate_ko_embeddings=generate_ko_embedding,
        embedding_provider=embedding_provider,
        ollama_model=ollama_model,
        ollama_embedding_model=ollama_embedding_model
    )

    # 2. ë…¼ë¬¸ ë¡œë“œ
    output_dir = Path("outputs")
    corpus_files = list(output_dir.glob("ragdb_final_corpus_*.json"))

    if not corpus_files:
        print("âŒ ë…¼ë¬¸ ì½”í¼ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € merge_korean_corpus.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    latest_corpus = sorted(corpus_files)[-1]
    papers = builder.load_papers(str(latest_corpus))
    
    # ë…¼ë¬¸ ê°œìˆ˜ ì œí•œ ì ìš©
    if limit and limit > 0:
        papers = papers[:limit]
        print(f"âœ‚ï¸ ì œí•œ ì ìš©: {len(papers)}ê°œ ë…¼ë¬¸ ì„ íƒ")

    # 3. ê·¸ë˜í”„ êµ¬ì¶•
    stats = builder.build_from_papers(papers)

    # 4. ë‚´ë³´ë‚´ê¸°
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê³ ìœ í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë…¼ë¬¸ ê°œìˆ˜ë„ íŒŒì¼ëª…ì— í¬í•¨
    paper_count = len(papers)
    
    # JSON í˜•ì‹
    json_output = output_dir / f"graph_rag_{paper_count}papers_{timestamp}.json"
    builder.export_to_json(str(json_output))

    # Neo4j Cypher ìŠ¤í¬ë¦½íŠ¸
    cypher_output = output_dir / f"graph_rag_neo4j_{paper_count}papers_{timestamp}.cypher"
    builder.export_to_neo4j_cypher(str(cypher_output))

    # 5. í†µê³„ ì €ì¥
    stats_output = output_dir / f"graph_rag_stats_{paper_count}papers_{timestamp}.json"
    with open(stats_output, 'w', encoding='utf-8') as f:
        # defaultdictë¥¼ ì¼ë°˜ dictë¡œ ë³€í™˜
        stats['concepts_by_domain'] = {
            domain: list(concepts)
            for domain, concepts in stats['concepts_by_domain'].items()
        }
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“Š í†µê³„ ì €ì¥: {stats_output}")

    print("\n" + "=" * 60)
    print("âœ… Graph RAG êµ¬ì¶• ì™„ë£Œ!")
    print("=" * 60)
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"1. JSON ì‚¬ìš©: {json_output}")
    print(f"2. Neo4j ì‚¬ìš©: {cypher_output} íŒŒì¼ì„ Neo4jì— ì„í¬íŠ¸")
    print(f"\nğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜:")
    print(f"   --ko-summary                      í•œêµ­ì–´ ìš”ì•½ ìƒì„± (ë¡œì»¬ Ollama)")
    print(f"   --ko-embedding                    í•œêµ­ì–´ ì„ë² ë”© ìƒì„±")
    print(f"   --embedding-provider=PROVIDER     ì„ë² ë”© ì œê³µì (ê¸°ë³¸: openai, ì˜µì…˜: ollama)")
    print(f"   --ollama-model=MODEL              Ollama ìš”ì•½ ëª¨ë¸ ì§€ì • (ê¸°ë³¸: qwen3:14b)")
    print(f"   --ollama-embedding-model=MODEL    Ollama ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: bge-m3:latest)")
    print(f"   --limit=N                         ì²˜ë¦¬í•  ë…¼ë¬¸ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)")
    print(f"\n   ì˜ˆì‹œ:")
    print(f"   # í…ŒìŠ¤íŠ¸: 50ê°œë§Œ ë¹ ë¥´ê²Œ (ì„ë² ë”© ì—†ìŒ)")
    print(f"   python build_graph_rag.py --limit=50")
    print(f"")
    print(f"   # í…ŒìŠ¤íŠ¸: 50ê°œ + Ollama ì„ë² ë”©")
    print(f"   python build_graph_rag.py --ko-embedding --embedding-provider=ollama --limit=50")
    print(f"")
    print(f"   # OpenAI ì„ë² ë”© (ì „ì²´)")
    print(f"   python build_graph_rag.py --ko-summary --ko-embedding")
    print(f"")
    print(f"   # Ollama bge-m3:latest ì„ë² ë”© (ì „ì²´, ë¡œì»¬)")
    print(f"   python build_graph_rag.py --ko-summary --ko-embedding --embedding-provider=ollama")
    print(f"")
    print(f"   # ë‹¤ë¥¸ Ollama ì„ë² ë”© ëª¨ë¸")
    print(f"   python build_graph_rag.py --ko-embedding --embedding-provider=ollama --ollama-embedding-model=nomic-embed-text")


if __name__ == "__main__":
    main()
