"""
í•œêµ­ì–´ ì½”í¼ìŠ¤ ë³‘í•© ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ í•œêµ­ì–´ ë…¼ë¬¸ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ê³ 
ê¸°ì¡´ PubMed ì˜ì–´ ë…¼ë¬¸ê³¼ í†µí•©í•©ë‹ˆë‹¤.
"""

import json
from typing import List, Dict
from pathlib import Path
from datetime import datetime
from difflib import SequenceMatcher

from models import PaperMetadata, CollectionStats


class KoreanCorpusMerger:
    """í•œêµ­ì–´ ì½”í¼ìŠ¤ ë³‘í•©ê¸°"""

    def __init__(self, similarity_threshold: float = 0.85):
        """
        Args:
            similarity_threshold: ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
        """
        self.similarity_threshold = similarity_threshold

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

    def is_duplicate(self, paper: PaperMetadata, existing_papers: List[PaperMetadata]) -> bool:
        """
        ê¸°ì¡´ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ ì—¬ë¶€ í™•ì¸

        Args:
            paper: í™•ì¸í•  ë…¼ë¬¸
            existing_papers: ê¸°ì¡´ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¤‘ë³µì´ë©´ True
        """
        for existing in existing_papers:
            # PMIDê°€ ìˆê³  ê°™ìœ¼ë©´ ì¤‘ë³µ
            if paper.pmid and existing.pmid and paper.pmid == existing.pmid:
                return True

            # DOIê°€ ìˆê³  ê°™ìœ¼ë©´ ì¤‘ë³µ
            if paper.doi and existing.doi and paper.doi == existing.doi:
                return True

            # ì œëª© ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ê²€ì‚¬
            similarity = self.calculate_similarity(paper.title, existing.title)
            if similarity >= self.similarity_threshold:
                return True

        return False

    def merge_papers(
        self,
        paper_lists: List[List[PaperMetadata]],
        source_names: List[str]
    ) -> tuple[List[PaperMetadata], Dict]:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ë…¼ë¬¸ì„ ë³‘í•©í•˜ê³  ì¤‘ë³µ ì œê±°

        Args:
            paper_lists: ì†ŒìŠ¤ë³„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ë“¤
            source_names: ê° ë¦¬ìŠ¤íŠ¸ì˜ ì†ŒìŠ¤ëª…

        Returns:
            (ë³‘í•©ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸, í†µê³„ ë”•ì…”ë„ˆë¦¬)
        """
        merged = []
        stats = {
            'total_input': 0,
            'total_output': 0,
            'duplicates_removed': 0,
            'by_source': {}
        }

        for papers, source_name in zip(paper_lists, source_names):
            stats['total_input'] += len(papers)
            added_count = 0

            print(f"\nğŸ“Š ì²˜ë¦¬ ì¤‘: {source_name} ({len(papers)}ê°œ)")

            for paper in papers:
                if not self.is_duplicate(paper, merged):
                    merged.append(paper)
                    added_count += 1
                else:
                    stats['duplicates_removed'] += 1

            stats['by_source'][source_name] = {
                'input': len(papers),
                'added': added_count,
                'duplicates': len(papers) - added_count
            }

            print(f"  âœ… {added_count}ê°œ ì¶”ê°€ (ì¤‘ë³µ {len(papers) - added_count}ê°œ ì œê±°)")

        stats['total_output'] = len(merged)

        return merged, stats

    def load_json_file(self, json_path: str) -> List[PaperMetadata]:
        """JSON íŒŒì¼ì—ì„œ PaperMetadata ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            papers = [PaperMetadata(**item) for item in data]
            print(f"  ğŸ“¥ ë¡œë“œ ì™„ë£Œ: {json_path} ({len(papers)}ê°œ)")
            return papers

        except Exception as e:
            print(f"  âŒ ë¡œë“œ ì‹¤íŒ¨ {json_path}: {e}")
            return []

    def create_final_stats(self, papers: List[PaperMetadata]) -> CollectionStats:
        """ìµœì¢… í†µê³„ ìƒì„±"""

        stats = CollectionStats()
        stats.total_collected = len(papers)

        for paper in papers:
            # ë„ë©”ì¸ë³„
            stats.by_domain[paper.domain] = stats.by_domain.get(paper.domain, 0) + 1

            # ì–¸ì–´ë³„
            stats.by_language[paper.language] = stats.by_language.get(paper.language, 0) + 1

            # ì†ŒìŠ¤ë³„
            stats.by_source[paper.source] = stats.by_source.get(paper.source, 0) + 1

        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸ”€ í•œêµ­ì–´ ì½”í¼ìŠ¤ ë³‘í•© ì‹œì‘")
    print("=" * 60)

    merger = KoreanCorpusMerger(similarity_threshold=0.85)

    # outputs í´ë”ì—ì„œ ìˆ˜ì§‘ëœ íŒŒì¼ë“¤ ì°¾ê¸°
    output_dir = Path("outputs")

    if not output_dir.exists():
        print(f"\nâŒ outputs í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {output_dir}")
        return

    # 1. ê¸°ì¡´ PubMed ì˜ì–´ ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("1ï¸âƒ£ PubMed ì˜ì–´ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # ì—¬ëŸ¬ íŒ¨í„´ì˜ PubMed íŒŒì¼ë“¤ ìˆ˜ì§‘
    pubmed_files = (
        list(output_dir.glob("ragdb_corpus_*.json")) +
        list(output_dir.glob("body_composition_*.json")) +
        list(output_dir.glob("fat_loss_*.json")) +
        list(output_dir.glob("protein_hypertrophy_*.json"))
    )
    pubmed_papers = []

    if pubmed_files:
        print(f"  ğŸ“ ë°œê²¬ëœ PubMed íŒŒì¼: {len(pubmed_files)}ê°œ")
        
        # ëª¨ë“  íŒŒì¼ ë¡œë“œ (ì¤‘ë³µ ì œê±°ëŠ” ë‚˜ì¤‘ì— ë³‘í•© ë‹¨ê³„ì—ì„œ)
        for pubmed_file in pubmed_files:
            papers = merger.load_json_file(str(pubmed_file))
            pubmed_papers.extend(papers)
        
        print(f"âœ… PubMed ë…¼ë¬¸ ì´í•©: {len(pubmed_papers)}ê°œ")
    else:
        print("âš ï¸ PubMed ì½”í¼ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € main.pyë¥¼ ì‹¤í–‰í•˜ì—¬ PubMed ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ì„¸ìš”.")

    # 2. Google Scholar í•œêµ­ì–´ ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("2ï¸âƒ£ Google Scholar í•œêµ­ì–´ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸í•˜ê³  ë…¼ë¬¸ ë°ì´í„°ë§Œ ë¡œë“œ
    all_scholar_files = list(output_dir.glob("google_scholar_*.json"))
    scholar_files = [f for f in all_scholar_files if "stats" not in f.name]
    scholar_papers = []

    if scholar_files:
        print(f"  ğŸ“ ë°œê²¬ëœ Google Scholar íŒŒì¼: {len(scholar_files)}ê°œ")
        
        # ëª¨ë“  ë…¼ë¬¸ íŒŒì¼ ë¡œë“œ
        for scholar_file in scholar_files:
            papers = merger.load_json_file(str(scholar_file))
            scholar_papers.extend(papers)
        
        print(f"âœ… Google Scholar ì´í•©: {len(scholar_papers)}ê°œ")
    else:
        print("âš ï¸ Google Scholar íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 3. ì •ë¶€ ë³´ê³ ì„œ ë¡œë“œ
    print("\n" + "=" * 60)
    print("3ï¸âƒ£ ì •ë¶€ ë³´ê³ ì„œ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸
    all_gov_files = list(output_dir.glob("government_reports_*.json"))
    gov_files = [f for f in all_gov_files if "stats" not in f.name]
    gov_papers = []

    if gov_files:
        print(f"  ğŸ“ ë°œê²¬ëœ ì •ë¶€ ë³´ê³ ì„œ íŒŒì¼: {len(gov_files)}ê°œ")
        
        for gov_file in gov_files:
            papers = merger.load_json_file(str(gov_file))
            gov_papers.extend(papers)
        
        print(f"âœ… ì •ë¶€ ë³´ê³ ì„œ ì´í•©: {len(gov_papers)}ê°œ")
    else:
        print("âš ï¸ ì •ë¶€ ë³´ê³ ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 4. í•™ìˆ ì§€ ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("4ï¸âƒ£ í•™ìˆ ì§€ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸
    all_society_files = list(output_dir.glob("society_papers_*.json"))
    society_files = [f for f in all_society_files if "stats" not in f.name]
    society_papers = []

    if society_files:
        print(f"  ğŸ“ ë°œê²¬ëœ í•™ìˆ ì§€ íŒŒì¼: {len(society_files)}ê°œ")
        
        for society_file in society_files:
            papers = merger.load_json_file(str(society_file))
            society_papers.extend(papers)
        
        print(f"âœ… í•™ìˆ ì§€ ì´í•©: {len(society_papers)}ê°œ")
    else:
        print("âš ï¸ í•™ìˆ ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 5. KCI API ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("5ï¸âƒ£ KCI API í•œêµ­ì–´ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸
    all_kci_files = list(output_dir.glob("kci_korean_*.json"))
    kci_files = [f for f in all_kci_files if "stats" not in f.name]
    kci_papers = []

    if kci_files:
        print(f"  ğŸ“ ë°œê²¬ëœ KCI íŒŒì¼: {len(kci_files)}ê°œ")
        
        for kci_file in kci_files:
            papers = merger.load_json_file(str(kci_file))
            kci_papers.extend(papers)
        
        print(f"âœ… KCI API ì´í•©: {len(kci_papers)}ê°œ")
    else:
        print("âš ï¸ KCI API íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 6. RISS API ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("6ï¸âƒ£ RISS API í•œêµ­ì–´ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸
    all_riss_files = list(output_dir.glob("riss_korean_*.json"))
    riss_files = [f for f in all_riss_files if "stats" not in f.name]
    riss_papers = []

    if riss_files:
        print(f"  ğŸ“ ë°œê²¬ëœ RISS íŒŒì¼: {len(riss_files)}ê°œ")
        
        for riss_file in riss_files:
            papers = merger.load_json_file(str(riss_file))
            riss_papers.extend(papers)
        
        print(f"âœ… RISS API ì´í•©: {len(riss_papers)}ê°œ")
    else:
        print("âš ï¸ RISS API íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 7. ScienceON API ë…¼ë¬¸ ë¡œë“œ
    print("\n" + "=" * 60)
    print("7ï¸âƒ£ ScienceON API í•œêµ­ì–´ ë…¼ë¬¸ ë¡œë“œ")
    print("=" * 60)

    # stats íŒŒì¼ ì œì™¸
    all_scienceon_files = list(output_dir.glob("scienceon_korean_*.json"))
    scienceon_files = [f for f in all_scienceon_files if "stats" not in f.name]
    scienceon_papers = []

    if scienceon_files:
        print(f"  ğŸ“ ë°œê²¬ëœ ScienceON íŒŒì¼: {len(scienceon_files)}ê°œ")
        
        for scienceon_file in scienceon_files:
            papers = merger.load_json_file(str(scienceon_file))
            scienceon_papers.extend(papers)
        
        print(f"âœ… ScienceON API ì´í•©: {len(scienceon_papers)}ê°œ")
    else:
        print("âš ï¸ ScienceON API íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 8. ë³‘í•©
    print("\n" + "=" * 60)
    print("8ï¸âƒ£ ì „ì²´ ë³‘í•© ë° ì¤‘ë³µ ì œê±°")
    print("=" * 60)

    all_paper_lists = []
    source_names = []

    if pubmed_papers:
        all_paper_lists.append(pubmed_papers)
        source_names.append("PubMed (ì˜ì–´)")

    if scholar_papers:
        all_paper_lists.append(scholar_papers)
        source_names.append("Google Scholar (í•œêµ­ì–´)")

    if gov_papers:
        all_paper_lists.append(gov_papers)
        source_names.append("ì •ë¶€ ë³´ê³ ì„œ")

    if society_papers:
        all_paper_lists.append(society_papers)
        source_names.append("í•™ìˆ ì§€")

    if kci_papers:
        all_paper_lists.append(kci_papers)
        source_names.append("KCI API")

    if riss_papers:
        all_paper_lists.append(riss_papers)
        source_names.append("RISS API")

    if scienceon_papers:
        all_paper_lists.append(scienceon_papers)
        source_names.append("ScienceON API")

    if not all_paper_lists:
        print("âŒ ë³‘í•©í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("   - python main.py (PubMed)")
        print("   - python kci_api_collector.py (KCI API)")
        print("   - python riss_api_collector.py (RISS API)")
        print("   - python scienceon_api_collector.py (ScienceON API)")
        print("   - python google_scholar_korean_collector.py (Google Scholar)")
        print("   - python government_report_parser.py --process (ì •ë¶€ ë³´ê³ ì„œ)")
        print("   - python society_csv_parser.py --process [íŒŒì¼] (í•™ìˆ ì§€)")
        return

    merged_papers, merge_stats = merger.merge_papers(all_paper_lists, source_names)

    # 9. ìµœì¢… í†µê³„
    print("\n" + "=" * 60)
    print("9ï¸âƒ£ ìµœì¢… í†µê³„ ìƒì„±")
    print("=" * 60)

    final_stats = merger.create_final_stats(merged_papers)

    print(f"\nğŸ“Š ë„ë©”ì¸ë³„ ë¶„í¬:")
    for domain, count in final_stats.by_domain.items():
        print(f"  - {domain}: {count}ê°œ")

    print(f"\nğŸŒ ì–¸ì–´ë³„ ë¶„í¬:")
    for lang, count in final_stats.by_language.items():
        lang_name = "í•œêµ­ì–´" if lang == "ko" else "ì˜ì–´"
        print(f"  - {lang_name} ({lang}): {count}ê°œ")

    print(f"\nğŸ“š ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in final_stats.by_source.items():
        print(f"  - {source}: {count}ê°œ")

    # 10. ì €ì¥
    print("\n" + "=" * 60)
    print("ğŸ”Ÿ ìµœì¢… ì½”í¼ìŠ¤ ì €ì¥")
    print("=" * 60)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ì „ì²´ ì½”í¼ìŠ¤ ì €ì¥
    final_corpus_path = output_dir / f"ragdb_final_corpus_{timestamp}.json"
    with open(final_corpus_path, 'w', encoding='utf-8') as f:
        json.dump([p.model_dump() for p in merged_papers], f, ensure_ascii=False, indent=2)

    print(f"âœ… ìµœì¢… ì½”í¼ìŠ¤: {final_corpus_path}")
    print(f"   ì´ {len(merged_papers)}ê°œ ë…¼ë¬¸")

    # í†µê³„ ì €ì¥
    stats_path = output_dir / f"ragdb_final_stats_{timestamp}.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        stats_data = {
            'collection_stats': final_stats.model_dump(),
            'merge_stats': merge_stats
        }
        json.dump(stats_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… í†µê³„: {stats_path}")

    # ë„ë©”ì¸ë³„ ë¶„í•  ì €ì¥
    print("\nğŸ“‚ ë„ë©”ì¸ë³„ ë¶„í•  ì €ì¥:")
    for domain in final_stats.by_domain.keys():
        domain_papers = [p for p in merged_papers if p.domain == domain]
        domain_path = output_dir / f"{domain}_final_{timestamp}.json"

        with open(domain_path, 'w', encoding='utf-8') as f:
            json.dump([p.model_dump() for p in domain_papers], f, ensure_ascii=False, indent=2)

        print(f"  âœ… {domain}: {domain_path} ({len(domain_papers)}ê°œ)")

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("âœ… ë³‘í•© ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ì…ë ¥: {merge_stats['total_input']}ê°œ")
    print(f"  ì¤‘ë³µ ì œê±°: {merge_stats['duplicates_removed']}ê°œ")
    print(f"  ì¶œë ¥: {merge_stats['total_output']}ê°œ")
    print(f"\nğŸ¯ ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ :")

    targets = {
        'protein_hypertrophy': 800,
        'fat_loss': 800,
        'korean_diet': 600,
        'body_composition': 800
    }

    total_target = sum(targets.values())
    current = len(merged_papers)
    progress = (current / total_target) * 100

    for domain, target in targets.items():
        actual = final_stats.by_domain.get(domain, 0)
        domain_progress = (actual / target) * 100
        status = "âœ…" if actual >= target else "ğŸ“ˆ"
        print(f"  {status} {domain}: {actual}/{target} ({domain_progress:.1f}%)")

    print(f"\n  ì´í•©: {current}/{total_target} ({progress:.1f}%)")

    if progress >= 100:
        print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! 3000ê°œ ì½”í¼ìŠ¤ ì™„ì„±!")
    else:
        remaining = total_target - current
        print(f"\nğŸ“Œ ì¶”ê°€ í•„ìš”: {remaining}ê°œ")

        # ë¶€ì¡±í•œ ë„ë©”ì¸ ì•ˆë‚´
        for domain, target in targets.items():
            actual = final_stats.by_domain.get(domain, 0)
            if actual < target:
                shortage = target - actual
                print(f"   - {domain}: {shortage}ê°œ ë¶€ì¡±")


if __name__ == "__main__":
    main()
