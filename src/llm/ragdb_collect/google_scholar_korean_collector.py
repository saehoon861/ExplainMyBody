"""
Google Scholar í•œêµ­ì–´ ë…¼ë¬¸ ìë™ ìˆ˜ì§‘ê¸°

scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Google Scholarì—ì„œ í•œêµ­ì–´ ë…¼ë¬¸ì„ ìë™ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import time
import json
from typing import List, Optional
from datetime import datetime
from pathlib import Path

try:
    from scholarly import scholarly, ProxyGenerator
except ImportError:
    print("âŒ scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install scholarly")
    exit(1)

from models import PaperMetadata, CollectionStats


class GoogleScholarKoreanCollector:
    """Google Scholar í•œêµ­ì–´ ë…¼ë¬¸ ìˆ˜ì§‘ê¸°"""

    def __init__(self, use_proxy: bool = True, rate_limit: float = 15.0):
        """
        Args:
            use_proxy: í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ (rate limit íšŒí”¼ìš©, ì„ íƒì‚¬í•­)
            rate_limit: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - Captcha ë°©ì§€ë¥¼ ìœ„í•´ 12-15ì´ˆ ê¶Œì¥
        """
        self.rate_limit = rate_limit

        if use_proxy:
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
                print("âœ… í”„ë¡ì‹œ ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ í”„ë¡ì‹œ ì„¤ì • ì‹¤íŒ¨ (ì§ì ‘ ì—°ê²° ì‚¬ìš©): {e}")

    def search_korean_papers(
        self,
        query: str,
        domain: str,
        max_results: int = 50,
        year_from: Optional[int] = None
    ) -> List[PaperMetadata]:
        """
        Google Scholarì—ì„œ í•œêµ­ì–´ ë…¼ë¬¸ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´ (í•œêµ­ì–´)
            domain: ë„ë©”ì¸ ë¶„ë¥˜ (korean_diet, body_composition)
            max_results: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            year_from: ì‹œì‘ ì—°ë„ (Noneì´ë©´ ì œí•œ ì—†ìŒ)

        Returns:
            PaperMetadata ë¦¬ìŠ¤íŠ¸
        """
        papers = []

        print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{query}' (ìµœëŒ€ {max_results}ê°œ)")

        # Google Scholar ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = query
        if year_from:
            search_query = f"{query} after:{year_from}"

        try:
            search_results = scholarly.search_pubs(search_query)

            collected = 0
            for result in search_results:
                if collected >= max_results:
                    break

                try:
                    # âœ¨ ì „ì²´ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì´ˆë¡ í¬í•¨)
                    print(f"  ğŸ”„ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...", end='', flush=True)

                    try:
                        filled_result = scholarly.fill(result)
                        print(" âœ…")
                    except Exception as fill_error:
                        # Captcha ë˜ëŠ” ì°¨ë‹¨ ê°ì§€
                        error_msg = str(fill_error).lower()
                        if 'captcha' in error_msg or 'blocked' in error_msg or 'unusual traffic' in error_msg:
                            print(f"\n\nâš ï¸  CAPTCHA ê°ì§€ë¨!")
                            print("=" * 60)
                            print("Google Scholarì—ì„œ ìë™í™” íƒì§€ë¡œ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                            print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
                            print("  1. ë¸Œë¼ìš°ì €ì—ì„œ https://scholar.google.com ì ‘ì† í›„ Captcha í’€ê¸°")
                            print("  2. 10-15ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„")
                            print("  3. í”„ë¡ì‹œ ì‚¬ìš© (--use-proxy ì˜µì…˜)")
                            print("  4. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ì§„í–‰ (Enter)")
                            print("=" * 60)

                            user_choice = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                            if user_choice != 'y':
                                print(f"ì¤‘ë‹¨ë¨. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(papers)}ê°œ")
                                return papers
                            else:
                                print("ì¬ì‹œë„ ì¤‘...")
                                time.sleep(15)  # 15ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                                continue
                        else:
                            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ìŠ¤í‚µ
                            print(f" âŒ ({fill_error})")
                            continue

                    # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
                    paper = self._parse_scholar_result(filled_result, domain)

                    # ì´ˆë¡ì´ ìˆëŠ” ë…¼ë¬¸ë§Œ ìˆ˜ì§‘
                    if paper and paper.abstract and len(paper.abstract) >= 100:
                        papers.append(paper)
                        collected += 1
                        print(f"  âœ… [{collected}/{max_results}] {paper.title[:50]}... (ì´ˆë¡: {len(paper.abstract)}ì)")

                    # Rate limiting (Captcha ë°©ì§€ë¥¼ ìœ„í•´ ì¦ê°€)
                    time.sleep(self.rate_limit)

                except Exception as e:
                    print(f"  âš ï¸ ë…¼ë¬¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            print(f"âœ… '{query}': {len(papers)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ '{query}': {e}")

        return papers

    def _parse_scholar_result(self, result: dict, domain: str) -> Optional[PaperMetadata]:
        """Google Scholar ê²€ìƒ‰ ê²°ê³¼ë¥¼ PaperMetadataë¡œ ë³€í™˜"""

        try:
            # ì œëª© ì¶”ì¶œ
            title = result.get('bib', {}).get('title', '')
            if not title:
                return None

            # ì´ˆë¡ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
            abstract = result.get('bib', {}).get('abstract', '')

            # ì—°ë„ ì¶”ì¶œ
            year_str = result.get('bib', {}).get('pub_year', '')
            year = int(year_str) if year_str and year_str.isdigit() else None

            # ì €ì ì¶”ì¶œ
            authors_raw = result.get('bib', {}).get('author', [])
            if isinstance(authors_raw, str):
                authors = [authors_raw]
            elif isinstance(authors_raw, list):
                authors = authors_raw[:5]  # ìµœëŒ€ 5ëª…
            else:
                authors = []

            # ì €ë„ ì¶”ì¶œ
            journal = result.get('bib', {}).get('venue', '') or \
                     result.get('bib', {}).get('journal', '')

            # Metadata ìƒì„±
            paper = PaperMetadata(
                domain=domain,
                language='ko',
                title=title,
                abstract=abstract,
                keywords=[],  # Google ScholarëŠ” í‚¤ì›Œë“œ ì œê³µ ì•ˆí•¨
                source='Google Scholar',
                year=year,
                pmid=None,
                doi=None,
                authors=authors,
                journal=journal
            )

            return paper

        except Exception as e:
            print(f"  âš ï¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def collect_domain(
        self,
        domain: str,
        queries: List[str],
        target_count: int,
        year_from: int = 2010
    ) -> List[PaperMetadata]:
        """
        íŠ¹ì • ë„ë©”ì¸ì˜ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            domain: ë„ë©”ì¸ ë¶„ë¥˜
            queries: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
            target_count: ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
            year_from: ì‹œì‘ ì—°ë„

        Returns:
            PaperMetadata ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
        """
        all_papers = []
        seen_titles = set()  # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±°

        results_per_query = max(10, target_count // len(queries))

        for query in queries:
            papers = self.search_korean_papers(
                query=query,
                domain=domain,
                max_results=results_per_query,
                year_from=year_from
            )

            # ì¤‘ë³µ ì œê±°
            for paper in papers:
                title_normalized = paper.title.lower().strip()
                if title_normalized not in seen_titles:
                    seen_titles.add(title_normalized)
                    all_papers.append(paper)

            # ëª©í‘œ ë‹¬ì„± í™•ì¸
            if len(all_papers) >= target_count:
                break

            # ì¿¼ë¦¬ ê°„ ëŒ€ê¸°
            time.sleep(5)

        return all_papers[:target_count]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í•œêµ­ì–´ ë…¼ë¬¸ ê²€ìƒ‰ì–´ (config.pyì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ)
    KOREAN_DIET_QUERIES = [
        "í•œêµ­ ì‹ë‹¨ ì˜ì–‘",
         ]

        # "í•œì‹ ì‹ì‚¬íŒ¨í„´ ê±´ê°•",
        # "ê¹€ì¹˜ ì„­ì·¨ íš¨ê³¼",
        # "í•œêµ­ì¸ ë‹¨ë°±ì§ˆ ì„­ì·¨",
        # "ì „í†µ ë°œíš¨ì‹í’ˆ ê±´ê°•",
        # "ëœì¥ ê±´ê°• íš¨ê³¼",
        # "í•œêµ­í˜• ì‹ìƒí™œ ì§€ì¹¨",
        # "êµ­ë¯¼ê±´ê°•ì˜ì–‘ì¡°ì‚¬ ì‹ì´ì„­ì·¨",
   
    BODY_COMPOSITION_QUERIES = [
     "ì²´ì§€ë°©ë¥  ë° ê³¨ê²©ê·¼ëŸ‰ ì§€ìˆ˜(SMI) ê¸°ë°˜ì˜ ì²´í˜• ë¶„ë¥˜ ëª¨ë¸"
    "ì²´í˜•ë¶€ìœ„ë³„ ê·¼ìœ¡ ë¶ˆê· í˜•(Segmental Lean Analysis)ê³¼ ì‹ ì²´ ê¸°ëŠ¥ì˜ ìƒê´€ê´€ê³„"
    "ìƒÂ·í•˜ì²´ ê·¼ìœ¡ëŸ‰ ë¹„ìœ¨ì— ë”°ë¥¸ ê·¼ê°ì†Œì„± ë¹„ë§Œ(Sarcopenic Obesity) íŒì • ê¸°ì¤€"
    "InBody ë°ì´í„°ë¥¼ í™œìš©í•œ ì²´í˜• ì§€ìˆ˜(Body Shape Index) ì‚°ì¶œ ë¡œì§"
    "ë³µë¶€ì§€ë°©ë¥ (WHR) ë° ë‚´ì¥ì§€ë°©ë ˆë²¨ì— ë”°ë¥¸ ê³ ê°•ë„ ì¸í„°ë²Œ íŠ¸ë ˆì´ë‹(HIIT)ì˜ íš¨ê³¼"
    "ì¢Œìš°ì¸¡ ìƒí•˜ì§€ ê·¼ìœ¡ ë¶ˆê· í˜• êµì •ì„ ìœ„í•œ í¸ì¸¡ì„± ìš´ë™(Unilateral Exercise) ì²˜ë°©"
    "ì‹ ì²´ ë¶€ìœ„ë³„ ì²´ì§€ë°© ë¶„í¬ì™€ ì¸ìŠë¦° ì €í•­ì„± ê°„ì˜ ê´€ê³„"
    "ë¬´ê¸°ì§ˆ ë° ë‹¨ë°±ì§ˆ ì„­ì·¨ ìƒíƒœì™€ ê·¼ë ¥ ìš´ë™ íš¨ìœ¨ì˜ ìƒê´€ì„±"
        
    ]

        # "ê·¼ê°ì†Œì¦ í•œêµ­ì¸",
        # "ì²´ì„±ë¶„ ë¶„ì„ ì¸ë°”ë””",
        # "ê³¨ê²©ê·¼ëŸ‰ í‰ê°€",
        # "ì²´ì§€ë°©ë¥  ê¸°ì¤€",
        # "ë…¸ì¸ ê·¼ìœ¡ëŸ‰",
        # "ìƒì²´ì „ê¸°ì €í•­ ë¶„ì„",

    INBODY_BIA_KR_QUERIES = [
    "ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ì²´ì„±ë¶„ í‰ê°€",
    "ì¸ë°”ë”” ì²´ì„±ë¶„ ë¶„ì„ ì‹ ë¢°ë„",
    "ìƒì²´ì „ê¸°ì €í•­ë²• ê³¨ê²©ê·¼ëŸ‰ ì •í™•ë„",
    "DXAì™€ ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ë¹„êµ",
    "ì²´ì„±ë¶„ ì¸¡ì •ë°©ë²• íƒ€ë‹¹ë„ ì—°êµ¬",
    ]

    BODY_TYPE_CLASSIFICATION_KR_QUERIES = [
    "ì²´ì„±ë¶„ ê¸°ë°˜ ì²´í˜• ë¶„ë¥˜",
    "ì²´ì§€ë°©ëŸ‰ ê³¨ê²©ê·¼ëŸ‰ ì²´í˜• ìœ í˜•",
    "ê·¼ìœ¡-ì§€ë°© ë¶ˆê· í˜• ì²´í˜• ë¶„ì„",
    "ì²´ì„±ë¶„ ì§€í‘œë¥¼ ì´ìš©í•œ êµ°ì§‘ë¶„ì„",
    "ë¹„ë§Œ ìœ í˜• ì²´ì„±ë¶„ phenotype ì—°êµ¬",
    ]

    SARCOPENIA_KR_QUERIES = [
    "ê·¼ê°ì†Œì¦ ê³¨ê²©ê·¼ëŸ‰ ê¸°ì¤€ í•œêµ­ì¸",
    "ì•„ì‹œì•„ ê·¼ê°ì†Œì¦ ì§„ë‹¨ê¸°ì¤€ ìƒì²´ì „ê¸°ì €í•­",
    "ì‚¬ì§€ê³¨ê²©ê·¼ëŸ‰ì§€ìˆ˜(SMI) ì°¸ê³ ì¹˜",
    "ë…¸ì¸ ê·¼ìœ¡ëŸ‰ ê°ì†Œ ì²´ì„±ë¶„ ì—°êµ¬",
    "ê·¼ê°ì†Œì„± ë¹„ë§Œ í•œêµ­ì¸ ìœ ë³‘ë¥ ",
    ]

    BODYFAT_OBESITY_KR_QUERIES = [
    "ì²´ì§€ë°©ë¥  ê¸°ì¤€ í•œêµ­ì¸",
    "BMIì™€ ì²´ì§€ë°©ë¥  ë¹„êµ ì—°êµ¬",
    "ì •ìƒì²´ì¤‘ë¹„ë§Œ ì²´ì„±ë¶„ ë¶„ì„",
    "ë¹„ë§Œë„ í‰ê°€ ì²´ì„±ë¶„ ì§€í‘œ",
    "ì²´ì§€ë°©ëŸ‰ê³¼ ëŒ€ì‚¬ì§ˆí™˜ ìœ„í—˜",
    ]
    
    # VISCERAL_FAT_KR_QUERIES = [
    # "ë‚´ì¥ì§€ë°© ìˆ˜ì¤€ ëŒ€ì‚¬ì¦í›„êµ° ìœ„í—˜",
    # "ë³µë¶€ë¹„ë§Œ ë‚´ì¥ì§€ë°© ì²´ì„±ë¶„ ë¶„ì„",
    # "ë‚´ì¥ì§€ë°©ë©´ì ê³¼ ì¸ìŠë¦°ì €í•­ì„±",
    # "ì¤‘ì‹¬ì„±ë¹„ë§Œ ê±´ê°•ìœ„í—˜ ì—°êµ¬",
    # "ìƒì²´ì „ê¸°ì €í•­ë¶„ì„ ë‚´ì¥ì§€ë°© ì¶”ì •",
    # ]

    # SEGMENTAL_BALANCE_KR_QUERIES = [
    # "ë¶€ìœ„ë³„ ê³¨ê²©ê·¼ëŸ‰ ë¶ˆê· í˜• ë¶„ì„",
    # "ì‚¬ì§€ ê·¼ìœ¡ëŸ‰ ì¢Œìš° ì°¨ì´",
    # "íŒ” ë‹¤ë¦¬ ê·¼ìœ¡ ë¹„ëŒ€ì¹­ ì²´ì„±ë¶„",
    # "ë¶€ìœ„ë³„ ì²´ì§€ë°© ë¶„í¬ ì—°êµ¬",
    # "êµ­ì†Œ ì²´ì„±ë¶„ ë¶ˆê· í˜• ìš´ë™ì²˜ë°©",
    # ]

    # EXERCISE_INTERVENTION_KR_QUERIES = [
    # "ì €í•­ìš´ë™ ê³¨ê²©ê·¼ëŸ‰ ì¦ê°€ ì²´ì„±ë¶„ ë³€í™”",
    # "ìœ ì‚°ì†Œìš´ë™ ë‚´ì¥ì§€ë°© ê°ì†Œ íš¨ê³¼",
    # "ë³µí•©ìš´ë™ ì²´ì§€ë°©ë¥  ê°œì„  ì—°êµ¬",
    # "ìš´ë™ì¤‘ì¬ ì²´ì„±ë¶„ ê°œì„  í”„ë¡œê·¸ë¨",
    # "ìš´ë™ì²˜ë°© ê¸°ë°˜ ì²´ì„±ë¶„ ë¶„ì„",
    # ]

    # MUSCLE_ADJUSTMENT_KR_QUERIES = [
    # "ê·¼ìœ¡ëŸ‰ ì¦ê°€ í”„ë¡œê·¸ë¨ íš¨ê³¼",
    # "ë‹¨ë°±ì§ˆ ì„­ì·¨ì™€ ê·¼ìœ¡ëŸ‰ ë³€í™”",
    # "ê·¼ê°ì†Œ ì˜ˆë°© ì €í•­ì„±ìš´ë™ ì²˜ë°©",
    # "ì œì§€ë°©ëŸ‰ ì¦ê°€ ì¤‘ì¬ì—°êµ¬",
    # ]

    # BMR_NUTRITION_KR_QUERIES = [
    # "ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ê³¼ ì œì§€ë°©ëŸ‰ ê´€ê³„",
    # "ì²´ì„±ë¶„ ê¸°ë°˜ ì—ë„ˆì§€ í•„ìš”ëŸ‰ ì¶”ì •",
    # "ê¶Œì¥ì„­ì·¨ì—´ëŸ‰ ì‚°ì • ì²´ì„±ë¶„ ì—°êµ¬",
    # "ì²´ì¤‘ì¡°ì ˆ í”„ë¡œê·¸ë¨ ëŒ€ì‚¬ëŸ‰ ë³€í™”",
    # ]

    # METABOLIC_RISK_KR_QUERIES = [
    # "ì²´ì„±ë¶„ê³¼ ëŒ€ì‚¬ì¦í›„êµ° ìœ„í—˜",
    # "ê³¨ê²©ê·¼ëŸ‰ê³¼ ë‹¹ë‡¨ë³‘ ìœ„í—˜",
    # "ë‚´ì¥ì§€ë°©ê³¼ ì‹¬í˜ˆê´€ì§ˆí™˜ ì—°ê´€",
    # "ì²´ì„±ë¶„ ì§€í‘œ ê±´ê°•ì˜ˆì¸¡ëª¨ë¸",
    # ]




    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (Captcha ë°©ì§€ë¥¼ ìœ„í•´ rate_limit 15ì´ˆ)
    collector = GoogleScholarKoreanCollector(use_proxy=False, rate_limit=15.0)

    # í•œêµ­ ì‹ë‹¨ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 1: í•œêµ­í˜• ì‹ë‹¨/í•œì‹ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    korean_diet_papers = collector.collect_domain(
        domain='korean_diet',
        queries=KOREAN_DIET_QUERIES,
        target_count=2,
        year_from=2010
    )

    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 2: ì²´í˜• ë¶„ì„/ì¸ë°”ë”” (ëª©í‘œ: 500ê°œ)")
    print("=" * 60)

    body_comp_papers1 = collector.collect_domain(
        domain='body_composition',
        queries=BODY_COMPOSITION_QUERIES,
        target_count=100,
        year_from=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 3: ì¸ë°”ë””/ì²´ì„±ë¶„ ë¶„ì„ í•µì‹¬ í‚¤ì›Œë“œ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers2 = collector.collect_domain(
        domain='body_composition',
        queries=INBODY_BIA_KR_QUERIES,
        target_count=100,
        year_from=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 4: ì²´í˜•ë¶„ì„/ì²´ì„±ë¶„ ê¸°ë°˜ ìœ í˜•í™” (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers3 = collector.collect_domain(
        domain='body_composition',
        queries=BODY_TYPE_CLASSIFICATION_KR_QUERIES,
        target_count=100,
        year_from=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 5: ê·¼ê°ì†Œì¦ + ê·¼ê°ì†Œì„±ë¹„ë§Œ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers4 = collector.collect_domain(
        domain='body_composition',
        queries=SARCOPENIA_KR_QUERIES,
        target_count=100,
        year_from=2010
    )
    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 6: ì²´ì§€ë°©ë¥ Â·ë¹„ë§Œë„Â·BMI í•œê³„ (ëª©í‘œ: 250ê°œ)")
    print("=" * 60)

    body_comp_papers5 = collector.collect_domain(
        domain='body_composition',
        queries=BODYFAT_OBESITY_KR_QUERIES,
        target_count=100,
        year_from=2010
    )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 7: ë³µë¶€ì§€ë°©ë¥ Â·ë‚´ì¥ì§€ë°© ë ˆë²¨ (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers6 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=VISCERAL_FAT_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 8: ë¶€ìœ„ë³„ ê·¼ìœ¡/ì§€ë°© ë¶ˆê· í˜• (Segmental) (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers7 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=SEGMENTAL_BALANCE_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 9: ìš´ë™ì²˜ë°© ê·¼ê±° (ê·¼ìœ¡ ì¦ê°€/ì§€ë°© ê°ì†Œ) (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers8 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=EXERCISE_INTERVENTION_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 10: ê¸°ì´ˆëŒ€ì‚¬ëŸ‰(BMR) + ì—ë„ˆì§€ ì²˜ë°© (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers9 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=MUSCLE_ADJUSTMENT_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 11: ê¸°ì´ˆëŒ€ì‚¬ëŸ‰(BMR) + ì—ë„ˆì§€ ì²˜ë°© (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers10 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=BMR_NUTRITION_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )
    # # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘ (ëª©í‘œ: 200-300ê°œ)
    # print("\n" + "=" * 60)
    # print("ğŸ“š ë„ë©”ì¸ 12: ì²´ì„±ë¶„ê³¼ ëŒ€ì‚¬ì¦í›„êµ° (ëª©í‘œ: 250ê°œ)")
    # print("=" * 60)

    # body_comp_papers11 = collector.collect_domain(
    #     domain='body_composition',
    #     queries=METABOLIC_RISK_KR_QUERIES,
    #     target_count=100,
    #     year_from=2010
    # )


    # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼
    all_papers = korean_diet_papers + body_comp_papers1 + body_comp_papers2 + body_comp_papers3 + body_comp_papers4 + body_comp_papers5 + body_comp_papers6 + body_comp_papers7 + body_comp_papers8 + body_comp_papers9 + body_comp_papers10 + body_comp_papers11


    # í†µê³„ ìƒì„±
    stats = CollectionStats(
        total_collected=len(all_papers),
        by_domain={
            'korean_diet': len(korean_diet_papers),
            'body_composition': len(body_comp_papers)
        },
        by_language={'ko': len(all_papers)},
        by_source={'Google Scholar': len(all_papers)},
        failed_count=0
    )

    # ê²°ê³¼ ì €ì¥
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ì „ì²´ ì €ì¥
    corpus_path = output_dir / f"google_scholar_korean_{timestamp}.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump([p.model_dump() for p in all_papers], f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {corpus_path}")

    # ë„ë©”ì¸ë³„ ë¶„í•  ì €ì¥
    if korean_diet_papers:
        diet_path = output_dir / f"korean_diet_scholar_{timestamp}.json"
        with open(diet_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in korean_diet_papers], f, ensure_ascii=False, indent=2)
        print(f"   - í•œêµ­ ì‹ë‹¨: {diet_path}")

    if body_comp_papers:
        body_path = output_dir / f"body_composition_scholar_{timestamp}.json"
        with open(body_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in body_comp_papers], f, ensure_ascii=False, indent=2)
        print(f"   - ì²´í˜• ë¶„ì„: {body_path}")

    # í†µê³„ ì €ì¥
    stats_path = output_dir / f"google_scholar_stats_{timestamp}.json"
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats.model_dump(), f, ensure_ascii=False, indent=2)

    print(f"   - í†µê³„: {stats_path}")

    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ í†µê³„")
    print("=" * 60)
    print(f"ì´ ìˆ˜ì§‘: {stats.total_collected}ê°œ")
    print(f"  - í•œêµ­ ì‹ë‹¨: {stats.by_domain['korean_diet']}ê°œ")
    print(f"  - ì²´í˜• ë¶„ì„: {stats.by_domain['body_composition']}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
