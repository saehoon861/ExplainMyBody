"""
RISS OpenAPIë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ ë…¼ë¬¸ ìë™ ìˆ˜ì§‘

ê³µê³µë°ì´í„°í¬í„¸ ë˜ëŠ” RISS API ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬
í•™ìˆ ì—°êµ¬ì •ë³´ì„œë¹„ìŠ¤(RISS)ì˜ ë…¼ë¬¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

API í‚¤ ë°œê¸‰:
- ê³µê³µë°ì´í„°í¬í„¸: https://www.data.go.kr/data/3046254/openapi.do
- RISS API ì„¼í„°: https://www.riss.kr/openAPI/OpenApiMain.do
"""

import requests
import time
import json
from typing import List, Optional
from datetime import datetime
from pathlib import Path
import xml.etree.ElementTree as ET

from models import PaperMetadata


class RISSAPICollector:
    """RISS OpenAPI ìˆ˜ì§‘ê¸°"""

    def __init__(self, api_key: str):
        """
        Args:
            api_key: RISSì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤
        """
        self.api_key = api_key

        # RISS OpenAPI ì—”ë“œí¬ì¸íŠ¸
        # êµ­ë‚´í•™ìˆ ì§€ë…¼ë¬¸
        self.search_url = "http://www.riss.kr/openapi/search/search.jsp"

        # Rate limiting
        self.rate_limit = 1.0  # 1ì´ˆì— 1ê°œ ìš”ì²­

    def search_papers(
        self,
        query: str,
        max_results: int = 100,
        start_year: Optional[int] = None,
        end_year: Optional[int] = None,
        paper_type: str = "ARTICLE"  # ARTICLE(í•™ìˆ ì§€), THESIS(í•™ìœ„ë…¼ë¬¸)
    ) -> List[dict]:
        """
        RISSì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ì–´ (í•œêµ­ì–´)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            start_year: ì‹œì‘ ì—°ë„
            end_year: ì¢…ë£Œ ì—°ë„
            paper_type: ARTICLE(í•™ìˆ ì§€) ë˜ëŠ” THESIS(í•™ìœ„ë…¼ë¬¸)

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        papers = []

        print(f"\nğŸ” RISS ê²€ìƒ‰: '{query}' (ìµœëŒ€ {max_results}ê°œ)")

        # í˜ì´ì§€ë„¤ì´ì…˜ (í•œ ë²ˆì— 100ê°œì”©)
        page_size = 100
        total_pages = (max_results + page_size - 1) // page_size

        for page in range(1, total_pages + 1):
            try:
                # API ìš”ì²­ íŒŒë¼ë¯¸í„°
                params = {
                    'apikey': self.api_key,
                    'query': query,
                    'displayCount': min(page_size, max_results - len(papers)),
                    'startIndex': (page - 1) * page_size + 1,
                    'searchGubun': paper_type,
                }

                # ì—°ë„ í•„í„°
                if start_year and end_year:
                    params['pubYear'] = f"{start_year}~{end_year}"
                elif start_year:
                    params['pubYear'] = f"{start_year}~{datetime.now().year}"

                # API ìš”ì²­
                response = requests.get(self.search_url, params=params, timeout=30)

                if response.status_code != 200:
                    print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨ (status: {response.status_code})")
                    break

                # XML íŒŒì‹±
                root = ET.fromstring(response.content)

                # ì´ ê²°ê³¼ ìˆ˜
                total_elem = root.find('.//result/totalCount')
                if total_elem is not None and page == 1:
                    total = int(total_elem.text)
                    print(f"  ğŸ“Š ì´ {total}ê°œ ë…¼ë¬¸ ë°œê²¬")

                # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
                items = root.findall('.//items/item')

                if not items:
                    print(f"  âš ï¸ {page}í˜ì´ì§€ì— ê²°ê³¼ ì—†ìŒ")
                    break

                for item in items:
                    paper_info = self._parse_riss_item(item)
                    if paper_info:
                        papers.append(paper_info)

                print(f"  âœ… {page}/{total_pages} í˜ì´ì§€: {len(items)}ê°œ ìˆ˜ì§‘ (ì´ {len(papers)}ê°œ)")

                # Rate limiting
                time.sleep(self.rate_limit)

                # ëª©í‘œ ë‹¬ì„±
                if len(papers) >= max_results:
                    break

            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"âœ… RISS ê²€ìƒ‰ ì™„ë£Œ: {len(papers)}ê°œ ìˆ˜ì§‘")
        return papers

    def _parse_riss_item(self, item: ET.Element) -> Optional[dict]:
        """RISS XML ì•„ì´í…œì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±"""
        try:
            # ì œëª©
            title_elem = item.find('.//title')
            title = title_elem.text if title_elem is not None else None

            if not title:
                return None

            # ì´ˆë¡
            abstract_elem = item.find('.//abstract')
            abstract = abstract_elem.text if abstract_elem is not None else ""

            # ì´ˆë¡ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if len(abstract) < 100:
                return None

            # í‚¤ì›Œë“œ
            keywords = []
            keyword_elem = item.find('.//keyword')
            if keyword_elem is not None and keyword_elem.text:
                # ì„¸ë¯¸ì½œë¡  ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„
                kw_text = keyword_elem.text.replace(';', ',')
                keywords = [k.strip() for k in kw_text.split(',') if k.strip()]

            # ì—°ë„
            year_elem = item.find('.//pubYear')
            year = None
            if year_elem is not None and year_elem.text:
                try:
                    year = int(year_elem.text)
                except:
                    pass

            # ì €ì
            authors = []
            author_elem = item.find('.//author')
            if author_elem is not None and author_elem.text:
                # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„
                author_text = author_elem.text.replace(';', ',')
                authors = [a.strip() for a in author_text.split(',') if a.strip()][:5]

            # ì €ë„/í•™íšŒì§€
            journal_elem = item.find('.//publisher')
            journal = journal_elem.text if journal_elem is not None else "RISS í•™ìˆ ì§€"

            # DOI
            doi_elem = item.find('.//doi')
            doi = doi_elem.text if doi_elem is not None else None

            return {
                'title': title,
                'abstract': abstract,
                'keywords': keywords,
                'year': year,
                'authors': authors,
                'journal': journal,
                'doi': doi
            }

        except Exception as e:
            print(f"  âš ï¸ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def collect_domain(
        self,
        domain: str,
        queries: List[str],
        target_count: int,
        start_year: int = 2010,
        include_thesis: bool = False
    ) -> List[PaperMetadata]:
        """
        íŠ¹ì • ë„ë©”ì¸ì˜ ë…¼ë¬¸ ìˆ˜ì§‘

        Args:
            domain: ë„ë©”ì¸ ë¶„ë¥˜
            queries: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
            target_count: ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜
            start_year: ì‹œì‘ ì—°ë„
            include_thesis: í•™ìœ„ë…¼ë¬¸ í¬í•¨ ì—¬ë¶€

        Returns:
            PaperMetadata ë¦¬ìŠ¤íŠ¸
        """
        all_papers = []
        seen_titles = set()

        results_per_query = max(10, target_count // len(queries))
        current_year = datetime.now().year

        # ë…¼ë¬¸ ìœ í˜•
        paper_types = ["ARTICLE"]  # í•™ìˆ ì§€
        if include_thesis:
            paper_types.append("THESIS")  # í•™ìœ„ë…¼ë¬¸

        for query in queries:
            for paper_type in paper_types:
                papers_data = self.search_papers(
                    query=query,
                    max_results=results_per_query // len(paper_types),
                    start_year=start_year,
                    end_year=current_year,
                    paper_type=paper_type
                )

                # PaperMetadataë¡œ ë³€í™˜
                for data in papers_data:
                    # ì¤‘ë³µ ì²´í¬
                    title_normalized = data['title'].lower().strip()
                    if title_normalized in seen_titles:
                        continue

                    seen_titles.add(title_normalized)

                    # ì¶œì²˜ í‘œì‹œ
                    source = "RISS í•™ìˆ ì§€" if paper_type == "ARTICLE" else "RISS í•™ìœ„ë…¼ë¬¸"

                    # PaperMetadata ìƒì„±
                    paper = PaperMetadata(
                        domain=domain,
                        language='ko',
                        title=data['title'],
                        abstract=data['abstract'],
                        keywords=data['keywords'],
                        source=source,
                        year=data['year'],
                        pmid=None,
                        doi=data.get('doi'),
                        authors=data['authors'],
                        journal=data['journal']
                    )

                    all_papers.append(paper)

                # ëª©í‘œ ë‹¬ì„±
                if len(all_papers) >= target_count:
                    break

            if len(all_papers) >= target_count:
                break

            # ì¿¼ë¦¬ ê°„ ëŒ€ê¸°
            time.sleep(2)

        return all_papers[:target_count]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸ‡°ğŸ‡· RISS OpenAPI í•œêµ­ì–´ ë…¼ë¬¸ ìˆ˜ì§‘")
    print("=" * 60)

    # API í‚¤ ì…ë ¥
    print("\nğŸ“‹ RISS API í‚¤ ë°œê¸‰:")
    print("  1. https://www.data.go.kr/ ë˜ëŠ” https://www.riss.kr/")
    print("  2. íšŒì›ê°€ì… â†’ API í‚¤ ì‹ ì²­")
    print("  3. ìŠ¹ì¸ í›„ API í‚¤ ë°œê¸‰")
    print("")

    api_key = input("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not api_key:
        print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = RISSAPICollector(api_key=api_key)

    # í•œêµ­ì–´ ê²€ìƒ‰ì–´
    KOREAN_DIET_QUERIES = [
        "í•œì‹ ê±´ê°•",
        "ê¹€ì¹˜ ì˜ì–‘",
        "í•œêµ­ì¸ ì‹ìŠµê´€",
        "ë°œíš¨ì‹í’ˆ íš¨ê³¼",
        "ì „í†µì‹ë‹¨",
    ]

    BODY_COMPOSITION_QUERIES = [
        "ê·¼ê°ì†Œì¦",
        "ì²´ì„±ë¶„",
        "ê³¨ê²©ê·¼",
        "ì²´ì§€ë°©",
        "ì¸ë°”ë””",
    ]

    # í•™ìœ„ë…¼ë¬¸ í¬í•¨ ì—¬ë¶€
    include_thesis = input("\ní•™ìœ„ë…¼ë¬¸ë„ í¬í•¨í• ê¹Œìš”? (y/n, ê¸°ë³¸: n): ").strip().lower() == 'y'

    # í•œêµ­ ì‹ë‹¨ ìˆ˜ì§‘
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 1: í•œêµ­í˜• ì‹ë‹¨ (ëª©í‘œ: 300ê°œ)")
    print("=" * 60)

    korean_diet_papers = collector.collect_domain(
        domain='korean_diet',
        queries=KOREAN_DIET_QUERIES,
        target_count=300,
        start_year=2010,
        include_thesis=include_thesis
    )

    # ì²´í˜• ë¶„ì„ ìˆ˜ì§‘
    print("\n" + "=" * 60)
    print("ğŸ“š ë„ë©”ì¸ 2: ì²´í˜• ë¶„ì„/ì¸ë°”ë”” (ëª©í‘œ: 300ê°œ)")
    print("=" * 60)

    body_comp_papers = collector.collect_domain(
        domain='body_composition',
        queries=BODY_COMPOSITION_QUERIES,
        target_count=300,
        start_year=2010,
        include_thesis=include_thesis
    )

    # ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼
    all_papers = korean_diet_papers + body_comp_papers

    # ê²°ê³¼ ì €ì¥
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ì „ì²´ ì €ì¥
    corpus_path = output_dir / f"riss_korean_{timestamp}.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump([p.model_dump() for p in all_papers], f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {corpus_path}")

    # ë„ë©”ì¸ë³„ ì €ì¥
    if korean_diet_papers:
        diet_path = output_dir / f"korean_diet_riss_{timestamp}.json"
        with open(diet_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in korean_diet_papers], f, ensure_ascii=False, indent=2)
        print(f"   - í•œêµ­ ì‹ë‹¨: {diet_path} ({len(korean_diet_papers)}ê°œ)")

    if body_comp_papers:
        body_path = output_dir / f"body_composition_riss_{timestamp}.json"
        with open(body_path, "w", encoding="utf-8") as f:
            json.dump([p.model_dump() for p in body_comp_papers], f, ensure_ascii=False, indent=2)
        print(f"   - ì²´í˜• ë¶„ì„: {body_path} ({len(body_comp_papers)}ê°œ)")

    # í†µê³„
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 60)
    print(f"ì´ ìˆ˜ì§‘: {len(all_papers)}ê°œ")
    print(f"  - í•œêµ­ ì‹ë‹¨: {len(korean_diet_papers)}ê°œ")
    print(f"  - ì²´í˜• ë¶„ì„: {len(body_comp_papers)}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
