"""
ì •ë¶€/ê³µê³µê¸°ê´€ ë³´ê³ ì„œ PDF íŒŒì„œ

ë³´ê±´ë³µì§€ë¶€, ì‹ì•½ì²˜, ì§ˆë³‘ê´€ë¦¬ì²­ ë“±ì˜ ê³µì‹ ë³´ê³ ì„œì—ì„œ
ì´ˆë¡/ìš”ì•½ ì„¹ì…˜ì„ ì¶”ì¶œí•˜ì—¬ RAG ì½”í¼ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import json
from typing import List, Optional, Dict
from pathlib import Path
from datetime import datetime

try:
    import pdfplumber
except ImportError:
    print("âŒ pdfplumber ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pdfplumber")
    exit(1)

from models import PaperMetadata


class GovernmentReportParser:
    """ì •ë¶€ ë³´ê³ ì„œ PDF íŒŒì„œ"""

    # ì´ˆë¡/ìš”ì•½ ì„¹ì…˜ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë“¤
    SUMMARY_KEYWORDS = [
        "ìš”ì•½", "ì´ˆë¡", "Summary", "Abstract", "Executive Summary",
        "ì—°êµ¬ìš”ì•½", "ì •ì±…ìš”ì•½", "ê°œìš”", "í•µì‹¬ë‚´ìš©"
    ]

    # ì„¹ì…˜ ì¢…ë£Œ í‚¤ì›Œë“œ (ì´í›„ëŠ” ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼)
    END_KEYWORDS = [
        "ëª©ì°¨", "ì°¨ë¡€", "ì„œë¡ ", "ë°°ê²½", "1.", "I.", "ì œ1ì¥", "ì œ1ì ˆ"
    ]

    def __init__(self):
        pass

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                text = ""
                # ì²˜ìŒ 10í˜ì´ì§€ë§Œ (ì´ˆë¡ì€ ëŒ€ë¶€ë¶„ ì•ìª½ì— ìˆìŒ)
                for page in pdf.pages[:10]:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"âŒ PDF ì½ê¸° ì‹¤íŒ¨ {pdf_path}: {e}")
            return ""

    def find_summary_section(self, text: str) -> Optional[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì´ˆë¡/ìš”ì•½ ì„¹ì…˜ ì¶”ì¶œ

        Returns:
            ì´ˆë¡ í…ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ None)
        """
        lines = text.split('\n')

        # ì´ˆë¡ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
        summary_start = -1
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if any(keyword in line_stripped for keyword in self.SUMMARY_KEYWORDS):
                summary_start = i
                break

        if summary_start == -1:
            return None

        # ì´ˆë¡ ë ìœ„ì¹˜ ì°¾ê¸°
        summary_lines = []
        for i in range(summary_start + 1, len(lines)):
            line = lines[i].strip()

            # ì¢…ë£Œ í‚¤ì›Œë“œ ë°œê²¬ ì‹œ ì¤‘ë‹¨
            if any(keyword in line for keyword in self.END_KEYWORDS):
                break

            # ë¹ˆ ì¤„ì´ 5ê°œ ì´ìƒ ì—°ì†ë˜ë©´ ì¤‘ë‹¨
            if not line:
                continue

            summary_lines.append(line)

            # ìµœëŒ€ 50ì¤„ê¹Œì§€ë§Œ
            if len(summary_lines) >= 50:
                break

        summary = ' '.join(summary_lines).strip()

        # ìµœì†Œ ê¸¸ì´ ì²´í¬ (100ì ì´ìƒ)
        if len(summary) < 100:
            return None

        return summary

    def parse_pdf_manual(
        self,
        pdf_path: str,
        title: str,
        domain: str,
        year: Optional[int] = None,
        authors: Optional[List[str]] = None,
        source: str = "ì •ë¶€ë³´ê³ ì„œ"
    ) -> Optional[PaperMetadata]:
        """
        PDF íŒŒì¼ì—ì„œ PaperMetadata ìƒì„±

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            title: ë³´ê³ ì„œ ì œëª©
            domain: ë„ë©”ì¸ ë¶„ë¥˜
            year: ë°œí–‰ ì—°ë„
            authors: ì €ì/ê¸°ê´€ ë¦¬ìŠ¤íŠ¸
            source: ì¶œì²˜

        Returns:
            PaperMetadata ê°ì²´ (ì‹¤íŒ¨ ì‹œ None)
        """
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {title}")

        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        if not text:
            print(f"  âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return None

        # ì´ˆë¡ ì„¹ì…˜ ì¶”ì¶œ
        abstract = self.find_summary_section(text)
        if not abstract:
            print(f"  âš ï¸ ì´ˆë¡ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            # ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ 500ìë¥¼ ì´ˆë¡ìœ¼ë¡œ ì‚¬ìš© (fallback)
            abstract = text[:500].strip()

        print(f"  âœ… ì´ˆë¡ ì¶”ì¶œ ì™„ë£Œ ({len(abstract)}ì)")

        # PaperMetadata ìƒì„±
        paper = PaperMetadata(
            domain=domain,
            language='ko',
            title=title,
            abstract=abstract,
            keywords=[],
            source=source,
            year=year,
            pmid=None,
            doi=None,
            authors=authors or [],
            journal=source
        )

        return paper

    def parse_batch_from_json(self, json_path: str) -> List[PaperMetadata]:
        """
        JSON ì„¤ì • íŒŒì¼ì„ ì½ì–´ì„œ ì—¬ëŸ¬ PDFë¥¼ ì¼ê´„ ì²˜ë¦¬

        JSON í˜•ì‹:
        [
          {
            "pdf_path": "path/to/report.pdf",
            "title": "ë³´ê³ ì„œ ì œëª©",
            "domain": "korean_diet",
            "year": 2020,
            "authors": ["ë³´ê±´ë³µì§€ë¶€"],
            "source": "ë³´ê±´ë³µì§€ë¶€"
          },
          ...
        ]
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            reports_config = json.load(f)

        papers = []
        for config in reports_config:
            paper = self.parse_pdf_manual(
                pdf_path=config['pdf_path'],
                title=config['title'],
                domain=config['domain'],
                year=config.get('year'),
                authors=config.get('authors'),
                source=config.get('source', 'ì •ë¶€ë³´ê³ ì„œ')
            )
            if paper:
                papers.append(paper)

        return papers


def create_template():
    """
    ì •ë¶€ ë³´ê³ ì„œ ìˆ˜ì§‘ì„ ìœ„í•œ JSON í…œí”Œë¦¿ ìƒì„±

    ì‚¬ìš©ë²•:
    1. ì •ë¶€ ì‚¬ì´íŠ¸ì—ì„œ PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ
    2. ì´ í…œí”Œë¦¿ì— ì •ë³´ ì…ë ¥
    3. government_report_parser.pyë¡œ ì¼ê´„ ì²˜ë¦¬
    """
    template = [
        {
            "pdf_path": "downloads/mohw_nutrition_2023.pdf",
            "title": "2023 êµ­ë¯¼ê±´ê°•ì˜ì–‘ì¡°ì‚¬ ì˜ì–‘ ì„­ì·¨ ê¸°ì¤€",
            "domain": "korean_diet",
            "year": 2023,
            "authors": ["ë³´ê±´ë³µì§€ë¶€", "ì§ˆë³‘ê´€ë¦¬ì²­"],
            "source": "ë³´ê±´ë³µì§€ë¶€"
        },
        {
            "pdf_path": "downloads/mfds_functional_food_2022.pdf",
            "title": "ê¸°ëŠ¥ì„± ì‹í’ˆ ì„­ì·¨ ê°€ì´ë“œë¼ì¸",
            "domain": "protein_hypertrophy",
            "year": 2022,
            "authors": ["ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜"],
            "source": "ì‹ì•½ì²˜"
        },
        {
            "pdf_path": "downloads/ksns_protein_guideline_2021.pdf",
            "title": "í•œêµ­ì¸ì„ ìœ„í•œ ë‹¨ë°±ì§ˆ ì„­ì·¨ ê¸°ì¤€",
            "domain": "protein_hypertrophy",
            "year": 2021,
            "authors": ["ëŒ€í•œì˜ì–‘í•™íšŒ"],
            "source": "ëŒ€í•œì˜ì–‘í•™íšŒ"
        }
    ]

    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)

    template_path = output_dir / "government_reports_template.json"
    with open(template_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, ensure_ascii=False, indent=2)

    print(f"âœ… í…œí”Œë¦¿ ìƒì„±: {template_path}")
    print("\nğŸ“‹ ì‚¬ìš© ë°©ë²•:")
    print("1. ì •ë¶€/ê¸°ê´€ ì‚¬ì´íŠ¸ì—ì„œ PDF ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ")
    print("2. í…œí”Œë¦¿ì— PDF ê²½ë¡œì™€ ë©”íƒ€ë°ì´í„° ì…ë ¥")
    print("3. python government_report_parser.py ì‹¤í–‰")

    return str(template_path)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í…œí”Œë¦¿ ìƒì„±
    template_path = create_template()

    print("\n" + "=" * 60)
    print("ğŸ“š ì •ë¶€ ë³´ê³ ì„œ ìˆ˜ì§‘ ê°€ì´ë“œ")
    print("=" * 60)

    print("\nğŸ”— ì¶”ì²œ ì‚¬ì´íŠ¸:")
    print("  1. ë³´ê±´ë³µì§€ë¶€: https://www.mohw.go.kr/")
    print("     - êµ­ë¯¼ê±´ê°•ì˜ì–‘ì¡°ì‚¬ ê²°ê³¼")
    print("     - í•œêµ­ì¸ ì˜ì–‘ì„­ì·¨ê¸°ì¤€")
    print("")
    print("  2. ì§ˆë³‘ê´€ë¦¬ì²­: https://www.kdca.go.kr/")
    print("     - KNHANES ì›ì‹œìë£Œ")
    print("     - ë§Œì„±ì§ˆí™˜ ì˜ˆë°© ê°€ì´ë“œ")
    print("")
    print("  3. ì‹ì•½ì²˜: https://www.mfds.go.kr/")
    print("     - ê¸°ëŠ¥ì„± ì‹í’ˆ ê°€ì´ë“œë¼ì¸")
    print("     - ì˜ì–‘ì„±ë¶„ ê¸°ì¤€")
    print("")
    print("  4. ëŒ€í•œì˜ì–‘í•™íšŒ: http://www.kns.or.kr/")
    print("     - ë‹¨ë°±ì§ˆ ì„­ì·¨ ê¸°ì¤€")
    print("     - í•œêµ­ì¸ ì˜ì–‘ì†Œ ì„­ì·¨ê¸°ì¤€")
    print("")
    print("  5. ì²´ìœ¡ê³¼í•™ì—°êµ¬ì›: https://www.sports.re.kr/")
    print("     - ìš´ë™ ì˜ì–‘ ê°€ì´ë“œ")
    print("     - ì²´ë ¥ í‰ê°€ ê¸°ì¤€")

    print("\nğŸ“¥ ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:")
    print("  - ë³´ê±´ë³µì§€ë¶€: 30-50ê°œ ë³´ê³ ì„œ")
    print("  - ì§ˆë³‘ê´€ë¦¬ì²­: 20-30ê°œ ë³´ê³ ì„œ")
    print("  - ì‹ì•½ì²˜: 10-20ê°œ ë³´ê³ ì„œ")
    print("  - í•™íšŒ: 20-30ê°œ ê°€ì´ë“œë¼ì¸")
    print("  - ì´ 80-130ê°œ ê³µì‹ ë¬¸ì„œ")

    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. {template_path} íŒŒì¼ ìˆ˜ì •")
    print("  2. PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
    print("  3. python government_report_parser.py --process ì‹¤í–‰")

    # ë§Œì•½ --process ì˜µì…˜ì´ ìˆìœ¼ë©´ ì‹¤ì œ ì²˜ë¦¬
    import sys
    if '--process' in sys.argv:
        parser = GovernmentReportParser()

        if Path(template_path).exists():
            print("\n" + "=" * 60)
            print("ğŸš€ ë³´ê³ ì„œ ì²˜ë¦¬ ì‹œì‘")
            print("=" * 60)

            papers = parser.parse_batch_from_json(template_path)

            if papers:
                # ê²°ê³¼ ì €ì¥
                output_dir = Path("outputs")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                output_path = output_dir / f"government_reports_{timestamp}.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump([p.model_dump() for p in papers], f, ensure_ascii=False, indent=2)

                print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(papers)}ê°œ ë¬¸ì„œ")
                print(f"   ì €ì¥ ê²½ë¡œ: {output_path}")
            else:
                print("\nâš ï¸ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâŒ í…œí”Œë¦¿ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {template_path}")


if __name__ == "__main__":
    main()
