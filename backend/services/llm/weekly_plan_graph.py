import asyncio
from typing import TypedDict, Annotated, Optional
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv

load_dotenv()

from services.llm.llm_clients import create_llm_client
from schemas.llm import GoalPlanInput
from schemas.inbody import InBodyData as InBodyMeasurements
from .rule_based_prompts import (
    create_summary_prompt,
    create_workout_prompt,
    create_diet_prompt,
    create_lifestyle_prompt,
)


# --- 1. ìƒíƒœ ì •ì˜ ---
class PlanState(TypedDict):
    """LLM2 (ì£¼ê°„ ê³„íš / Q&A) ì—ì´ì „íŠ¸ì˜ ìƒíƒœ"""
    plan_input: GoalPlanInput
    messages: Annotated[list, add_messages]
    # ì‚¬ìš©ì í”¼ë“œë°±ì„ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ í•„ë“œ ì¶”ê°€
    feedback_category: Optional[str]
    feedback_text: Optional[str]
    # ê¸°ì¡´ ê³„íš ë‚´ìš© ì£¼ì…ìš© (ìƒˆ ì„¸ì…˜ì—ì„œ ë§¥ë½ ì œê³µ)
    existing_plan: Optional[str]


# --- 2. ë³‘ë ¬ ë¹„ë™ê¸° ì´ˆê¸° ê³„íš ìƒì„± í•¨ìˆ˜ ---
async def generate_initial_plan_concurrently(state: PlanState, llm_client) -> dict:
    """
    Node: ì£¼ê°„ ê³„íš ì´ˆì•ˆ ë³‘ë ¬ ìƒì„±
    4ê°œì˜ LLM Call(ìš”ì•½, ìš´ë™, ì‹ë‹¨, ë¼ì´í”„ìŠ¤íƒ€ì¼)ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì·¨í•©í•©ë‹ˆë‹¤.
    """
    print("--- LLM2: ì£¼ê°„ ê³„íš ìƒì„± (ë³‘ë ¬ ì‹¤í–‰) ---")
    plan_input = state["plan_input"]

    # InBody ë°ì´í„° ëª¨ë¸ ë³€í™˜
    measurements = InBodyMeasurements(**plan_input.measurements)
    user_profile = plan_input.user_profile if hasattr(plan_input, 'user_profile') else {}

    # --- 4ê°€ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„± ---
    prompts = {
        "summary": create_summary_prompt(
            goal_input=plan_input, measurements=measurements, rag_context="", user_profile=user_profile
        ),
        "workout": create_workout_prompt(
            goal_input=plan_input, measurements=measurements, rag_context="", user_profile=user_profile
        ),
        "diet": create_diet_prompt(
            goal_input=plan_input, measurements=measurements, rag_context="", user_profile=user_profile
        ),
        "lifestyle": create_lifestyle_prompt(
            goal_input=plan_input, measurements=measurements, rag_context="", user_profile=user_profile
        ),
    }

    # --- LLM ë¹„ë™ê¸° í˜¸ì¶œ íƒœìŠ¤í¬ ìƒì„± ---
    tasks = []
    for key, (system_prompt, user_prompt) in prompts.items():
        task = llm_client.agenerate_chat(system_prompt, user_prompt, key)
        tasks.append(task)

    # --- ëª¨ë“  LLM í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ ---
    results = await asyncio.gather(*tasks)

    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
    plan_results = {res['key']: res['content'] for res in results}
    summary_result = plan_results.get("summary", "ì£¼ê°„ ëª©í‘œ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    workout_result = plan_results.get("workout", "ìš´ë™ ê³„íš ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    diet_result = plan_results.get("diet", "ì‹ë‹¨ ê³„íš ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    lifestyle_result = plan_results.get("lifestyle", "ìƒí™œ ìŠµê´€ ë° ë™ê¸°ë¶€ì—¬ ë©”ì‹œì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    # --- ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ---
    combined_response = f"""### ğŸ“ ì£¼ê°„ ëª©í‘œ í•µì‹¬ ì „ëµ
{summary_result}

### ğŸ‹ï¸â€â™€ï¸ ìš”ì¼ë³„ ìš´ë™ ê³„íš
{workout_result}

### ğŸ½ï¸ ì¼ì¼ ì‹ë‹¨ ê³„íš
{diet_result}

### ğŸ’¡ ìƒí™œ ìŠµê´€ ë° ë™ê¸°ë¶€ì—¬
{lifestyle_result}

---
ìœ„ ê³„íšì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆê±°ë‚˜ ìˆ˜ì •ì„ ì›í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”!
"""

    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” ìƒëµí•˜ê³  AI ì‘ë‹µë§Œ ë°˜í™˜
    return {"messages": [("ai", combined_response)]}


# --- 3. ê·¸ë˜í”„ ìƒì„± ---
def create_weekly_plan_agent(llm_client):
    """ì£¼ê°„ ê³„íš ìƒì„± ë° ìˆ˜ì •ì„ ìœ„í•œ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„±"""

    # --- 2. ë…¸ë“œ ì •ì˜ ---
    def _generate_feedback_response(state: PlanState, category_name: str, system_prompt: str) -> dict:
        """ê³µí†µ í”¼ë“œë°± ê¸°ë°˜ ë‹µë³€ ìƒì„± ë¡œì§"""
        print(f"--- LLM2: ì£¼ê°„ ê³„íš í”¼ë“œë°± ë°˜ì˜ ({category_name}) ---")

        # [ê°•ë ¥ ì¡°ì¹˜] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì§ì ‘ í˜„ì¬ ê³„íš ì •ë³´ ì£¼ì…
        existing_plan = state.get("existing_plan")
        if existing_plan:
            system_prompt += f"\n\n[ì°¸ê³ : í˜„ì¬ ì‚¬ìš©ìì˜ ì£¼ê°„ ê³„íš ì •ë³´]\n{existing_plan}\n\nì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì´ ìœ„ ê³„íšê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´, ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
            print(f"--- [DEBUG] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— existing_plan ì»¨í…ìŠ¤íŠ¸ ì£¼ì… ì™„ë£Œ ---")

        for i, msg in enumerate(state["messages"]):
            msg_content = msg.content if msg.content else ""
            print(f"--- [DEBUG] msg[{i}] type={msg.type}, content={msg_content[:50].replace(chr(10), ' ')}... ---")

        history = []

        # AI ë©”ì‹œì§€(ê¸°ì¡´ ê³„íš)ê°€ íˆìŠ¤í† ë¦¬ì— ì—†ê³  existing_planì´ ìˆìœ¼ë©´ ë§¥ë½ìœ¼ë¡œ ì£¼ì…
        has_ai_message = any(msg.type == "ai" for msg in state["messages"])
        if not has_ai_message and existing_plan:
            print("--- [DEBUG] íˆìŠ¤í† ë¦¬ì— ê¸°ì¡´ ê³„íš ì£¼ì… (Assistant ë©”ì‹œì§€ë¡œ ì¶”ê°€) ---")
            history.append(("assistant", existing_plan))
        else:
            print(f"--- [DEBUG] íˆìŠ¤í† ë¦¬ ì£¼ì… ìŠ¤í‚µ: has_ai_msg={has_ai_message}, has_plan={bool(existing_plan)} ---")

        for msg in state["messages"]:
            role = "user" if msg.type == "human" else "assistant"
            # OpenAI APIëŠ” contentê°€ Noneì¸ ê²ƒì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            content = msg.content if msg.content is not None else ""
            history.append((role, content))

        feedback_text = state.get("feedback_text")
        if feedback_text:
            history.append(("user", feedback_text))

        print(f"--- [DEBUG] LLM í˜¸ì¶œ ì§ì „ íˆìŠ¤í† ë¦¬ ì´ ë©”ì‹œì§€ ìˆ˜: {len(history)} ---")

        response = llm_client.generate_chat_with_history(
            system_prompt=system_prompt,
            messages=history
        )

        # í”¼ë“œë°± ì²˜ë¦¬ í›„ ìƒíƒœ ì´ˆê¸°í™”
        return {"messages": [("ai", response)], "feedback_category": None, "feedback_text": None}

    def adjust_exercise_plan(state: PlanState) -> dict:
        """Node 2-1: ìš´ë™ í”Œëœ ì¡°ì •"""
        system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìš´ë™ í”Œëœ(ì¼ì •, ì¢…ëª©, ë¶„í•  ë°©ì‹ ë“±)ì˜ ì¡°ì •ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •ëœ **êµ¬ì²´ì ì¸ ì „ì²´ ì£¼ê°„ ìš´ë™ ê³„íší‘œ**ë¥¼ ë‹¤ì‹œ ì œì‹œí•´ì£¼ì„¸ìš”.
        ìˆ˜ì •ëœ ì´ìœ ë„ í•¨ê»˜ ì„¤ëª…í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤."""
        return _generate_feedback_response(state, "ìš´ë™ í”Œëœ ì¡°ì •", system_prompt)

    def adjust_diet_plan(state: PlanState) -> dict:
        """Node 2-2: ì‹ë‹¨ ì¡°ì •"""
        system_prompt = """ë‹¹ì‹ ì€ ì˜ì–‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì‹ë‹¨ ê³„íšì˜ ì¡°ì •ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ í”¼ë“œë°±(ê¸°í˜¸, ì•Œë ˆë¥´ê¸°, ìƒí™© ë“±)ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •ëœ **êµ¬ì²´ì ì¸ ì „ì²´ ì£¼ê°„ ì‹ë‹¨ ê³„íší‘œ**ë¥¼ ë‹¤ì‹œ ì œì‹œí•´ì£¼ì„¸ìš”.
        ì¹¼ë¡œë¦¬ì™€ ì˜ì–‘ ë°¸ëŸ°ìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ ì¡°ì–¸í•´ì£¼ì„¸ìš”."""
        return _generate_feedback_response(state, "ì‹ë‹¨ ì¡°ì •", system_prompt)

    def adjust_intensity(state: PlanState) -> dict:
        """Node 2-3: ê°•ë„ ì¡°ì •"""
        system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìš´ë™ ê°•ë„(ë¬´ê²Œ, íšŸìˆ˜, ì„¸íŠ¸, íœ´ì‹ ì‹œê°„ ë“±)ì˜ ì¡°ì •ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ê°•ë„ë¥¼ ë†’ì´ê±°ë‚˜ ë‚®ì¶˜ **êµ¬ì²´ì ì¸ ì „ì²´ ì£¼ê°„ ìš´ë™ ê³„íší‘œ**ë¥¼ ë‹¤ì‹œ ì œì‹œí•´ì£¼ì„¸ìš”.
        ë¶€ìƒ ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ì–¸ë„ í¬í•¨í•´ì£¼ì„¸ìš”."""
        return _generate_feedback_response(state, "ê°•ë„ ì¡°ì •", system_prompt)

    def qa_general(state: PlanState) -> dict:
        """Node 2-4: ì¼ë°˜ Q&A"""
        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì£¼ê°„ ìš´ë™ ë° ì‹ë‹¨ ê³„íšì„ ë‹´ë‹¹í•˜ëŠ” í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìƒì„±ëœ ê³„íšì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ì´ì „ ëŒ€í™” ë§¥ë½(ì‚¬ìš©ìì˜ ì‹ ì²´ ì •ë³´, ëª©í‘œ, ìƒì„±ëœ ê³„íš)ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."""
        return _generate_feedback_response(state, "ì¼ë°˜ Q&A", system_prompt)

    def router(state: PlanState) -> dict:
        """ë¼ìš°íŒ…ì„ ìœ„í•œ ë¹ˆ ë…¸ë“œ. ìƒíƒœ ë³€ê²½ ì—†ìŒ."""
        print("--- ë¼ìš°í„° ì§„ì… ---")
        return {}

    def finalize_plan(state: PlanState) -> dict:
        """Node 3: ê³„íš í™•ì • ë° ì €ì¥"""
        print("--- LLM2: ê³„íš í™•ì • ---")
        final_plan_message = state["messages"][-1].content
        return {
            "messages": [
                ("ai", f"ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. í˜„ì¬ ê³„íšì„ ìµœì¢… í”Œëœìœ¼ë¡œ ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤.\n\n{final_plan_message}")
            ]
        }

    def route_feedback(state: PlanState) -> str:
        """ì‚¬ìš©ì í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        print("--- [DEBUG] route_feedback function called ---")
        category = state.get("feedback_category")
        print(f"--- [DEBUG] Current State 'feedback_category': {category}")

        if not category:
            # feedback_categoryê°€ ì—†ì„ ê²½ìš°, ì¼ë°˜ Q&A ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
            # ê¸°ì¡´ chat API í˜¸ì¶œì€ ì—¬ê¸°ì— í•´ë‹¹í•˜ë©° qa_general ë…¸ë“œë¡œ ë¼ìš°íŒ…
            print("--- ë¼ìš°íŒ…: í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ ì—†ìŒ, ì¼ë°˜ Q&A (í´ë°±) ---")
            return "qa_general" # ê¸°ì¡´ chat API í˜¸í™˜ì„±ì„ ìœ„í•´ qa_generalë¡œ ë°”ë¡œ ë¼ìš°íŒ…

        print(f"--- ë¼ìš°íŒ…: {category} ---")
        if category == "ìš´ë™ í”Œëœ ì¡°ì •" or category == "adjust_exercise_plan":
            return "adjust_exercise_plan"
        elif category == "ì‹ë‹¨ ì¡°ì •" or category == "adjust_diet_plan":
            return "adjust_diet_plan"
        elif category == "ê°•ë„ ì¡°ì •" or category == "adjust_intensity":
            return "adjust_intensity"
        elif category == "ìµœì¢… í”Œëœìœ¼ë¡œ ì €ì¥" or category == "finalize_plan":
            return "finalize_plan"
        else:
            # ì •ì˜ë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ì˜ ê²½ìš°ì—ë„ ì¼ë°˜ Q&Aë¡œ ì²˜ë¦¬
            print(f"--- ë¼ìš°íŒ…: ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ '{category}', ì¼ë°˜ Q&Aë¡œ ì²˜ë¦¬ ---")
            return "qa_general"

    workflow = StateGraph(PlanState)

    # ë¹„ë™ê¸° ë…¸ë“œ ì¶”ê°€ (lambdaë¡œ ë˜í•‘)
    workflow.add_node(
        "initial_plan",
        lambda state: generate_initial_plan_concurrently(state, llm_client)
    )
    workflow.add_node("router", router)
    workflow.add_node("adjust_exercise_plan", adjust_exercise_plan)
    workflow.add_node("adjust_diet_plan", adjust_diet_plan)
    workflow.add_node("adjust_intensity", adjust_intensity)
    workflow.add_node("qa_general", qa_general)
    workflow.add_node("finalize_plan", finalize_plan)

    def decide_entry_point(state: PlanState) -> str:
        """ì§„ì…ì  ê²°ì • ë¡œì§: ì²« ì‹¤í–‰(ë©”ì‹œì§€ ì—†ìŒ)ì´ë©´ initial_plan, ì•„ë‹ˆë©´ router"""
        messages = state.get("messages", [])
        category = state.get("feedback_category")

        # ë©”ì‹œì§€ê°€ ìˆê±°ë‚˜ í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ ëŒ€í™” -> ë¼ìš°í„°
        if (messages and len(messages) > 0) or category:
            print(f"--- [DEBUG] ì§„ì…ì  ê²°ì •: Router (msgs={len(messages)}, cat={category}) ---")
            return "router"

        # ì•„ë¬´ ê¸°ë¡ë„ ì—†ìœ¼ë©´ ì´ˆê¸° ê³„íš ìƒì„±
        print("--- [DEBUG] ì§„ì…ì  ê²°ì •: Initial Plan (ì²« ì‹¤í–‰) ---")
        return "initial_plan"

    workflow.set_conditional_entry_point(
        decide_entry_point,
        {
            "router": "router",
            "initial_plan": "initial_plan"
        }
    )

    # ì´ˆê¸° ê³„íš ìƒì„± í›„ ì¢…ë£Œ (ì‚¬ìš©ì í”¼ë“œë°± ëŒ€ê¸°)
    workflow.add_edge("initial_plan", END)

    # ê° í”¼ë“œë°± ì¡°ì • í›„ ì¢…ë£Œ (Request/Response ëª¨ë¸ì´ë¯€ë¡œ í„´ ì¢…ë£Œ)
    workflow.add_edge("adjust_exercise_plan", END)
    workflow.add_edge("adjust_diet_plan", END)
    workflow.add_edge("adjust_intensity", END)
    workflow.add_edge("qa_general", END)

    # ë¼ìš°í„°ì—ì„œ ì¡°ê±´ì— ë”°ë¼ ë¶„ê¸°
    feedback_routing_map = {
        "adjust_exercise_plan": "adjust_exercise_plan",
        "adjust_diet_plan": "adjust_diet_plan",
        "adjust_intensity": "adjust_intensity",
        "qa_general": "qa_general",
        "finalize_plan": "finalize_plan",
        END: END
    }
    workflow.add_conditional_edges("router", route_feedback, feedback_routing_map)

    # ìµœì¢… ë…¸ë“œì—ì„œ ê·¸ë˜í”„ ì¢…ë£Œ
    workflow.add_edge("finalize_plan", END)

    memory = MemorySaver()

    # interrupt_before ì œê±° (ë¼ìš°í„° ì§„ì… ì‹œ ë©ˆì¶”ì§€ ì•Šê³  ì¦‰ì‹œ ì‹¤í–‰)
    return workflow.compile(checkpointer=memory)
