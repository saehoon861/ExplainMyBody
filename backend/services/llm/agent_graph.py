from typing import TypedDict, Optional, Annotated, Dict, List
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv

load_dotenv()

from services.llm.llm_clients import BaseLLMClient
from schemas.llm import StatusAnalysisInput
from .prompt_generator import create_inbody_analysis_prompt
from schemas.inbody import InBodyData as InBodyMeasurements


# --- Custom Reducer: ê¸°ì¡´ ê°’ ìœ ì§€ ---
def keep_existing(existing, new):
    """
    ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ ìƒˆ ê°’ ì‚¬ìš©
    ì´ë ‡ê²Œ í•˜ë©´ analysis_inputì´ ì²« invoke()ì—ì„œ ì„¤ì •ë˜ë©´ ê³„ì† ìœ ì§€ë¨
    """
    return existing if existing is not None else new


# --- 1. ìƒíƒœ ì •ì˜ ---
class AnalysisState(TypedDict):
    """LLM1 (ê±´ê°• ìƒíƒœ ë¶„ì„ / Q&A) ì—ì´ì „íŠ¸ì˜ ìƒíƒœ"""
    # ì…ë ¥: ê±´ê°• ê¸°ë¡ ë°ì´í„° (ì²« ì„¤ì • í›„ ê³„ì† ìœ ì§€)
    analysis_input: Annotated[Optional[StatusAnalysisInput], keep_existing]
    # ëŒ€í™” ê¸°ë¡ (HumanMessage, AIMessageì˜ ë¦¬ìŠ¤íŠ¸)
    # add_messagesëŠ” ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    messages: Annotated[list, add_messages]
    # ìƒì„±ëœ ì„ë² ë”© ë²¡í„° (ì²« ì„¤ì • í›„ ê³„ì† ìœ ì§€)
    embedding: Annotated[Optional[Dict[str, List[float]]], keep_existing]
    
    
# --- 3. ê·¸ë˜í”„ ìƒì„± ---
def create_analysis_agent(llm_client: BaseLLMClient):
    """
    ê±´ê°• ë¶„ì„ ë° íœ´ë¨¼ í”¼ë“œë°± Q&A ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    
    - `interrupt_after`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° AI ì‘ë‹µ í›„ì— ë©ˆì¶”ê³  ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    - ì‚¬ìš©ìê°€ '5. ê´œì°®ìŠµë‹ˆë‹¤'ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì€ ì„œë¹„ìŠ¤ ê³„ì¸µì—ì„œ ì²˜ë¦¬í•˜ë©°,
      ë” ì´ìƒ ê·¸ë˜í”„ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.
    """
    
    # --- 2. ë…¸ë“œ(ê·¸ë˜í”„ì˜ ê° ë‹¨ê³„) ì •ì˜ ---
    def generate_initial_analysis(state: AnalysisState) -> dict:
        """Node 1: ìµœì´ˆ ë¶„ì„ ê²°ê³¼ ìƒì„± ë° ì„ë² ë”©"""
        print("[DEBUG][initial_analysis] ===== NODE ENTER =====")
        print(f"[DEBUG][initial_analysis] state keys: {list(state.keys())}")
        print(f"[DEBUG][initial_analysis] messages count: {len(state.get('messages', []))}")
        if state.get("messages"):
            last_msg = state["messages"][-1]
            print(f"[DEBUG][initial_analysis] last message: type={last_msg.type}, content[:80]='{str(last_msg.content)[:80]}'")

        # ğŸ”§ Resume ë°©ì§€: ì´ë¯¸ AI ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì´ë¯¸ ë¶„ì„ ì™„ë£Œ â†’ skip
        if state.get("messages"):
            for msg in state["messages"]:
                if msg.type == "ai":
                    print("[DEBUG][initial_analysis] !! AI ë©”ì‹œì§€ ì´ë¯¸ ì¡´ì¬ â†’ ì´ë¯¸ ë¶„ì„ ì™„ë£Œ, PASSTHROUGH")
                    return {}

        analysis_input = state.get("analysis_input")
        print(f"[DEBUG][initial_analysis] analysis_input present: {analysis_input is not None}")
        if not analysis_input:
            # ì²´í¬í¬ì¸íŠ¸ ì†Œì‹¤ í›„ ì¬ì‹¤í–‰ëœ ê²½ìš° â€” route_qaë¡œ íŒ¨ìŠ¤ìŠ¤ë£¨
            print("[DEBUG][initial_analysis] !! PASSTHROUGH â€” analysis_input ì—†ìŒ, route_qaë¡œ ì§„í–‰")
            return {}

        print(f"[DEBUG][initial_analysis] user_id={analysis_input.user_id}, record_id={analysis_input.record_id}")

        # 1. InBodyMeasurements ëª¨ë¸ë¡œ ë³€í™˜ (prompt_generatorê°€ ìš”êµ¬í•˜ëŠ” íƒ€ì…)
        measurements = InBodyMeasurements(**analysis_input.measurements)

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
        system_prompt, user_prompt = create_inbody_analysis_prompt(
            measurements,
            body_type1=analysis_input.body_type1,
            body_type2=analysis_input.body_type2
        )
        print(f"[DEBUG][initial_analysis] prompt ready, calling LLM...")
        response = llm_client.generate_chat(system_prompt, user_prompt)
        print(f"[DEBUG][initial_analysis] LLM response length: {len(response)}, preview: '{response[:100]}'")

        # --- 3. ì„ë² ë”© ìƒì„± (embedder.py ë¡œì§ ë°˜ì˜) ---
        print("[DEBUG][initial_analysis] ì„ë² ë”© ìƒì„± ì¤‘...")
        embedding_1536 = None
        embedding_1024 = None

        # 3-1. OpenAI ì„BEë”© ìƒì„± (1536ì°¨ì›)
        try:
            embedding_1536 = llm_client.create_embedding(text=response)
            print(f"[DEBUG][initial_analysis] OpenAI ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(embedding_1536)})")
        except Exception as e:
            print(f"[DEBUG][initial_analysis] OpenAI ì„ë² ë”© ì‹¤íŒ¨: {e}")

        final_embedding = {
            "embedding_1536": embedding_1536,
            "embedding_1024": embedding_1024,
        }

        print("[DEBUG][initial_analysis] ===== NODE COMPLETE (interrupt_after ëŒ€ê¸°) =====")
        return {
            "messages": [("human", user_prompt), ("ai", response)],
            "embedding": final_embedding
        }

    def format_measurements(data: dict, indent: int = 0) -> list:
        """
        InBody ì¸¡ì • ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ í¬ë§·íŒ…
        ëª¨ë“  í•„ë“œë¥¼ ìë™ìœ¼ë¡œ ìˆœíšŒí•˜ì—¬ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        lines = []
        prefix = "  " * indent

        for key, value in data.items():
            if isinstance(value, dict):
                # ì¤‘ì²© ê°ì²´ (ë¶€ìœ„ë³„ê·¼ìœ¡ë¶„ì„, ë¶€ìœ„ë³„ì²´ì§€ë°©ë¶„ì„ ë“±)
                lines.append(f"{prefix}**{key}:**")
                lines.extend(format_measurements(value, indent + 1))
            elif isinstance(value, list):
                # ë¦¬ìŠ¤íŠ¸ (ë“œë¬¼ì§€ë§Œ ì²˜ë¦¬)
                lines.append(f"{prefix}- {key}: {', '.join(map(str, value))}")
            else:
                # ë‹¨ìˆœ ê°’ (ìˆ«ì, ë¬¸ìì—´ ë“±)
                lines.append(f"{prefix}- {key}: {value}")

        return lines

    def _generate_qa_response(state: AnalysisState, category_name: str, system_prompt: str) -> dict:
        """ê³µí†µ Q&A ë‹µë³€ ìƒì„± ë¡œì§"""
        print(f"\n[DEBUG][qa:{category_name}] ===== NODE ENTER =====")
        print(f"[DEBUG][qa:{category_name}] messages count: {len(state.get('messages', []))}")

        # ğŸ”§ InBody ì¸¡ì • ë°ì´í„°ë¥¼ System Promptì— ì¶”ê°€ (ëª¨ë“  í•„ë“œ ìë™ í¬í•¨)
        analysis_input = state.get("analysis_input")
        if analysis_input and analysis_input.measurements:
            measurements = analysis_input.measurements

            # ëª¨ë“  ì¸¡ì • ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ í¬ë§·íŒ… (30ê°œ+ í•„ë“œ ì „ë¶€ í¬í•¨)
            key_metrics_lines = ["**[ì‚¬ìš©ìì˜ í˜„ì¬ InBody ì¸¡ì • ë°ì´í„°]**"]
            key_metrics_lines.extend(format_measurements(measurements))
            key_metrics = "\n".join(key_metrics_lines)

            # System Prompt ê°•í™”
            enhanced_prompt = f"""{system_prompt}

{key_metrics}

âš ï¸ ìœ„ ì¸¡ì •ê°’ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬, ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ë“¤ì–´ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì¼ë°˜ì ì¸ ì„¤ëª…ì´ ì•„ë‹Œ, ìœ„ ë°ì´í„° ê¸°ë°˜ì˜ ê°œì¸í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
        else:
            enhanced_prompt = system_prompt
            print(f"[DEBUG][qa:{category_name}] âš ï¸ analysis_input ì—†ìŒ, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")

        # LangGraph ë©”ì‹œì§€ ê°ì²´ë¥¼ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì´í•´í•˜ëŠ” íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        history = []
        for msg in state["messages"]:
            role = "user" if msg.type == "human" else "assistant"
            history.append((role, msg.content))

        # ë§ˆì§€ë§‰ human ë©”ì‹œì§€ (ì‚¬ìš©ì ì§ˆë¬¸) ì¶œë ¥
        last_human = next((c for r, c in reversed(history) if r == "user"), None)
        print(f"[DEBUG][qa:{category_name}] last human message: '{str(last_human)[:100]}'")
        print(f"[DEBUG][qa:{category_name}] history tuples: {len(history)} (roles: {[r for r, _ in history]})")

        # ì‹¤ì œ LLM í˜¸ì¶œ (ëŒ€í™” ê¸°ë¡ í¬í•¨)
        print(f"[DEBUG][qa:{category_name}] calling LLM with history...")
        response = llm_client.generate_chat_with_history(
            system_prompt=enhanced_prompt,  # â† ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            messages=history
        )

        print(f"[DEBUG][qa:{category_name}] LLM response length: {len(response)}, preview: '{response[:100]}'")
        print(f"[DEBUG][qa:{category_name}] ===== NODE COMPLETE (interrupt_after ëŒ€ê¸°) =====\n")
        return {"messages": [("ai", response)]}

    def qa_strength_weakness(state: AnalysisState) -> dict:
        """Node 2-1: ì¸ë°”ë”” ì¢…í•©ì˜ê²¬ Q&A"""
        system_prompt = """ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ì˜ ì²´ì„±ë¶„ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìì‹ ì˜ ì‹ ì²´ ê°•ì ê³¼ ì•½ì ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
        ì´ì „ ëŒ€í™”ì—ì„œ ì œê³µëœ ì¸ë°”ë”” ë°ì´í„°ì™€ ìµœì´ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        - **ê°•ì **: í‘œì¤€ ë²”ìœ„ ì´ìƒì´ê±°ë‚˜ ê¸ì •ì ì¸ ì§€í‘œ (ì˜ˆ: ë†’ì€ ê³¨ê²©ê·¼ëŸ‰, ì ì • ì²´ìˆ˜ë¶„ ë“±)
        - **ì•½ì **: ê°œì„ ì´ í•„ìš”í•œ ì§€í‘œ (ì˜ˆ: ë†’ì€ ì²´ì§€ë°©ë¥ , ë¶€ìœ„ë³„ ë¶ˆê· í˜•, ë‚®ì€ ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ ë“±)
        - **ì¢…í•© í‰ê°€**: í˜„ì¬ ì‹ ì²´ì˜ ê°€ì¥ í° íŠ¹ì§•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: êµ¬ì²´ì ì¸ ìš´ë™ ê³„íšì´ë‚˜ ì‹ë‹¨ ê³„íšì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. í˜„ì¬ ìƒíƒœ ë¶„ì„ë§Œ ì œê³µí•˜ì„¸ìš”."""
        return _generate_qa_response(state, "ê°•ì /ì•½ì ", system_prompt)

    def qa_health_status(state: AnalysisState) -> dict:
        """Node 2-2: ì´ì „ ì¸ë°”ë””ì™€ ë¹„êµ Q&A"""
        system_prompt = """ë‹¹ì‹ ì€ ì˜ˆë°© ì˜í•™ ê´€ì ì—ì„œ ì¡°ì–¸í•˜ëŠ” ê±´ê°• ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ í˜„ì¬ ìì‹ ì˜ ì „ë°˜ì ì¸ ê±´ê°• ìƒíƒœì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê±´ê°• ê´€ì ì—ì„œ ê¸ì •ì ì¸ ë¶€ë¶„ê³¼ ì ì¬ì ì¸ ìœ„í—˜ ìš”ì†Œë¥¼ ë‚˜ëˆ„ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        - **ê¸ì •ì  ì‹ í˜¸**: ì •ìƒ ë²”ìœ„ì— ìˆëŠ” BMI, ê·¼ìœ¡ëŸ‰, í˜ˆì•• ê´€ë ¨ ì§€í‘œ ë“±
        - **ì£¼ì˜/ê²½ê³  ì‹ í˜¸**: ë³µë¶€ì§€ë°©ë¥ , ë‚´ì¥ì§€ë°©ë ˆë²¨ ë“± ê±´ê°• ìœ„í—˜ë„ì™€ ì§ê²°ë˜ëŠ” ì§€í‘œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì–´ë–¤ ì§ˆë³‘ì˜ ìœ„í—˜ì„ ë†’ì¼ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”. (ì˜í•™ì  ì§„ë‹¨ì´ ì•„ë‹˜ì„ ëª…ì‹œ)
        - **ê²°ë¡ **: í˜„ì¬ ìƒíƒœê°€ 'ë§¤ìš° ê±´ê°•', 'ê±´ê°•í•œ í¸', 'ì£¼ì˜ í•„ìš”', 'ê´€ë¦¬ í•„ìš”' ì¤‘ ì–´ë””ì— ê°€ê¹Œìš´ì§€ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: êµ¬ì²´ì ì¸ ìš´ë™ ê³„íšì´ë‚˜ ì‹ë‹¨ ê³„íšì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. í˜„ì¬ ìƒíƒœ ë¶„ì„ë§Œ ì œê³µí•˜ì„¸ìš”."""
        return _generate_qa_response(state, "ê±´ê°• ìƒíƒœ", system_prompt)

    def qa_impact(state: AnalysisState) -> dict:
        """Node 2-3: ì§ˆë³‘ ë° íŠ¹ì´ì‚¬í•­"""
        system_prompt = """ë‹¹ì‹ ì€ ìš´ë™ìƒë¦¬í•™ìì´ì ë¼ì´í”„ìŠ¤íƒ€ì¼ ì½”ì¹˜ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ í˜„ì¬ ì‹ ì²´ ìƒíƒœê°€ ì¼ìƒê³¼ ìš´ë™ ìˆ˜í–‰ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, í˜„ì¬ ì²´ì„±ë¶„ ìƒíƒœê°€ ì–´ë–¤ ê²°ê³¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        - **ìš´ë™ ìˆ˜í–‰ëŠ¥ë ¥**: í˜„ì¬ ê·¼ìœ¡ëŸ‰ê³¼ ì²´ì§€ë°©ëŸ‰ì´ ê·¼ë ¥, ì§€êµ¬ë ¥, ìˆœë°œë ¥ ë“±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ì˜ˆ: 'í•˜ì²´ ê·¼ìœ¡ì´ ë°œë‹¬í•˜ì—¬ ìŠ¤ì¿¼íŠ¸ë‚˜ ë“±ì‚°ì— ìœ ë¦¬í•˜ì§€ë§Œ, ì²´ì¤‘ ëŒ€ë¹„ ìƒì²´ ê·¼ë ¥ì´ ë¶€ì¡±í•˜ì—¬ í„±ê±¸ì´ ê°™ì€ ìš´ë™ì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
        - **ì¼ìƒ ìƒí™œ**: ê¸°ì´ˆëŒ€ì‚¬ëŸ‰, ì²´ë ¥ ìˆ˜ì¤€ì´ ì¼ìƒì ì¸ í”¼ë¡œë„, í™œë™ì„±, ìì„¸ ìœ ì§€ ë“±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ì˜ˆ: 'ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ì´ ë‚®ì•„ ì‰½ê²Œ í”¼ë¡œê°ì„ ëŠë‚„ ìˆ˜ ìˆìœ¼ë©°, ì½”ì–´ ê·¼ìœ¡ ë¶€ì¡±ìœ¼ë¡œ ì˜¤ë˜ ì•‰ì•„ìˆì„ ë•Œ í—ˆë¦¬ í†µì¦ì„ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.')

        âš ï¸ ì¤‘ìš”: êµ¬ì²´ì ì¸ ìš´ë™ ê³„íšì´ë‚˜ ì‹ë‹¨ ê³„íšì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. ì˜í–¥ ë¶„ì„ë§Œ ì œê³µí•˜ì„¸ìš”."""
        return _generate_qa_response(state, "ì¼ìƒ/ìš´ë™ ì˜í–¥", system_prompt)

    def qa_priority(state: AnalysisState) -> dict:
        """Node 2-4: ê°œì„  ì‚¬í•­ Q&A"""
        system_prompt = """ë‹¹ì‹ ì€ ë™ê¸°ë¶€ì—¬ê°€ ë›°ì–´ë‚œ í˜„ì‹¤ì ì¸ ì²´ì„±ë¶„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ê°€ì¥ ë¨¼ì € ê°œì„ í•´ì•¼ í•  ìš°ì„ ìˆœìœ„ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬, ê°€ì¥ ì‹œê¸‰í•˜ê³  íš¨ê³¼ê°€ í° 'ê°œì„  ì˜ì—­'ì„ 3ê°€ì§€ ìš°ì„ ìˆœìœ„ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”.
        - **1ìˆœìœ„ (ê°€ì¥ ì‹œê¸‰)**: ê±´ê°• ìœ„í—˜ì„ ë‚®ì¶”ê±°ë‚˜, ê°€ì¥ í° ë¶ˆê· í˜•ì„ í•´ì†Œí•´ì•¼ í•  ì˜ì—­ (ì˜ˆ: ë‚´ì¥ì§€ë°© ê°ì†Œ í•„ìš”, ë³µë¶€ë¹„ë§Œ ê°œì„  í•„ìš”)
        - **2ìˆœìœ„ (ì²´ê° íš¨ê³¼ê°€ í° ê²ƒ)**: ë‹¨ê¸°ê°„ì— ë³€í™”ë¥¼ ëŠë¼ê±°ë‚˜, ë‹¤ë¥¸ ëŠ¥ë ¥ í–¥ìƒì— ê¸°ë°˜ì´ ë˜ëŠ” ì˜ì—­ (ì˜ˆ: ì½”ì–´ ê·¼ë ¥ ë¶€ì¡±, í•˜ì²´ ê·¼ìœ¡ ë¶ˆê· í˜•)
        - **3ìˆœìœ„ (ì¥ê¸°ì  ê´€ì )**: ê¾¸ì¤€íˆ ê°œì„ í•´ë‚˜ê°€ì•¼ í•  ì˜ì—­ (ì˜ˆ: ì „ë°˜ì ì¸ ê·¼ìœ¡ëŸ‰ ì¦ê°€, ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ í–¥ìƒ)
        ê° í•­ëª©ì— ëŒ€í•´ 'ì™œ' ê·¸ê²ƒì´ ì¤‘ìš”í•œì§€ ì´ìœ ë¥¼ ëª…í™•íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: "ë¬´ìŠ¨ ìš´ë™ì„ í•˜ì„¸ìš”", "ë¬´ì—‡ì„ ë“œì„¸ìš”"ì™€ ê°™ì€ êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ë²•ì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ê³¼ ì´ìœ ë§Œ ì„¤ëª…í•˜ì„¸ìš”."""
        return _generate_qa_response(state, "ê°œì„  ìš°ì„ ìˆœìœ„", system_prompt)

    def qa_general(state: AnalysisState) -> dict:
        """Node 2-5: ì¼ë°˜ Q&A"""
        system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ì²´ì„±ë¶„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: êµ¬ì²´ì ì¸ ìš´ë™ ê³„íšì´ë‚˜ ì‹ë‹¨ ê³„íšì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”. í˜„ì¬ ìƒíƒœ ë¶„ì„ê³¼ ê°œì„  í•„ìš” ì˜ì—­ë§Œ ì„¤ëª…í•˜ì„¸ìš”."""
        return _generate_qa_response(state, "ì¼ë°˜", system_prompt)

    def finalize_analysis(state: AnalysisState) -> dict:
        """Node 3: ë¶„ì„ í™•ì • ë° ì €ì¥"""
        print(f"\n[DEBUG][finalize] ===== NODE ENTER =====")
        print(f"[DEBUG][finalize] messages count: {len(state.get('messages', []))}")
        print(f"[DEBUG][finalize] â†’ ENDë¡œ ì¢…ë£Œ\n")
        return {"messages": [("ai", "ë„¤, ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì •í•˜ê³  ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”.")]}

    def route_qa(state: AnalysisState) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ Q&A ë…¸ë“œë¡œ ë¼ìš°íŒ…"""
        print(f"\n[DEBUG][route_qa] ===== ROUTING =====")
        print(f"[DEBUG][route_qa] total messages: {len(state.get('messages', []))}")

        # ğŸ”§ ìˆ˜ì •: ê°€ì¥ ìµœê·¼ HUMAN ë©”ì‹œì§€ë§Œ ì„ íƒ (ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì œì™¸)
        last_human_msg = None
        for msg in reversed(state["messages"]):
            if msg.type == "human":
                # ì´ˆê¸° ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì œì™¸ (ë§¤ìš° ê¸¸ê±°ë‚˜ "# InBody"ë¡œ ì‹œì‘)
                content = msg.content.strip()
                if not (content.startswith("# InBody") or content.startswith("##") or len(content) > 1000):
                    # ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ (ì§§ê³  ê°„ê²°)
                    last_human_msg = msg
                    break

        # human ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ initial_analysis ì§í›„ ì²« ì‹¤í–‰ â†’ ê¸°ë³¸ ë…¸ë“œë¡œ
        if not last_human_msg:
            print(f"[DEBUG][route_qa] !! ì‚¬ìš©ì ì§ˆë¬¸ ì—†ìŒ (initial_analysis ì§í›„) â†’ qa_generalë¡œ ê¸°ë³¸ ë¼ìš°íŒ…")
            return "qa_general"

        user_question = last_human_msg.content.strip()
        print(f"[DEBUG][route_qa] last human message found: '{user_question[:120]}'")

        # í”„ë¡ íŠ¸ì—”ë“œ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í‚¤ì›Œë“œ ë§¤ì¹­
        if "ê·¼ìœ¡" in user_question:
            destination = "qa_strength_weakness"
        elif "ì²´ì§€ë°©" in user_question:
            destination = "qa_health_status"
        elif "ê· í˜•" in user_question or "ë¶ˆê· í˜•" in user_question:
            destination = "qa_impact"
        elif "ìš°ì„ ìˆœìœ„" in user_question or "ê°œì„ " in user_question:
            destination = "qa_priority"
        elif "í™•ì •" in user_question:
            destination = "finalize_analysis"
        # ìˆ«ì ê¸°ë°˜ ë¼ìš°íŒ… (í•˜ìœ„ í˜¸í™˜)
        elif user_question.startswith("1"):
            destination = "qa_strength_weakness"
        elif user_question.startswith("2"):
            destination = "qa_health_status"
        elif user_question.startswith("3"):
            destination = "qa_impact"
        elif user_question.startswith("4"):
            destination = "qa_priority"
        elif user_question.startswith("6"):
            destination = "finalize_analysis"
        else:
            destination = "qa_general"

        print(f"[DEBUG][route_qa] â†’ routing to: {destination}\n")
        return destination

    # ë¼ìš°íŒ… ë§µ ì •ì˜
    routing_map = {
        "qa_strength_weakness": "qa_strength_weakness",
        "qa_health_status": "qa_health_status",
        "qa_impact": "qa_impact",
        "qa_priority": "qa_priority",
        "qa_general": "qa_general",
        "finalize_analysis": "finalize_analysis",
    }

    workflow = StateGraph(AnalysisState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("initial_analysis", generate_initial_analysis)
    workflow.add_node("qa_strength_weakness", qa_strength_weakness)
    workflow.add_node("qa_health_status", qa_health_status)
    workflow.add_node("qa_impact", qa_impact)
    workflow.add_node("qa_priority", qa_priority)
    workflow.add_node("qa_general", qa_general)
    workflow.add_node("finalize_analysis", finalize_analysis)
    
    # ì§„ì…ì  ì„¤ì •
    workflow.set_entry_point("initial_analysis")
    
    # ì—£ì§€ ì—°ê²°
    # ìµœì´ˆ ë¶„ì„ í›„ì—ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    workflow.add_conditional_edges(
        "initial_analysis",
        route_qa,
        routing_map
    )
    
    # ê° Q&A ë…¸ë“œëŠ” ë‹¤ì‹œ ë¼ìš°í„°ë¥¼ ê±°ì³ ë‹¤ìŒ ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë£¨í”„)
    qa_nodes = ["qa_strength_weakness", "qa_health_status", "qa_impact", "qa_priority", "qa_general"]
    for node in qa_nodes:
        workflow.add_conditional_edges(node, route_qa, routing_map)

    # í™•ì • í›„ ì¢…ë£Œ
    workflow.add_edge("finalize_analysis", END)

    # ì²´í¬í¬ì¸í„° ì„¤ì • (ì¸ë©”ëª¨ë¦¬ ì €ì¥ì†Œ)
    # ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” PostgresSaver ë“±ì„ ì‚¬ìš©í•˜ì—¬ DBì— ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    memory = MemorySaver()

    # íœ´ë¨¼ í”¼ë“œë°±ì„ ìœ„í•´, LLMì´ ë‹µë³€ì„ ìƒì„±í•œ í›„ì—ëŠ” í•­ìƒ ë©ˆì¶¥ë‹ˆë‹¤.
    # ì„œë¹„ìŠ¤(API)ëŠ” ì´ ë©ˆì¶˜ ì§€ì ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    # initial_analysisëŠ” ì œì™¸ (PASSTHROUGH ì‹œ ë‹¤ìŒ ë…¸ë“œë¡œ ê³„ì† ì§„í–‰ë˜ì–´ì•¼ í•¨)
    agent = workflow.compile(checkpointer=memory, interrupt_after=qa_nodes)
    
    return agent
    
