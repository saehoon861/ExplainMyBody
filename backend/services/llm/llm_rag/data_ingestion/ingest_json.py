"""
JSON í˜•ì‹ RAG ë°ì´í„°ë¥¼ PostgreSQL paper_nodes í…Œì´ë¸”ì— ì…ë ¥
"""

import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

load_dotenv()


class RAGDataIngestion:
    """RAG ë°ì´í„° ì…ë ¥ í´ë˜ìŠ¤"""

    def __init__(self, db_url: Optional[str] = None):
        """
        Args:
            db_url: PostgreSQL ì—°ê²° URL (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
        """
        if not db_url:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.engine = create_engine(db_url)
        self.SessionLocal = sessionmaker(bind=self.engine)

    def load_json_file(self, file_path: str) -> Dict[str, Any]:
        """JSON íŒŒì¼ ë¡œë“œ"""
        print(f"\nğŸ“‚ JSON íŒŒì¼ ë¡œë”©: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"  âœ“ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
        print(f"  ë…¸ë“œ ìˆ˜: {len(data.get('nodes', []))}")
        print(f"  ì—£ì§€ ìˆ˜: {len(data.get('links', []))}")

        return data

    def parse_paper_nodes(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """JSON ë°ì´í„°ì—ì„œ Paper ë…¸ë“œë§Œ ì¶”ì¶œ ë° íŒŒì‹±"""
        paper_nodes = []

        for node in data.get('nodes', []):
            if node.get('node_type') == 'paper':
                paper = {
                    'paper_id': node.get('id', '').replace('paper_', ''),
                    'title': node.get('title', ''),
                    'chunk_text': node.get('chunk_text', ''),
                    'chunk_ko_summary': node.get('chunk_ko_summary', ''),
                    'lang': node.get('lang', 'en'),
                    'source': node.get('source', 'unknown'),
                    'year': node.get('year'),
                    'pmid': node.get('pmid'),
                    'doi': node.get('doi'),
                    'embedding_ko': node.get('embedding_ko'),
                    'embedding_en': node.get('embedding_en')
                }
                paper_nodes.append(paper)

        print(f"\n  ğŸ“„ Paper ë…¸ë“œ íŒŒì‹± ì™„ë£Œ: {len(paper_nodes)}ê°œ")
        return paper_nodes

    def insert_papers_to_db(
        self,
        papers: List[Dict[str, Any]],
        batch_size: int = 100,
        skip_existing: bool = True
    ):
        """
        Paper ë°ì´í„°ë¥¼ PostgreSQLì— ì‚½ì…

        Args:
            papers: Paper ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ ì‚½ì… í¬ê¸°
            skip_existing: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” paper_idëŠ” ìŠ¤í‚µ
        """
        session = self.SessionLocal()

        try:
            print(f"\nğŸ’¾ PostgreSQLì— ë°ì´í„° ì‚½ì… ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")

            inserted_count = 0
            skipped_count = 0
            error_count = 0

            for i in range(0, len(papers), batch_size):
                batch = papers[i:i+batch_size]

                for paper in batch:
                    try:
                        # ì¤‘ë³µ í™•ì¸
                        if skip_existing:
                            existing = session.execute(
                                text("SELECT id FROM paper_nodes WHERE paper_id = :paper_id"),
                                {"paper_id": paper['paper_id']}
                            ).fetchone()

                            if existing:
                                skipped_count += 1
                                continue

                        # Embeddingì„ PostgreSQL array í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        embedding_ko_str = None
                        if paper.get('embedding_ko'):
                            embedding_ko_str = "[" + ",".join(str(x) for x in paper['embedding_ko']) + "]"

                        embedding_en_str = None
                        if paper.get('embedding_en'):
                            embedding_en_str = "[" + ",".join(str(x) for x in paper['embedding_en']) + "]"

                        # INSERT ì¿¼ë¦¬
                        insert_query = text("""
                            INSERT INTO paper_nodes (
                                paper_id, title, chunk_text, chunk_ko_summary,
                                lang, source, year, pmid, doi,
                                embedding_ko_openai, embedding_en_openai,
                                created_at, updated_at
                            )
                            VALUES (
                                :paper_id, :title, :chunk_text, :chunk_ko_summary,
                                :lang, :source, :year, :pmid, :doi,
                                :embedding_ko, :embedding_en,
                                NOW(), NOW()
                            )
                        """)

                        session.execute(insert_query, {
                            'paper_id': paper['paper_id'],
                            'title': paper['title'],
                            'chunk_text': paper['chunk_text'],
                            'chunk_ko_summary': paper['chunk_ko_summary'],
                            'lang': paper['lang'],
                            'source': paper['source'],
                            'year': paper['year'],
                            'pmid': paper['pmid'],
                            'doi': paper['doi'],
                            'embedding_ko': embedding_ko_str,
                            'embedding_en': embedding_en_str
                        })

                        inserted_count += 1

                    except Exception as e:
                        error_count += 1
                        print(f"  âš ï¸  ì—ëŸ¬ (paper_id: {paper.get('paper_id')}): {e}")
                        continue

                # ë°°ì¹˜ ì»¤ë°‹
                session.commit()
                print(f"  ì§„í–‰: {min(i+batch_size, len(papers))}/{len(papers)} papers...")

            print(f"\nâœ… ì‚½ì… ì™„ë£Œ!")
            print(f"  - ì‚½ì…: {inserted_count}ê°œ")
            print(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ")
            print(f"  - ì—ëŸ¬: {error_count}ê°œ")

        except Exception as e:
            session.rollback()
            print(f"\nâŒ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {e}")
            raise
        finally:
            session.close()

    def run(self, json_file_path: str, batch_size: int = 100, skip_existing: bool = True):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 60)
        print("RAG ë°ì´í„° ì…ë ¥ (JSON â†’ PostgreSQL)")
        print("=" * 60)

        # 1. JSON íŒŒì¼ ë¡œë“œ
        data = self.load_json_file(json_file_path)

        # 2. Paper ë…¸ë“œ íŒŒì‹±
        papers = self.parse_paper_nodes(data)

        if not papers:
            print("\nâš ï¸  Paper ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 3. DB ì‚½ì…
        self.insert_papers_to_db(papers, batch_size=batch_size, skip_existing=skip_existing)

        print("\n" + "=" * 60)
        print("âœ… ì™„ë£Œ!")
        print("=" * 60)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="RAG ë°ì´í„° ì…ë ¥ (JSON â†’ PostgreSQL)")
    parser.add_argument(
        "json_file",
        type=str,
        help="ì…ë ¥í•  JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="ë°°ì¹˜ ì‚½ì… í¬ê¸° (ê¸°ë³¸: 100)"
    )
    parser.add_argument(
        "--no-skip-existing",
        action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë„ ë‹¤ì‹œ ì‚½ì… (ê¸°ë³¸: ìŠ¤í‚µ)"
    )
    parser.add_argument(
        "--db-url",
        type=str,
        default=None,
        help="PostgreSQL ì—°ê²° URL (ê¸°ë³¸: í™˜ê²½ë³€ìˆ˜ DATABASE_URL)"
    )

    args = parser.parse_args()

    # ì‹¤í–‰
    ingestion = RAGDataIngestion(db_url=args.db_url)
    ingestion.run(
        json_file_path=args.json_file,
        batch_size=args.batch_size,
        skip_existing=not args.no_skip_existing
    )


if __name__ == "__main__":
    main()
