"""
LLM ì„œë¹„ìŠ¤
AI ë¶„ì„ ë° ê³„íš ìƒì„± (LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš©)
"""

from sqlalchemy.orm import Session
from typing import Dict, Any, Optional
from datetime import datetime
import os
from dotenv import load_dotenv

from schemas.llm import StatusAnalysisInput, GoalPlanInput, LLMInteractionCreate
from repositories.llm.llm_interaction_repository import LLMInteractionRepository
from services.llm.llm_clients import create_llm_client
from .agent_graph import create_analysis_agent
from .weekly_plan_graph import create_weekly_plan_agent
from typing import AsyncGenerator
from typing import AsyncGenerator, Optional
import re

load_dotenv()


class LLMService:
    """LLM API í˜¸ì¶œ ì„œë¹„ìŠ¤"""
    async def stream_chat_with_plan(
        self,
        thread_id: str,
        user_message: str,
        existing_plan: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        LLM2 íœ´ë¨¼ í”¼ë“œë°± (Q&A) - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „
        """
        print(f"--- [DEBUG] stream_chat_with_plan ì§„ì… ---")
        print(f"--- [DEBUG] thread_id: {thread_id}")
        print(f"--- [DEBUG] raw user_message: {user_message}")

        config = {"configurable": {"thread_id": thread_id}}

        # =========================
        # 1. Category íŒŒì‹± (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
        # =========================
        category = "qa_general"
        clean_message = user_message

        match = re.match(r"^\[Category:\s*(.*?)\]\s*(.*)$", user_message, re.DOTALL)
        if match:
            category_label = match.group(1).strip()
            clean_message = match.group(2).strip()

            if "ìš´ë™" in category_label or "í”Œëœ" in category_label or "ê³„íš" in category_label:
                category = "adjust_exercise_plan"
            elif "ì‹ë‹¨" in category_label:
                category = "adjust_diet_plan"
            elif "ê°•ë„" in category_label:
                category = "adjust_intensity"

        print(f"--- [DEBUG] category: {category}")
        print(f"--- [DEBUG] clean_message: {clean_message}")

        # =================================================
        # 2. LangGraph ìƒíƒœì—ì„œ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ë©”ëª¨ë¦¬ ì—°ë™)
        # =================================================
        state_snapshot = self.weekly_plan_agent.get_state(config)
        history_messages = []
        
        if state_snapshot and state_snapshot.values:
            for msg in state_snapshot.values.get("messages", []):
                role = "user" if msg.type == "human" else "assistant"
                history_messages.append((role, msg.content))

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì£¼ê°„ ìš´ë™ ë° ì‹ë‹¨ ê³„íšì„ ë‹´ë‹¹í•˜ëŠ” í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìƒì„±ëœ ê³„íšì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´, ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ì´ì „ ëŒ€í™” ë§¥ë½(ì‚¬ìš©ìì˜ ì‹ ì²´ ì •ë³´, ëª©í‘œ, ìƒì„±ëœ ê³„íš)ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."""
        
        if existing_plan:
            system_prompt += f"\n\n[ì°¸ê³ : í˜„ì¬ ì‚¬ìš©ìì˜ ì£¼ê°„ ê³„íš ì •ë³´]\n{existing_plan}\n\nì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì´ ìœ„ ê³„íšê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´, ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê±°ë‚˜ ìˆ˜ì •í•´ì£¼ì„¸ìš”."

        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (LLM í˜¸ì¶œìš©)
        # history_messagesì—ëŠ” ì´ë¯¸ ì´ì „ ëŒ€í™”ê°€ í¬í•¨ë˜ì–´ ìˆìŒ
        messages_to_send = history_messages + [("user", clean_message)]

        full_text = ""

        # =================================================
        # 3. ì§ì ‘ LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (LangGraph ìš°íšŒ)
        # =================================================
        try:
            async for chunk in self.llm_client.generate_chat_with_history_stream(system_prompt, messages_to_send):
                full_text += chunk
                yield chunk
            
            # =================================================
            # 4. ëŒ€í™” ì™„ë£Œ í›„ LangGraph ìƒíƒœ ìˆ˜ë™ ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬ ì €ì¥)
            # =================================================
            print(f"--- [DEBUG] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ. ìƒíƒœ ì—…ë°ì´íŠ¸ ì§„í–‰ ---")
            self.weekly_plan_agent.update_state(
                config, 
                {"messages": [("human", clean_message), ("ai", full_text)]}
            )
            
        except Exception as e:
            print(f"--- [ERROR] ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬ ê°€ëŠ¥
            yield f"\n[ì˜¤ë¥˜ ë°œìƒ] ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

        # =========================
        # 4. (ì„ íƒ) ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ DB ì €ì¥
        # =========================
        # ì§€ê¸ˆì€ MVPë¼ ìƒëµ
        # ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ LLMInteractionRepository.create(...)

    def __init__(self):
        """LLM ì—ì´ì „íŠ¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.model_version = "gpt-4o-mini"  # ë˜ëŠ” ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´
        self.llm_client = create_llm_client(self.model_version)
        self.analysis_agent = create_analysis_agent(self.llm_client)
        self.weekly_plan_agent = create_weekly_plan_agent(self.llm_client)

    # LLM1: ê±´ê°• ê¸°ë¡ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„± - ë°ì´í„° ì¤€ë¹„ 
    # fixme : ë‹¨ìˆœíˆ inputì— ëŒ€í•´ì„œ ê·¸ëŒ€ë¡œ ë˜ëŒë ¤ì£¼ê³  ìˆìŒ. 
    # í•„ìš”ê°€ ì—†ìŒ. health_serviceì—ì„œ ì²˜ë¦¬í•´ì•¼í•¨.
    def prepare_status_analysis_input(
        self,
        record_id: int,
        user_id: int,
        measured_at: datetime,
        measurements: Dict[str, Any],
        body_type1: Optional[str],
        body_type2: Optional[str],
        prev_inbody_data: Optional[Dict[str, Any]] = None,
        prev_inbody_date: Optional[datetime] = None,
        interval_days: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        LLM1: ê±´ê°• ìƒíƒœ ë¶„ì„ìš© input ë°ì´í„° ì¤€ë¹„

        Args:
            record_id: ê±´ê°• ê¸°ë¡ ID
            user_id: ì‚¬ìš©ì ID
            measured_at: ì¸¡ì • ì¼ì‹œ
            measurements: ì¸ë°”ë”” ì¸¡ì • ë°ì´í„°(ì²´í˜• ë¶„ë¥˜ í¬í•¨)
            body_type1: 1ì°¨ ì²´í˜• ë¶„ë¥˜
            body_type2: 2ì°¨ ì²´í˜• ë¶„ë¥˜
            prev_inbody_data: ì´ì „ ì¸ë°”ë”” ì¸¡ì • ë°ì´í„° (ì„ íƒ)
            prev_inbody_date: ì´ì „ ì¸ë°”ë”” ì¸¡ì • ì¼ì‹œ (ì„ íƒ)
            interval_days: ì´ì „ InBody ì¸¡ì • ì¼ì‹œ (ì„ íƒ)

        Returns:
            LLMì— ì „ë‹¬í•  input ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ LLM API í˜¸ì¶œ ì‹œ ì‚¬ìš©)
        """
        print(f"\n[DEBUG][LLMService] prepare_status_analysis_input í˜¸ì¶œ")
        print(f"[DEBUG][LLMService] prev_inbody_data is None: {prev_inbody_data is None}")
        print(f"[DEBUG][LLMService] prev_inbody_date is None: {prev_inbody_date is None}")
        print(f"[DEBUG][LLMService] interval_days is None: {interval_days is None}")
        if prev_inbody_data:
            print(f"[DEBUG][LLMService] prev_inbody_data í‚¤: {list(prev_inbody_data.keys())[:5]}...")
        
        return {
            "record_id": record_id,
            "user_id": user_id,
            "measured_at": measured_at,
            "measurements": measurements,
            "body_type1": body_type1,
            "body_type2": body_type2,
            "prev_inbody_data": prev_inbody_data,
            "prev_inbody_date": prev_inbody_date,
            "interval_days": interval_days
        }

    def prepare_goal_plan_input(
        self,
        # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
        user_goal_type: Optional[str],
        user_goal_description: Optional[str],
        # ì„ íƒëœ ê±´ê°• ê¸°ë¡
        record_id: int,
        user_id: int,
        measured_at: datetime,
        measurements: Dict[str, Any],
        # LLM1 ë¶„ì„ ê²°ê³¼
        status_analysis_result: Optional[str] = None,
        status_analysis_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        LLM2: ì£¼ê°„ ê³„íšì„œ ìƒì„±ìš© input ë°ì´í„° ì¤€ë¹„

        Args:
            user_goal_type: ì‚¬ìš©ì ëª©í‘œ íƒ€ì…
            user_goal_description: ì‚¬ìš©ì ëª©í‘œ ìƒì„¸
            record_id: ì„ íƒëœ ê±´ê°• ê¸°ë¡ ID
            user_id: ì‚¬ìš©ì ID
            measured_at: ì¸¡ì • ì¼ì‹œ
            measurements: ì¸ë°”ë”” ì¸¡ì • ë°ì´í„°(ì²´í˜• ë¶„ë¥˜ í¬í•¨)
            status_analysis_result: LLM1ì˜ ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
            status_analysis_id: LLM1 ë¶„ì„ ê²°ê³¼ ID

        Returns:
            LLMì— ì „ë‹¬í•  input ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ LLM API í˜¸ì¶œ ì‹œ ì‚¬ìš©)
        """
        return {
            "user_goal_type": user_goal_type,
            "user_goal_description": user_goal_description,
            "record_id": record_id,
            "user_id": user_id,
            "measured_at": measured_at,
            "measurements": measurements,
            "status_analysis_result": status_analysis_result,
            "status_analysis_id": status_analysis_id
        }

    # =====================================================
    # ì•„ë˜ëŠ” íŒ€ì›ì´ LLM API ì—°ë™ ì‹œ êµ¬í˜„í•  ë©”ì„œë“œë“¤
    # =====================================================

    # LLM1: ê±´ê°• ê¸°ë¡ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„± - LLM í˜¸ì¶œ
    async def call_status_analysis_llm(
        self,
        input_data: StatusAnalysisInput
    ) -> Dict[str, Any]:
        """
        LangGraph ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ê±´ê°• ìƒíƒœ ë¶„ì„ ìˆ˜í–‰

        Args:
            input_data: StatusAnalysisInput ìŠ¤í‚¤ë§ˆ ê°ì²´

        Returns:
            {
                "analysis_text": str,
                "embedding": {"embedding_1536": [...], "embedding_1024": [...]},
                "thread_id": str
            }
        """
        # 1. ê° ë¶„ì„ ì„¸ì…˜ì„ ìœ„í•œ ê³ ìœ  ìŠ¤ë ˆë“œ ID ìƒì„±
        thread_id = f"analysis_{input_data.user_id}_{input_data.record_id}_{datetime.now().timestamp()}"
        config = {"configurable": {"thread_id": thread_id}}
        print(f"\n[DEBUG][call_status_analysis_llm] ìƒì„±ëœ thread_id: {thread_id}")
        print(f"[DEBUG][call_status_analysis_llm] user_id: {input_data.user_id}, record_id: {input_data.record_id}")
        
        # ğŸ” ì¤‘ìš”: agentì— ì „ë‹¬í•˜ê¸° ì „ input_data ê²€ì¦
        print(f"\n[DEBUG][call_status_analysis_llm] === Agent í˜¸ì¶œ ì „ input_data ê²€ì¦ ===")
        print(f"[DEBUG][call_status_analysis_llm] input_data íƒ€ì…: {type(input_data)}")
        print(f"[DEBUG][call_status_analysis_llm] input_data.prev_inbody_data is None: {input_data.prev_inbody_data is None}")
        print(f"[DEBUG][call_status_analysis_llm] input_data.prev_inbody_date is None: {input_data.prev_inbody_date is None}")
        if input_data.prev_inbody_data:
            print(f"[DEBUG][call_status_analysis_llm] âœ… prev_inbody_data ì¡´ì¬!")
            print(f"[DEBUG][call_status_analysis_llm] prev_inbody_data íƒ€ì…: {type(input_data.prev_inbody_data)}")
            if isinstance(input_data.prev_inbody_data, dict):
                print(f"[DEBUG][call_status_analysis_llm] prev_inbody_data í‚¤ ìƒ˜í”Œ: {list(input_data.prev_inbody_data.keys())[:3]}")
        else:
            print(f"[DEBUG][call_status_analysis_llm] âš ï¸ prev_inbody_data ì—†ìŒ (ì²« ì¸ë°”ë”” ë˜ëŠ” ìœ ì‹¤)")

        # 2. LangGraph ì—ì´ì „íŠ¸ í˜¸ì¶œ (ìµœì´ˆ ë¶„ì„)
        print(f"[DEBUG][call_status_analysis_llm] === Agent í˜¸ì¶œ ì‹œì‘ ===")
        initial_state = self.analysis_agent.invoke(
            {"analysis_input": input_data},
            config=config
        )

        print(f"[DEBUG][call_status_analysis_llm] initial_state keys: {list(initial_state.keys())}")
        print(f"[DEBUG][call_status_analysis_llm] has analysis_input: {'analysis_input' in initial_state}")
        if 'analysis_input' in initial_state:
            print(f"[DEBUG][call_status_analysis_llm] analysis_input is None: {initial_state['analysis_input'] is None}")

        # 3. ê²°ê³¼ ì¶”ì¶œ
        # ğŸ”§ ìˆ˜ì •: initial_analysis ê²°ê³¼ë§Œ ì¶”ì¶œ (qa_generalë¡œ ë„˜ì–´ê°„ ê²½ìš° ë°©ì§€)
        # - messages[0]: human (InBody ë°ì´í„°)
        # - messages[1]: ai (initial_analysis ê²°ê³¼) â† ì´ê²ƒë§Œ í•„ìš”
        # - messages[2]: ai (qa_general ì‘ë‹µ) â† ìˆìœ¼ë©´ ì•ˆ ë¨
        messages = initial_state['messages']
        if len(messages) >= 2:
            # í•­ìƒ ë‘ ë²ˆì§¸ ë©”ì‹œì§€(initial_analysis ê²°ê³¼)ë¥¼ ì‚¬ìš©
            analysis_text = messages[1].content
        else:
            # ì˜ˆì™¸ ìƒí™©: ë©”ì‹œì§€ê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì‚¬ìš©
            analysis_text = messages[-1].content

        embedding = initial_state.get("embedding")

        return {"analysis_text": analysis_text, "embedding": embedding, "thread_id": thread_id}

    async def chat_with_analysis(
        self,
        thread_id: str,
        user_message: str,
        report_id: int = None,
        db: Session = None
    ) -> str:
        """
        LLM1 ì— ëŒ€í•œ
        íœ´ë¨¼ í”¼ë“œë°± (Q&A) ì²˜ë¦¬: ê¸°ì¡´ ìŠ¤ë ˆë“œì— ì´ì–´ì„œ ëŒ€í™” ìˆ˜í–‰
        checkpoint ì—†ìœ¼ë©´ DBì—ì„œ InBody ë°ì´í„° ë³µì›
        """
        config = {"configurable": {"thread_id": thread_id}}

        # ğŸ” checkpoint ìƒíƒœ í™•ì¸
        print(f"\n[DEBUG][chat_with_analysis] thread_id: {thread_id}")
        checkpoint_exists = False
        try:
            checkpointer = self.analysis_agent.checkpointer
            if checkpointer:
                checkpoint = checkpointer.get(config)
                if checkpoint:
                    checkpoint_exists = True
                    print(f"[DEBUG][chat_with_analysis] âœ… checkpoint found")
                else:
                    print(f"[DEBUG][chat_with_analysis] âš ï¸ checkpoint NOT FOUND")
        except Exception as e:
            print(f"[DEBUG][chat_with_analysis] checkpoint ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ğŸ“¦ DB Fallback: checkpoint ì—†ìœ¼ë©´ InBody ë°ì´í„° ë³µì›
        initial_messages = []
        if not checkpoint_exists and report_id and db:
            print(f"[DEBUG][chat_with_analysis] ğŸ”„ DB Fallback ì‹œì‘ (report_id={report_id})")
            try:
                from repositories.llm.analysis_report_repository import AnalysisReportRepository
                from repositories.common.health_record_repository import HealthRecordRepository
                from services.llm.prompt_generator import create_inbody_analysis_prompt
                from schemas.inbody import InBodyData as InBodyMeasurements

                # 1. analysis_reportì—ì„œ record_id ì¡°íšŒ
                analysis_report = AnalysisReportRepository.get_by_id(db, report_id)
                if analysis_report and analysis_report.record_id:
                    # 2. health_recordì—ì„œ measurements ì¡°íšŒ 
                    # ì´ë•Œ, ê°€ì¥ ìµœì‹ ì˜ ë‘ê°œì˜ ì¸ë°”ë”” ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼í•¨
                    # ë‹¨, ì´ì „ ì¸ë°”ë”” ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ í•´ì•¼í•¨
                    health_record = HealthRecordRepository.get_by_id(db, analysis_report.record_id)
                    
                    # ì´ì „ ì¸ë°”ë”” ë°ì´í„° ì¡°íšŒ: ê°™ì€ ì‚¬ìš©ìì˜ í˜„ì¬ ê¸°ë¡ë³´ë‹¤ ì´ì „ ì¸¡ì • ê¸°ë¡
                    prev_health_record = None
                    if health_record:
                        prev_health_record = HealthRecordRepository.get_previous_record(
                            db, health_record.user_id, health_record
                        )
                    if health_record and health_record.measurements:
                        # 3. InBody í”„ë¡¬í”„íŠ¸ ì¬ìƒì„±
                        measurements = InBodyMeasurements(**health_record.measurements)
                        prev_measurements = InBodyMeasurements(**prev_health_record.measurements) if prev_health_record else None
                        system_prompt, user_prompt = create_inbody_analysis_prompt(
                            measurements,
                            body_type1=getattr(health_record, 'body_type1', None),
                            body_type2=getattr(health_record, 'body_type2', None),
                            # ì´ì „ ì¸ë°”ë”” ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ í•´ì•¼í•¨
                            prev_inbody_data=prev_measurements if prev_measurements else None,
                            # ì´ì „ ì¸ë°”ë”” ë°ì´í„°ê°€ ê°€ì¥ ìµœì‹ ì˜ ì¸ë°”ë”” ë°ì´í„°ì™€ ê°„ê²© ê³„ì‚°
                            interval_days=(health_record.created_at - prev_health_record.created_at).days if prev_health_record else None
                        )
                        initial_messages.append(("human", user_prompt))
                        print(f"[DEBUG][chat_with_analysis] âœ… InBody ë°ì´í„° ë³µì› ì™„ë£Œ (record_id={analysis_report.record_id})")
            except Exception as e:
                print(f"[DEBUG][chat_with_analysis] âš ï¸ DB Fallback ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        # LangGraph ì‹¤í–‰
        messages_to_send = initial_messages + [("human", user_message)]
        result = self.analysis_agent.invoke(
            {"messages": messages_to_send},
            config=config
        )

        print(f"[DEBUG][chat_with_analysis] result has analysis_input: {'analysis_input' in result}")

        # ë§ˆì§€ë§‰ AI ì‘ë‹µ ë°˜í™˜
        return result["messages"][-1].content

    async def call_goal_plan_llm(
        self,
        db: Session,
        input_data: GoalPlanInput
    ) -> dict:
        """
        LLM2: ì£¼ê°„ ê³„íšì„œ ìƒì„± API í˜¸ì¶œ ë° ì´ˆê¸° ìƒí˜¸ì‘ìš© ì €ì¥
        """
        # 1. ìŠ¤ë ˆë“œ ID ìƒì„±
        thread_id = f"plan_{input_data.user_id}_{input_data.record_id}_{datetime.now().timestamp()}"
        config = {"configurable": {"thread_id": thread_id}}



        # 2. LangGraph ì—ì´ì „íŠ¸ í˜¸ì¶œ
        initial_state = self.weekly_plan_agent.invoke(
            {"plan_input": input_data},
            config=config
        )
        
        # (
        #     {"plan_input": input_data},
        #     config=config
        # )
        
        plan_text = initial_state['messages'][-1].content

        # 3. ì´ˆê¸° LLM ìƒí˜¸ì‘ìš© DBì— ì €ì¥
        interaction_schema = LLMInteractionCreate(
            llm_stage="llm2",
            source_type="weekly_plan_initial",
            source_id=input_data.record_id,
            output_text=plan_text,
            model_version=self.model_version
        )
        new_interaction = LLMInteractionRepository.create(db, input_data.user_id, interaction_schema)

        # 4. ê²°ê³¼ ë°˜í™˜
        return {
            "plan_text": plan_text,
            "thread_id": thread_id,
            "llm_interaction_id": new_interaction.id
        }

    async def chat_with_plan(
        self,
        thread_id: str,
        user_message: str,
        existing_plan: Optional[str] = None
    ) -> str:
        """
        LLM2 íœ´ë¨¼ í”¼ë“œë°± (Q&A) ì²˜ë¦¬: ì£¼ê°„ ê³„íš ìˆ˜ì • ë° ì§ˆì˜ì‘ë‹µ
        """
        import re
        print(f"--- [DEBUG] chat_with_plan ì§„ì… ---")
        print(f"--- [DEBUG] thread_id: {thread_id}")
        print(f"--- [DEBUG] raw user_message: {user_message}")

        config = {"configurable": {"thread_id": thread_id}}
        
        # ë©”ì‹œì§€ íŒŒì‹± ([Category: ...] ë¶„ë¦¬)
        category = None
        clean_message = user_message
        
        # Regexë¡œ [Category: ...] íŒ¨í„´ ì¶”ì¶œ
        match = re.match(r"^\[Category:\s*(.*?)\]\s*(.*)$", user_message, re.DOTALL)
        if match:
            category_label = match.group(1).strip()
            clean_message = match.group(2).strip()
            print(f"--- [DEBUG] Parsed Category Label: {category_label}")
            print(f"--- [DEBUG] Clean Message: {clean_message}")
            
            # ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì„ ë‚´ë¶€ í‚¤ë¡œ ë§¤í•‘ (í•„ìš”ì‹œ)
            # í˜„ì¬ëŠ” ë‹¨ìˆœ ë§¤í•‘ ë˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬. ê·¸ë˜í”„ì˜ router ë¡œì§ì— ì˜ì¡´.
            # ê·¸ë˜í”„ì—ì„œëŠ” "ìš´ë™ í”Œëœ ì¡°ì •", "ì‹ë‹¨ ì¡°ì •" ë“±ì„ ê¸°ëŒ€í•¨.
            # í”„ë¡ íŠ¸ì—”ë“œ ë¼ë²¨: "ğŸ“… ì£¼ê°„ ê³„íš", "ğŸ‹ï¸ ë¶€ìœ„ë³„ ìš´ë™" ë“±.
            # ì—¬ê¸°ì„œëŠ” ìš°ì„  ë§¤í•‘ ì—†ì´ ì „ë‹¬í•˜ê±°ë‚˜, ê°„ë‹¨í•œ ë³€í™˜ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.
            
            # ì„ì‹œ ë§¤í•‘ ë¡œì§ (í”„ë¡ íŠ¸ ë¼ë²¨ -> ê·¸ë˜í”„ ë‚´ë¶€ ì¹´í…Œê³ ë¦¬)
            if "ìš´ë™" in category_label or "í”Œëœ" in category_label or "ê³„íš" in category_label: # ì˜ˆ: ìš´ë™ í”Œëœ ì¡°ì •
                category = "adjust_exercise_plan"
            elif "ì‹ë‹¨" in category_label:
                category = "adjust_diet_plan"
            elif "ê°•ë„" in category_label:
                category = "adjust_intensity"
            else:
                # ê·¸ ì™¸ (ì£¼ê°„ ê³„íš, ë¶€ìœ„ë³„ ìš´ë™ ë“±)ì€ ì¼ë°˜ Q&Aë¡œ ì²˜ë¦¬í•˜ë˜,
                # ê·¸ë˜í”„ ë¼ìš°í„°ê°€ "qa_general"ë¡œ í´ë°±í•˜ë„ë¡ None ë˜ëŠ” "qa_general" ì „ë‹¬
                category = "qa_general"
            
            print(f"--- [DEBUG] Mapped Category Key: {category}")
            
        else:
            print("--- [DEBUG] No Category prefix found. Treating as general message.")
            category = "qa_general"

        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤€ë¹„
        input_payload = {
            "messages": [("human", clean_message)],
            # feedback_categoryë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë¼ìš°í„°ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ íƒ€ë„ë¡ í•¨
            "feedback_category": category,
            "existing_plan": existing_plan
        }
        
        print(f"--- [DEBUG] llm_service: existing_plan ì „ë‹¬ ì—¬ë¶€: {bool(existing_plan)} ---")
        if existing_plan:
            print(f"--- [DEBUG] llm_service: existing_plan preview: {existing_plan[:100]}... ---")
        print(f"--- [DEBUG] Invoking weekly_plan_agent with payload: {input_payload}")

        # LangGraph ì‹¤í–‰ (ì´ì „ ìƒíƒœì—ì„œ ì´ì–´ì„œ ì‹¤í–‰)
        try:
            result = self.weekly_plan_agent.invoke(
                input_payload,
                config=config
            )
            print("--- [DEBUG] LangGraph invocation successful")
            # print(f"--- [DEBUG] Result messages: {result.get('messages')}")
            
            last_message = result["messages"][-1].content
            print(f"--- [DEBUG] Last AI Message: {last_message[:100]}...") # ê¸¸ë©´ ìë¦„
            
            return last_message
            
        except Exception as e:
            print(f"--- [ERROR] LangGraph invocation failed: {e}")
            import traceback
            traceback.print_exc()
            raise e

    async def refine_plan(
        self,
        thread_id: str,
        state_update: dict
    ) -> str:
        """
        LLM2 íœ´ë¨¼ í”¼ë“œë°±: êµ¬ì¡°í™”ëœ í”¼ë“œë°±ìœ¼ë¡œ ì£¼ê°„ ê³„íš ìˆ˜ì •
        """
        print(f"--- [DEBUG] refine_plan ì§„ì… ---")
        print(f"--- [DEBUG] thread_id: {thread_id}")
        print(f"--- [DEBUG] state_update: {state_update}")

        config = {"configurable": {"thread_id": thread_id}}
        
        # LangGraph ì‹¤í–‰ (ì „ë‹¬ë°›ì€ state_updateë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸)
        try:
            result = self.weekly_plan_agent.invoke(
                state_update,
                config=config
            )
            print("--- [DEBUG] LangGraph invocation successful")
            
            last_message = result["messages"][-1].content
            print(f"--- [DEBUG] Last AI Message: {last_message[:100]}...")
            
            return last_message
        except Exception as e:
            print(f"--- [ERROR] LangGraph invocation failed: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
# NOTE:
# - ì´ˆê¸° ì£¼ê°„ ê³„íš ìƒì„±ì€ invoke ë°©ì‹ ì‚¬ìš©
# - ìŠ¤íŠ¸ë¦¬ë°ì€ íœ´ë¨¼ í”¼ë“œë°±(chat/refine)ì—ë§Œ ì‚¬ìš© ì˜ˆì •
# - stream_goal_plan_llmì€ í–¥í›„ UX ë³€ê²½ ëŒ€ë¹„ìš©

    async def stream_goal_plan_llm(
            self,
            input_data: GoalPlanInput
            ) -> AsyncGenerator[str, None]:
        """
        LLM2: ì£¼ê°„ ê³„íšì„œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
        """
        thread_id = f"plan_{input_data.user_id}_{input_data.record_id}_{datetime.now().timestamp()}"
        config = {"configurable": {"thread_id": thread_id}}

        full_text = ""

        # ğŸ”¥ invoke â†’ stream
        async for event in self.weekly_plan_agent.stream(
            {"plan_input": input_data},
            config=config
        ):
            """
            LangGraph stream ì´ë²¤íŠ¸ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìŒ
            ì—¬ê¸°ì„œëŠ” 'LLMì´ ë§í•˜ëŠ” í…ìŠ¤íŠ¸'ë§Œ ê³¨ë¼ëƒ„
            """

            # ì´ë²¤íŠ¸ êµ¬ì¡°ëŠ” LangGraph ë²„ì „ì— ë”°ë¼ ì¡°ê¸ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            # ê°€ì¥ í”í•œ íŒ¨í„´ ì˜ˆì‹œ:
            if event.get("event") == "on_chat_model_stream":
                chunk = event["data"]["chunk"].content
                if chunk:
                    full_text += chunk
                    yield chunk

                # â— í•˜ë£¨ MVPì—ì„œëŠ” ì—¬ê¸°ì„œ DB ì €ì¥ ì•ˆ í•¨
                # ë‚˜ì¤‘ì—:
                # LLMInteractionRepository.create(..., output_text=full_text)
