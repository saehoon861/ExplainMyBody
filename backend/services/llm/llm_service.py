"""
LLM ì„œë¹„ìŠ¤
AI ë¶„ì„ ë° ê³„íš ìƒì„± (LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš©)
"""

from sqlalchemy.orm import Session
from typing import Dict, Any, Optional
from datetime import datetime
import os
from dotenv import load_dotenv

from schemas.llm import StatusAnalysisInput, GoalPlanInput, LLMInteractionCreate
from repositories.llm.llm_interaction_repository import LLMInteractionRepository
from services.llm.llm_clients import create_llm_client
from .agent_graph import create_analysis_agent
from .weekly_plan_graph import create_weekly_plan_agent

load_dotenv()


class LLMService:
    """LLM API í˜¸ì¶œ ì„œë¹„ìŠ¤"""

    def __init__(self):
        """LLM ì—ì´ì „íŠ¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.model_version = "gpt-4o-mini"  # ë˜ëŠ” ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´
        self.llm_client = create_llm_client(self.model_version)
        self.analysis_agent = create_analysis_agent(self.llm_client)
        self.weekly_plan_agent = create_weekly_plan_agent(self.llm_client)

    def prepare_status_analysis_input(
        self,
        record_id: int,
        user_id: int,
        measured_at: datetime,
        measurements: Dict[str, Any],
        body_type1: Optional[str],
        body_type2: Optional[str]
    ) -> Dict[str, Any]:
        """
        LLM1: ê±´ê°• ìƒíƒœ ë¶„ì„ìš© input ë°ì´í„° ì¤€ë¹„

        Args:
            record_id: ê±´ê°• ê¸°ë¡ ID
            user_id: ì‚¬ìš©ì ID
            measured_at: ì¸¡ì • ì¼ì‹œ
            measurements: ì¸ë°”ë”” ì¸¡ì • ë°ì´í„°(ì²´í˜• ë¶„ë¥˜ í¬í•¨)
            body_type1: 1ì°¨ ì²´í˜• ë¶„ë¥˜
            body_type2: 2ì°¨ ì²´í˜• ë¶„ë¥˜

        Returns:
            LLMì— ì „ë‹¬í•  input ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ LLM API í˜¸ì¶œ ì‹œ ì‚¬ìš©)
        """
        return {
            "record_id": record_id,
            "user_id": user_id,
            "measured_at": measured_at,
            "measurements": measurements,
            "body_type1": body_type1,
            "body_type2": body_type2,
        }

    def prepare_goal_plan_input(
        self,
        # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
        user_goal_type: Optional[str],
        user_goal_description: Optional[str],
        # ì„ íƒëœ ê±´ê°• ê¸°ë¡
        record_id: int,
        user_id: int,
        measured_at: datetime,
        measurements: Dict[str, Any],
        # LLM1 ë¶„ì„ ê²°ê³¼
        status_analysis_result: Optional[str] = None,
        status_analysis_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        LLM2: ì£¼ê°„ ê³„íšì„œ ìƒì„±ìš© input ë°ì´í„° ì¤€ë¹„

        Args:
            user_goal_type: ì‚¬ìš©ì ëª©í‘œ íƒ€ì…
            user_goal_description: ì‚¬ìš©ì ëª©í‘œ ìƒì„¸
            record_id: ì„ íƒëœ ê±´ê°• ê¸°ë¡ ID
            user_id: ì‚¬ìš©ì ID
            measured_at: ì¸¡ì • ì¼ì‹œ
            measurements: ì¸ë°”ë”” ì¸¡ì • ë°ì´í„°(ì²´í˜• ë¶„ë¥˜ í¬í•¨)
            status_analysis_result: LLM1ì˜ ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
            status_analysis_id: LLM1 ë¶„ì„ ê²°ê³¼ ID

        Returns:
            LLMì— ì „ë‹¬í•  input ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ LLM API í˜¸ì¶œ ì‹œ ì‚¬ìš©)
        """
        return {
            "user_goal_type": user_goal_type,
            "user_goal_description": user_goal_description,
            "record_id": record_id,
            "user_id": user_id,
            "measured_at": measured_at,
            "measurements": measurements,
            "status_analysis_result": status_analysis_result,
            "status_analysis_id": status_analysis_id
        }

    # =====================================================
    # ì•„ë˜ëŠ” íŒ€ì›ì´ LLM API ì—°ë™ ì‹œ êµ¬í˜„í•  ë©”ì„œë“œë“¤
    # =====================================================

    async def call_status_analysis_llm(
        self,
        input_data: StatusAnalysisInput
    ) -> Dict[str, Any]:
        """
        LangGraph ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ê±´ê°• ìƒíƒœ ë¶„ì„ ìˆ˜í–‰

        Args:
            input_data: StatusAnalysisInput ìŠ¤í‚¤ë§ˆ ê°ì²´

        Returns:
            {
                "analysis_text": str,
                "embedding": {"embedding_1536": [...], "embedding_1024": [...]},
                "thread_id": str
            }
        """
        # 1. ê° ë¶„ì„ ì„¸ì…˜ì„ ìœ„í•œ ê³ ìœ  ìŠ¤ë ˆë“œ ID ìƒì„±
        thread_id = f"analysis_{input_data.user_id}_{input_data.record_id}_{datetime.now().timestamp()}"
        config = {"configurable": {"thread_id": thread_id}}

        # 2. LangGraph ì—ì´ì „íŠ¸ í˜¸ì¶œ (ìµœì´ˆ ë¶„ì„)
        initial_state = self.analysis_agent.invoke(
            {"analysis_input": input_data},
            config=config
        )

        # 3. ê²°ê³¼ ì¶”ì¶œ
        analysis_text = initial_state['messages'][-1].content
        embedding = initial_state.get("embedding")

        return {"analysis_text": analysis_text, "embedding": embedding, "thread_id": thread_id}

    async def chat_with_analysis(
        self,
        thread_id: str,
        user_message: str
    ) -> str:
        """
        LLM1 ì— ëŒ€í•œ
        íœ´ë¨¼ í”¼ë“œë°± (Q&A) ì²˜ë¦¬: ê¸°ì¡´ ìŠ¤ë ˆë“œì— ì´ì–´ì„œ ëŒ€í™” ìˆ˜í–‰
        """
        config = {"configurable": {"thread_id": thread_id}}
        
        # LangGraph ì‹¤í–‰ (ì´ì „ ìƒíƒœì—ì„œ ì´ì–´ì„œ ì‹¤í–‰)
        # messages í‚¤ì— ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        result = self.analysis_agent.invoke(
            {"messages": [("human", user_message)]},
            config=config
        )
        
        # ë§ˆì§€ë§‰ AI ì‘ë‹µ ë°˜í™˜
        return result["messages"][-1].content

    async def call_goal_plan_llm(
        self,
        db: Session,
        input_data: GoalPlanInput
    ) -> dict:
        """
        LLM2: ì£¼ê°„ ê³„íšì„œ ìƒì„± API í˜¸ì¶œ ë° ì´ˆê¸° ìƒí˜¸ì‘ìš© ì €ì¥
        """
        # 1. ìŠ¤ë ˆë“œ ID ìƒì„±
        thread_id = f"plan_{input_data.user_id}_{input_data.record_id}_{datetime.now().timestamp()}"
        config = {"configurable": {"thread_id": thread_id}}



        # 2. LangGraph ì—ì´ì „íŠ¸ í˜¸ì¶œ
        initial_state = self.weekly_plan_agent.invoke(
            {"plan_input": input_data},
            config=config
        )
        
        # (
        #     {"plan_input": input_data},
        #     config=config
        # )
        
        plan_text = initial_state['messages'][-1].content

        # 3. ì´ˆê¸° LLM ìƒí˜¸ì‘ìš© DBì— ì €ì¥
        interaction_schema = LLMInteractionCreate(
            llm_stage="llm2",
            source_type="weekly_plan_initial",
            source_id=input_data.record_id,
            output_text=plan_text,
            model_version=self.model_version
        )
        new_interaction = LLMInteractionRepository.create(db, input_data.user_id, interaction_schema)

        # 4. ê²°ê³¼ ë°˜í™˜
        return {
            "plan_text": plan_text,
            "thread_id": thread_id,
            "llm_interaction_id": new_interaction.id
        }

    async def chat_with_plan(
        self,
        thread_id: str,
        user_message: str,
        existing_plan: Optional[str] = None
    ) -> str:
        """
        LLM2 íœ´ë¨¼ í”¼ë“œë°± (Q&A) ì²˜ë¦¬: ì£¼ê°„ ê³„íš ìˆ˜ì • ë° ì§ˆì˜ì‘ë‹µ
        """
        import re
        print(f"--- [DEBUG] chat_with_plan ì§„ì… ---")
        print(f"--- [DEBUG] thread_id: {thread_id}")
        print(f"--- [DEBUG] raw user_message: {user_message}")

        config = {"configurable": {"thread_id": thread_id}}
        
        # ë©”ì‹œì§€ íŒŒì‹± ([Category: ...] ë¶„ë¦¬)
        category = None
        clean_message = user_message
        
        # Regexë¡œ [Category: ...] íŒ¨í„´ ì¶”ì¶œ
        match = re.match(r"^\[Category:\s*(.*?)\]\s*(.*)$", user_message, re.DOTALL)
        if match:
            category_label = match.group(1).strip()
            clean_message = match.group(2).strip()
            print(f"--- [DEBUG] Parsed Category Label: {category_label}")
            print(f"--- [DEBUG] Clean Message: {clean_message}")
            
            # ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì„ ë‚´ë¶€ í‚¤ë¡œ ë§¤í•‘ (í•„ìš”ì‹œ)
            # í˜„ì¬ëŠ” ë‹¨ìˆœ ë§¤í•‘ ë˜ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬. ê·¸ë˜í”„ì˜ router ë¡œì§ì— ì˜ì¡´.
            # ê·¸ë˜í”„ì—ì„œëŠ” "ìš´ë™ í”Œëœ ì¡°ì •", "ì‹ë‹¨ ì¡°ì •" ë“±ì„ ê¸°ëŒ€í•¨.
            # í”„ë¡ íŠ¸ì—”ë“œ ë¼ë²¨: "ğŸ“… ì£¼ê°„ ê³„íš", "ğŸ‹ï¸ ë¶€ìœ„ë³„ ìš´ë™" ë“±.
            # ì—¬ê¸°ì„œëŠ” ìš°ì„  ë§¤í•‘ ì—†ì´ ì „ë‹¬í•˜ê±°ë‚˜, ê°„ë‹¨í•œ ë³€í™˜ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.
            
            # ì„ì‹œ ë§¤í•‘ ë¡œì§ (í”„ë¡ íŠ¸ ë¼ë²¨ -> ê·¸ë˜í”„ ë‚´ë¶€ ì¹´í…Œê³ ë¦¬)
            if "ìš´ë™" in category_label or "í”Œëœ" in category_label or "ê³„íš" in category_label: # ì˜ˆ: ìš´ë™ í”Œëœ ì¡°ì •
                category = "adjust_exercise_plan"
            elif "ì‹ë‹¨" in category_label:
                category = "adjust_diet_plan"
            elif "ê°•ë„" in category_label:
                category = "adjust_intensity"
            else:
                # ê·¸ ì™¸ (ì£¼ê°„ ê³„íš, ë¶€ìœ„ë³„ ìš´ë™ ë“±)ì€ ì¼ë°˜ Q&Aë¡œ ì²˜ë¦¬í•˜ë˜,
                # ê·¸ë˜í”„ ë¼ìš°í„°ê°€ "qa_general"ë¡œ í´ë°±í•˜ë„ë¡ None ë˜ëŠ” "qa_general" ì „ë‹¬
                category = "qa_general"
            
            print(f"--- [DEBUG] Mapped Category Key: {category}")
            
        else:
            print("--- [DEBUG] No Category prefix found. Treating as general message.")
            category = "qa_general"

        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤€ë¹„
        input_payload = {
            "messages": [("human", clean_message)],
            # feedback_categoryë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë¼ìš°í„°ê°€ ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ íƒ€ë„ë¡ í•¨
            "feedback_category": category,
            "existing_plan": existing_plan
        }
        
        print(f"--- [DEBUG] llm_service: existing_plan ì „ë‹¬ ì—¬ë¶€: {bool(existing_plan)} ---")
        if existing_plan:
            print(f"--- [DEBUG] llm_service: existing_plan preview: {existing_plan[:100]}... ---")
        print(f"--- [DEBUG] Invoking weekly_plan_agent with payload: {input_payload}")

        # LangGraph ì‹¤í–‰ (ì´ì „ ìƒíƒœì—ì„œ ì´ì–´ì„œ ì‹¤í–‰)
        try:
            result = self.weekly_plan_agent.invoke(
                input_payload,
                config=config
            )
            print("--- [DEBUG] LangGraph invocation successful")
            # print(f"--- [DEBUG] Result messages: {result.get('messages')}")
            
            last_message = result["messages"][-1].content
            print(f"--- [DEBUG] Last AI Message: {last_message[:100]}...") # ê¸¸ë©´ ìë¦„
            
            return last_message
            
        except Exception as e:
            print(f"--- [ERROR] LangGraph invocation failed: {e}")
            import traceback
            traceback.print_exc()
            raise e

    async def refine_plan(
        self,
        thread_id: str,
        state_update: dict
    ) -> str:
        """
        LLM2 íœ´ë¨¼ í”¼ë“œë°±: êµ¬ì¡°í™”ëœ í”¼ë“œë°±ìœ¼ë¡œ ì£¼ê°„ ê³„íš ìˆ˜ì •
        """
        print(f"--- [DEBUG] refine_plan ì§„ì… ---")
        print(f"--- [DEBUG] thread_id: {thread_id}")
        print(f"--- [DEBUG] state_update: {state_update}")

        config = {"configurable": {"thread_id": thread_id}}
        
        # LangGraph ì‹¤í–‰ (ì „ë‹¬ë°›ì€ state_updateë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸)
        try:
            result = self.weekly_plan_agent.invoke(
                state_update,
                config=config
            )
            print("--- [DEBUG] LangGraph invocation successful")
            
            last_message = result["messages"][-1].content
            print(f"--- [DEBUG] Last AI Message: {last_message[:100]}...")
            
            return last_message
        except Exception as e:
            print(f"--- [ERROR] LangGraph invocation failed: {e}")
            import traceback
            traceback.print_exc()
            raise e
